{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LexXlomCaMfgk8Ru2l0vX-lY9Cot79SR",
      "authorship_tag": "ABX9TyM/1VPnglDUMV1k7tXD+0nH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f1f1ad559284421aea0d688c26fefc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0150f2b0173343858328f3ff642bffde",
              "IPY_MODEL_cf8b380499a047cc963edbaf41b4d021",
              "IPY_MODEL_563ee6b2069c4096b776a356de840958"
            ],
            "layout": "IPY_MODEL_fb9887601b764b708cc9994da5264d94"
          }
        },
        "0150f2b0173343858328f3ff642bffde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba72196927244bb4a00323d1c13b212c",
            "placeholder": "​",
            "style": "IPY_MODEL_923570006dcd4d46bcbf3f4a418df69c",
            "value": "model.safetensors: 100%"
          }
        },
        "cf8b380499a047cc963edbaf41b4d021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f45c768390401d9f503df9714c3d26",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_159e48c8f83545368e4c7c9ba77cc435",
            "value": 346284714
          }
        },
        "563ee6b2069c4096b776a356de840958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a9316c1314c43809729cfbb826ebffc",
            "placeholder": "​",
            "style": "IPY_MODEL_0299984ac4f84750af313248adbf55d9",
            "value": " 346M/346M [00:01&lt;00:00, 193MB/s]"
          }
        },
        "fb9887601b764b708cc9994da5264d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba72196927244bb4a00323d1c13b212c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "923570006dcd4d46bcbf3f4a418df69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81f45c768390401d9f503df9714c3d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159e48c8f83545368e4c7c9ba77cc435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a9316c1314c43809729cfbb826ebffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0299984ac4f84750af313248adbf55d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravindh4404/FYPSeagullClassification01/blob/main/VITRefine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "dfOYnrYT1p_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3f0c27-59b2-4b0d-e304-e4d82e0608cd",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from captum) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
            "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed captum-0.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Model Architectures\n",
        "# ----------------------------\n",
        "class InterpretableViT(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3, hidden_dim=512):\n",
        "        super(InterpretableViT, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()  # Remove original classification head\n",
        "        self.embed_dim = self.vit.embed_dim if hasattr(self.vit, 'embed_dim') else 768\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim * 2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.embed_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = self.vit.forward_features(x)\n",
        "        cls_token = tokens[:, 0, :]\n",
        "        patch_tokens = tokens[:, 1:, :]\n",
        "        attn_scores = self.attention_layer(patch_tokens)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        weighted_patch = torch.sum(attn_weights * patch_tokens, dim=1)\n",
        "        combined = torch.cat([cls_token, weighted_patch], dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "        return logits, attn_weights\n",
        "\n",
        "class EnhancedViT(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3, hidden_dim=512):\n",
        "        super(EnhancedViT, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()\n",
        "        self.embed_dim = self.vit.embed_dim if hasattr(self.vit, 'embed_dim') else self.vit.head.in_features\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = self.vit.forward_features(x)\n",
        "        attn_scores = self.attention_layer(tokens)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        weighted_feature = torch.sum(attn_weights * tokens, dim=1)\n",
        "        out = self.classifier(weighted_feature)\n",
        "        return out\n",
        "\n",
        "class ViTModified(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTModified, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        num_ftrs = self.vit.head.in_features\n",
        "        self.vit.head = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Image Processing Functions\n",
        "# ----------------------------\n",
        "def preprocess_image(image_path, device, normalize=True):\n",
        "    if normalize:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "def get_original_image(image_path, size=(224, 224)):\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize(size, Image.LANCZOS)\n",
        "    return np.array(image)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Interpretability Analyzers\n",
        "# ----------------------------\n",
        "from captum.attr import DeepLift, IntegratedGradients, GuidedBackprop\n",
        "from captum.attr import LayerGradCam  # For GradCAM in InterpretableViT\n",
        "\n",
        "class GradCAMAnalyzer:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradcam = LayerGradCam(self.model, self.target_layer)\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = self.model(image_tensor)\n",
        "            logits = output[0] if isinstance(output, tuple) else output\n",
        "            if target_class is None:\n",
        "                target_class = logits.argmax().item()\n",
        "        attribution = self.gradcam.attribute(image_tensor, target=target_class)\n",
        "        attr = attribution.detach().cpu().numpy()[0]\n",
        "        if attr.ndim == 3:\n",
        "            attr = np.mean(attr, axis=0)\n",
        "        attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "        return attr\n",
        "\n",
        "class DeepLIFTAnalyzer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        # For models returning tuples (e.g. InterpretableViT)\n",
        "        if isinstance(model, InterpretableViT):\n",
        "            self.deep_lift = DeepLift(self._model_wrapper)\n",
        "        else:\n",
        "            self.deep_lift = DeepLift(self.model)\n",
        "\n",
        "    def _model_wrapper(self, x):\n",
        "        output = self.model(x)\n",
        "        return output[0] if isinstance(output, tuple) else output\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        if target_class is None:\n",
        "            with torch.no_grad():\n",
        "                output = self._model_wrapper(image_tensor)\n",
        "                target_class = output.argmax().item()\n",
        "        baseline = torch.zeros_like(image_tensor).to(self.device)\n",
        "        attribution = self.deep_lift.attribute(image_tensor, baselines=baseline, target=target_class)\n",
        "        attr = attribution.detach().cpu().numpy()[0]\n",
        "        if attr.ndim == 3:\n",
        "            attr = np.transpose(attr, (1, 2, 0))\n",
        "            attr = np.sum(np.abs(attr), axis=2)\n",
        "        attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "        return attr\n",
        "\n",
        "class LRPAnalyzer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        if isinstance(model, InterpretableViT):\n",
        "            self.ig = IntegratedGradients(self._model_wrapper)\n",
        "        else:\n",
        "            self.ig = IntegratedGradients(self.model)\n",
        "\n",
        "    def _model_wrapper(self, x):\n",
        "        output = self.model(x)\n",
        "        return output[0] if isinstance(output, tuple) else output\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        if target_class is None:\n",
        "            with torch.no_grad():\n",
        "                output = self._model_wrapper(image_tensor)\n",
        "                target_class = output.argmax().item()\n",
        "        baseline = torch.zeros_like(image_tensor).to(self.device)\n",
        "        attribution = self.ig.attribute(image_tensor, baselines=baseline, target=target_class, n_steps=50)\n",
        "        attr = attribution.detach().cpu().numpy()[0]\n",
        "        if attr.ndim == 3:\n",
        "            attr = np.transpose(attr, (1, 2, 0))\n",
        "            attr = np.sum(np.abs(attr), axis=2)\n",
        "        attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "        return attr\n",
        "\n",
        "class SaliencyMapAnalyzer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        if isinstance(model, InterpretableViT):\n",
        "            self.guided_backprop = GuidedBackprop(self._model_wrapper)\n",
        "        else:\n",
        "            self.guided_backprop = GuidedBackprop(self.model)\n",
        "\n",
        "    def _model_wrapper(self, x):\n",
        "        output = self.model(x)\n",
        "        return output[0] if isinstance(output, tuple) else output\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        if target_class is None:\n",
        "            with torch.no_grad():\n",
        "                output = self._model_wrapper(image_tensor)\n",
        "                target_class = output.argmax().item()\n",
        "        attribution = self.guided_backprop.attribute(image_tensor, target=target_class)\n",
        "        attr = attribution.detach().cpu().numpy()[0]\n",
        "        if attr.ndim == 3:\n",
        "            attr = np.transpose(attr, (1, 2, 0))\n",
        "            attr = np.sum(np.abs(attr), axis=2)\n",
        "        attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "        return attr\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Visualization & Saving Functions\n",
        "# ----------------------------\n",
        "def save_visualization(original_image, attribution_maps, class_name, predicted_class, confidence, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    n_panels = 1 + len(attribution_maps)\n",
        "    fig, axes = plt.subplots(1, n_panels, figsize=(n_panels * 4, 4))\n",
        "    # Original image\n",
        "    axes[0].imshow(original_image)\n",
        "    axes[0].set_title(f\"Predicted: {predicted_class}\\nConf: {confidence:.2f}\")\n",
        "    axes[0].axis('off')\n",
        "    # Attribution maps\n",
        "    for i, (name, attr_map) in enumerate(attribution_maps.items(), 1):\n",
        "        if name == 'Saliency':\n",
        "            axes[i].imshow(attr_map, cmap='hot')\n",
        "            axes[i].set_title(f\"{name} Map\")\n",
        "        else:\n",
        "            heatmap = cv2.applyColorMap(np.uint8(255 * attr_map), cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "            if original_image.shape[:2] != heatmap.shape[:2]:\n",
        "                heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
        "            overlay = cv2.addWeighted(original_image, 0.6, heatmap, 0.4, 0)\n",
        "            axes[i].imshow(overlay)\n",
        "            axes[i].set_title(f\"{name} Overlay\")\n",
        "        axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_statistics_csv(stat_list, csv_path):\n",
        "    df = pd.DataFrame(stat_list)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Statistics saved to {csv_path}\")\n",
        "\n",
        "def generate_confusion_matrix(stats_list, save_path, class_names):\n",
        "    try:\n",
        "        df = pd.DataFrame(stats_list)\n",
        "        if not df.empty:\n",
        "            confusion = pd.crosstab(df['true_class'], df['predicted_class'],\n",
        "                                    rownames=['True'], colnames=['Predicted'])\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "            plt.title(\"Confusion Matrix\")\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Confusion matrix saved to: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating confusion matrix: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Main Dataset Processing Function\n",
        "# ----------------------------\n",
        "def process_dataset_with_models(dataset_path, model_paths, output_dir_base,\n",
        "                                class_names=['Glaucous_Winged_Gull', 'Slaty_Backed_Gull']):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    main_output_dir = Path(output_dir_base) / timestamp\n",
        "    main_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Main output directory: {main_output_dir}\")\n",
        "\n",
        "    for model_idx, model_path in enumerate(model_paths, 1):\n",
        "        model_name = Path(model_path).stem\n",
        "        print(f\"\\n[{model_idx}/{len(model_paths)}] Processing with model: {model_name}\")\n",
        "        # Attempt to load with available architectures\n",
        "        model_loaded = False\n",
        "        for arch in [ViTModified, InterpretableViT, EnhancedViT]:\n",
        "            try:\n",
        "                model = arch().to(device)\n",
        "                model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "                print(f\"  Loaded with {arch.__name__}\")\n",
        "                arch_used = arch.__name__\n",
        "                model_loaded = True\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"  Error loading with {arch.__name__}: {e}\")\n",
        "        if not model_loaded:\n",
        "            print(f\"Skipping model {model_path} as it could not be loaded.\")\n",
        "            continue\n",
        "\n",
        "        # Set up output directories for this model\n",
        "        model_output_dir = main_output_dir / model_name\n",
        "        deeplift_dir = model_output_dir / \"deeplift_visualizations\"\n",
        "        lrp_dir = model_output_dir / \"lrp_visualizations\"\n",
        "        saliency_dir = model_output_dir / \"saliency_visualizations\"\n",
        "        combined_dir = model_output_dir / \"combined_visualizations\"\n",
        "        correct_images_dir = model_output_dir / \"correct_images\"\n",
        "        for d in [deeplift_dir, lrp_dir, saliency_dir, combined_dir, correct_images_dir]:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Initialize analyzers\n",
        "        deeplift_analyzer = DeepLIFTAnalyzer(model, device)\n",
        "        lrp_analyzer = LRPAnalyzer(model, device)\n",
        "        saliency_analyzer = SaliencyMapAnalyzer(model, device)\n",
        "        model_stats = []\n",
        "\n",
        "        # Process images in each class folder\n",
        "        dataset_path = Path(dataset_path)\n",
        "        for class_folder in dataset_path.iterdir():\n",
        "            if not class_folder.is_dir():\n",
        "                continue\n",
        "            true_class = class_folder.name\n",
        "            if true_class not in class_names:\n",
        "                print(f\"Skipping folder '{true_class}' as it is not in class_names\")\n",
        "                continue\n",
        "\n",
        "            # Create class-specific directories under each output type\n",
        "            class_deeplift_dir = deeplift_dir / true_class\n",
        "            class_lrp_dir = lrp_dir / true_class\n",
        "            class_saliency_dir = saliency_dir / true_class\n",
        "            class_combined_dir = combined_dir / true_class\n",
        "            class_correct_dir = correct_images_dir / true_class\n",
        "            for d in [class_deeplift_dir, class_lrp_dir, class_saliency_dir, class_combined_dir, class_correct_dir]:\n",
        "                d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Find image files (jpg, jpeg, png)\n",
        "            image_files = list(class_folder.glob(\"*.[jJ][pP][gG]\")) + \\\n",
        "                          list(class_folder.glob(\"*.[jJ][pP][eE][gG]\")) + \\\n",
        "                          list(class_folder.glob(\"*.[pP][nN][gG]\"))\n",
        "            print(f\"Processing {len(image_files)} images in class '{true_class}'...\")\n",
        "\n",
        "            for image_path in tqdm(image_files, desc=f\"[{model_name}] {true_class}\"):\n",
        "                try:\n",
        "                    img_path_str = str(image_path)\n",
        "                    image_tensor = preprocess_image(img_path_str, device)\n",
        "                    original_image = get_original_image(img_path_str)\n",
        "                    with torch.no_grad():\n",
        "                        output = model(image_tensor)\n",
        "                        logits = output[0] if isinstance(output, tuple) else output\n",
        "                        probs = F.softmax(logits, dim=1)\n",
        "                        predicted_class_idx = torch.argmax(probs, dim=1).item()\n",
        "                        confidence = probs[0, predicted_class_idx].item()\n",
        "                    predicted_class = class_names[predicted_class_idx] if predicted_class_idx < len(class_names) else str(predicted_class_idx)\n",
        "                    is_correct = (predicted_class == true_class)\n",
        "\n",
        "                    # Generate visualizations only for correctly predicted images\n",
        "                    if is_correct:\n",
        "                        deeplift_attr = deeplift_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                        lrp_attr = lrp_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                        # For InterpretableViT use GradCAM; otherwise saliency overlay\n",
        "                        if isinstance(model, InterpretableViT):\n",
        "                            target_layer = model.vit.patch_embed.proj\n",
        "                            gradcam_analyzer = GradCAMAnalyzer(model, target_layer)\n",
        "                            gradcam_attr = gradcam_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                            attributions = {'DeepLIFT': deeplift_attr,\n",
        "                                            'LRP': lrp_attr,\n",
        "                                            'GradCAM': gradcam_attr}\n",
        "                        else:\n",
        "                            saliency_attr = saliency_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                            attributions = {'DeepLIFT': deeplift_attr,\n",
        "                                            'LRP': lrp_attr,\n",
        "                                            'Saliency': saliency_attr}\n",
        "\n",
        "                        # Save individual visualizations\n",
        "                        save_visualization(original_image, {'DeepLIFT': deeplift_attr},\n",
        "                                           true_class, predicted_class, confidence,\n",
        "                                           str(class_deeplift_dir / f\"{image_path.stem}_deeplift.png\"))\n",
        "                        save_visualization(original_image, {'LRP': lrp_attr},\n",
        "                                           true_class, predicted_class, confidence,\n",
        "                                           str(class_lrp_dir / f\"{image_path.stem}_lrp.png\"))\n",
        "                        if 'Saliency' in attributions:\n",
        "                            save_visualization(original_image, {'Saliency': saliency_attr},\n",
        "                                               true_class, predicted_class, confidence,\n",
        "                                               str(class_saliency_dir / f\"{image_path.stem}_saliency.png\"))\n",
        "                        else:\n",
        "                            save_visualization(original_image, {'GradCAM': gradcam_attr},\n",
        "                                               true_class, predicted_class, confidence,\n",
        "                                               str(class_saliency_dir / f\"{image_path.stem}_gradcam.png\"))\n",
        "                        # Save combined visualization\n",
        "                        save_visualization(original_image, attributions,\n",
        "                                           true_class, predicted_class, confidence,\n",
        "                                           str(class_combined_dir / f\"{image_path.stem}_combined.png\"))\n",
        "                        # Copy original image to correct images folder\n",
        "                        shutil.copy2(img_path_str, str(class_correct_dir / image_path.name))\n",
        "\n",
        "                    # Log statistics for each image\n",
        "                    model_stats.append({\n",
        "                        \"model\": arch_used,\n",
        "                        \"file\": img_path_str,\n",
        "                        \"true_class\": true_class,\n",
        "                        \"predicted_class\": predicted_class,\n",
        "                        \"confidence\": confidence,\n",
        "                        \"correct\": is_correct\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {image_path} with model {arch_used}: {e}\")\n",
        "\n",
        "        # Save per-model statistics and confusion matrix\n",
        "        stats_csv_path = model_output_dir / f\"{model_name}_statistics.csv\"\n",
        "        save_statistics_csv(model_stats, str(stats_csv_path))\n",
        "        confusion_matrix_path = model_output_dir / f\"{model_name}_confusion_matrix.png\"\n",
        "        generate_confusion_matrix(model_stats, str(confusion_matrix_path), class_names)\n",
        "        df_stats = pd.DataFrame(model_stats)\n",
        "        accuracy = df_stats['correct'].mean() * 100\n",
        "        print(f\"Model '{model_name}' (Architecture: {arch_used}) accuracy: {accuracy:.2f}%\")\n",
        "        with open(str(model_output_dir / f\"{model_name}_summary.txt\"), \"w\") as f:\n",
        "            f.write(f\"Model: {model_name} (Architecture: {arch_used})\\n\")\n",
        "            f.write(f\"Processed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Overall Accuracy: {accuracy:.2f}%\\n\\n\")\n",
        "            f.write(\"Per-Class Metrics:\\n\")\n",
        "            for cls in class_names:\n",
        "                cls_df = df_stats[df_stats['true_class'] == cls]\n",
        "                cls_acc = cls_df['correct'].mean() * 100 if len(cls_df) > 0 else 0\n",
        "                correct_count = len(cls_df[cls_df['correct']])\n",
        "                total_count = len(cls_df)\n",
        "                f.write(f\"  {cls}: {cls_acc:.2f}% ({correct_count}/{total_count} correct)\\n\")\n",
        "        print(f\"Completed processing with model: {model_name}\")\n",
        "\n",
        "    print(f\"\\nAll processing complete. Results saved to {main_output_dir}\")\n",
        "\n",
        "    # Optionally, generate a combined report across models.\n",
        "    try:\n",
        "        all_stats = []\n",
        "        for model_path in model_paths:\n",
        "            m_name = Path(model_path).stem\n",
        "            stats_path = main_output_dir / m_name / f\"{m_name}_statistics.csv\"\n",
        "            if stats_path.exists():\n",
        "                df_m = pd.read_csv(stats_path)\n",
        "                all_stats.append(df_m)\n",
        "        if all_stats:\n",
        "            combined_df = pd.concat(all_stats)\n",
        "            combined_stats_path = main_output_dir / \"all_models_comparison.csv\"\n",
        "            combined_df.to_csv(combined_stats_path, index=False)\n",
        "            model_summary = combined_df.groupby('model')['correct'].agg(['mean', 'count']).reset_index()\n",
        "            model_summary['accuracy'] = model_summary['mean'] * 100\n",
        "            model_summary = model_summary.drop('mean', axis=1)\n",
        "            model_summary = model_summary.rename(columns={'count': 'total_images', 'accuracy': 'accuracy_percent'})\n",
        "            model_summary = model_summary.sort_values('accuracy_percent', ascending=False)\n",
        "            model_summary.to_csv(main_output_dir / \"model_accuracy_comparison.csv\", index=False)\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(x='model', y='accuracy_percent', data=model_summary)\n",
        "            plt.title('Model Accuracy Comparison')\n",
        "            plt.xlabel('Model')\n",
        "            plt.ylabel('Accuracy (%)')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(main_output_dir / \"model_accuracy_comparison.png\", dpi=300)\n",
        "            plt.close()\n",
        "            print(f\"Generated combined model comparison reports in {main_output_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating combined summary: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Main Execution Example\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # For Colab, you might need to mount your drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define your dataset and output paths (update these as needed)\n",
        "    DATASET_PATH = \"/content/drive/My Drive/FYP/Black Background\"\n",
        "\n",
        "    # List your model checkpoint paths (update these as needed)\n",
        "    MODEL_PATHS = [\n",
        "        \"/content/drive/My Drive/FYP/MODELS/VIT/VIT2_HQ2_20241222/latest_model_vit_20241222.pth\",\n",
        "        \"/content/drive/My Drive/FYP/MODELS/VIT/VIT2_HQ3_20250208/final_model_vit_20250208.pth\",\n",
        "        \"/content/drive/My Drive/FYP/MODELS/VIT/InterpretableViT_20250213/final_model_vit_20250213.pth\",\n",
        "    ]\n",
        "    # Define class names (subfolder names)\n",
        "    CLASS_NAMES = [\"Glaucous_Winged_Gull\", \"Slaty_Backed_Gull\"]\n",
        "    # Base output directory\n",
        "    OUTPUT_DIR_BASE = \"./model_output\"\n",
        "\n",
        "    valid_models = [m for m in MODEL_PATHS if os.path.exists(m)]\n",
        "    if not valid_models:\n",
        "        print(\"ERROR: No valid models found. Please check your model paths.\")\n",
        "    else:\n",
        "        process_dataset_with_models(DATASET_PATH, valid_models, OUTPUT_DIR_BASE, CLASS_NAMES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1xwmy5iqpC1",
        "outputId": "f3d81442-67f4-4564-e164-1b47d65d5dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Main output directory: model_output/20250310_013627\n",
            "\n",
            "[1/3] Processing with model: latest_model_vit_20241222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-2-c0581b4ff0b7>:287: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded with ViTModified\n",
            "Processing 143 images in class 'Slaty_Backed_Gull'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[latest_model_vit_20241222] Slaty_Backed_Gull:   1%|▏         | 2/143 [00:00<00:55,  2.56it/s]/usr/local/lib/python3.11/dist-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
            "               activations. The hooks and attributes will be removed\n",
            "            after the attribution is finished\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/captum/attr/_core/guided_backprop_deconvnet.py:64: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
            "  warnings.warn(\n",
            "[latest_model_vit_20241222] Slaty_Backed_Gull:  39%|███▉      | 56/143 [03:03<04:24,  3.04s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngYy0jZ_0ggs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4f1f1ad559284421aea0d688c26fefc9",
            "0150f2b0173343858328f3ff642bffde",
            "cf8b380499a047cc963edbaf41b4d021",
            "563ee6b2069c4096b776a356de840958",
            "fb9887601b764b708cc9994da5264d94",
            "ba72196927244bb4a00323d1c13b212c",
            "923570006dcd4d46bcbf3f4a418df69c",
            "81f45c768390401d9f503df9714c3d26",
            "159e48c8f83545368e4c7c9ba77cc435",
            "4a9316c1314c43809729cfbb826ebffc",
            "0299984ac4f84750af313248adbf55d9"
          ]
        },
        "outputId": "fea1855d-eedc-48f0-d17d-7ef455e5f479"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.1+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 2 valid models. Starting processing...\n",
            "Using device: cuda\n",
            "Main output directory: /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739\n",
            "\n",
            "[1/2] Processing with model file: final_model_vit_20250208\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f1f1ad559284421aea0d688c26fefc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-38eb490eac3f>:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading model from /content/drive/My Drive/FYP/MODELS/VIT/VIT2_HQ3_20250208/final_model_vit_20250208.pth with ViTModified: Error(s) in loading state_dict for ViTModified:\n",
            "\tMissing key(s) in state_dict: \"vit.head.weight\", \"vit.head.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"attention_layer.0.weight\", \"attention_layer.0.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.5.weight\", \"classifier.5.bias\". \n",
            "Error loading model from /content/drive/My Drive/FYP/MODELS/VIT/VIT2_HQ3_20250208/final_model_vit_20250208.pth with InterpretableViT: Error(s) in loading state_dict for InterpretableViT:\n",
            "\tsize mismatch for classifier.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
            "\tsize mismatch for classifier.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
            "\tsize mismatch for classifier.2.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([512, 1536]).\n",
            "Successfully loaded checkpoint with EnhancedViT\n",
            "Processing 143 images in class 'Slaty_Backed_Gull'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250208] Class: Slaty_Backed_Gull:   1%|          | 1/143 [00:01<03:20,  1.41s/it]/usr/local/lib/python3.11/dist-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
            "               activations. The hooks and attributes will be removed\n",
            "            after the attribution is finished\n",
            "  warnings.warn(\n",
            "[final_model_vit_20250208] Class: Slaty_Backed_Gull: 100%|██████████| 143/143 [09:42<00:00,  4.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 124 images in class 'Glaucous_Winged_Gull'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250208] Class: Glaucous_Winged_Gull: 100%|██████████| 124/124 [06:58<00:00,  3.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics saved to /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739/final_model_vit_20250208/final_model_vit_20250208_statistics.csv\n",
            "Confusion matrix saved to: /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739/final_model_vit_20250208/final_model_vit_20250208_confusion_matrix.png\n",
            "Model 'final_model_vit_20250208' (loaded as EnhancedViT) accuracy: 91.01%\n",
            "Completed processing with model: final_model_vit_20250208\n",
            "\n",
            "[2/2] Processing with model file: final_model_vit_20250213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-38eb490eac3f>:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model from /content/drive/My Drive/FYP/MODELS/VIT/InterpretableViT_20250213/final_model_vit_20250213.pth with ViTModified: Error(s) in loading state_dict for ViTModified:\n",
            "\tMissing key(s) in state_dict: \"vit.head.weight\", \"vit.head.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"attention_layer.0.weight\", \"attention_layer.0.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.5.weight\", \"classifier.5.bias\". \n",
            "Successfully loaded checkpoint with InterpretableViT\n",
            "Processing 143 images in class 'Slaty_Backed_Gull'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   0%|          | 0/143 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/001.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
            "               activations. The hooks and attributes will be removed\n",
            "            after the attribution is finished\n",
            "  warnings.warn(\n",
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:   2%|▏         | 3/143 [00:00<00:47,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/003.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   3%|▎         | 4/143 [00:01<00:58,  2.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/004.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   3%|▎         | 5/143 [00:01<01:04,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/006.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   4%|▍         | 6/143 [00:02<01:07,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/005.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   5%|▍         | 7/143 [00:03<01:08,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/007.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   6%|▌         | 8/143 [00:03<01:11,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/008.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   6%|▋         | 9/143 [00:04<01:11,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/009.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   7%|▋         | 10/143 [00:04<01:12,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/011.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   8%|▊         | 11/143 [00:05<01:12,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/010.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   8%|▊         | 12/143 [00:05<01:12,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/012.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:   9%|▉         | 13/143 [00:06<01:12,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/013.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  10%|▉         | 14/143 [00:07<01:12,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/014.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  10%|█         | 15/143 [00:07<01:11,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/015.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  11%|█         | 16/143 [00:08<01:10,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/017.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  13%|█▎        | 18/143 [00:08<00:52,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/016.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/019.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  13%|█▎        | 19/143 [00:09<01:04,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/018.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/020.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  15%|█▍        | 21/143 [00:10<01:03,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/021.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  15%|█▌        | 22/143 [00:11<01:13,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/023.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  16%|█▌        | 23/143 [00:12<01:19,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/022.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/024.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  17%|█▋        | 25/143 [00:12<01:00,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/025.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  18%|█▊        | 26/143 [00:13<01:01,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/026.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  19%|█▉        | 27/143 [00:13<01:01,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/028.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  20%|█▉        | 28/143 [00:14<01:01,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/027.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  20%|██        | 29/143 [00:15<01:01,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/029.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  21%|██        | 30/143 [00:15<01:01,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/030.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  22%|██▏       | 31/143 [00:16<01:00,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/031.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  22%|██▏       | 32/143 [00:16<01:00,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/033.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  23%|██▎       | 33/143 [00:17<01:00,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/032.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  24%|██▍       | 34/143 [00:17<00:59,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/034.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  24%|██▍       | 35/143 [00:18<00:58,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/036.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  25%|██▌       | 36/143 [00:18<00:57,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/035.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  27%|██▋       | 38/143 [00:19<00:43,  2.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/037.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/038.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  27%|██▋       | 39/143 [00:20<00:47,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/039.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  28%|██▊       | 40/143 [00:20<00:49,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/041.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  29%|██▊       | 41/143 [00:21<00:50,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/040.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  29%|██▉       | 42/143 [00:21<00:51,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/042.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  30%|███       | 43/143 [00:22<00:54,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/043.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  31%|███       | 44/143 [00:22<00:43,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/044.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  31%|███▏      | 45/143 [00:23<00:57,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/046.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  32%|███▏      | 46/143 [00:24<01:05,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/045.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  33%|███▎      | 47/143 [00:25<01:09,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/047.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  34%|███▎      | 48/143 [00:25<01:04,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/049.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  35%|███▍      | 50/143 [00:26<00:44,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/048.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/050.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  36%|███▌      | 51/143 [00:27<00:46,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/051.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  37%|███▋      | 53/143 [00:27<00:36,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/053.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/052.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  38%|███▊      | 54/143 [00:28<00:40,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/054.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  38%|███▊      | 55/143 [00:28<00:42,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/056.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  39%|███▉      | 56/143 [00:29<00:43,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/055.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  40%|███▉      | 57/143 [00:29<00:39,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/057.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  41%|████      | 58/143 [00:30<00:40,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/060.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  43%|████▎     | 62/143 [00:30<00:21,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/059.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/063.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/062.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  46%|████▌     | 66/143 [00:31<00:12,  6.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/061.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/066.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/065.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/068.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/069.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  48%|████▊     | 69/143 [00:31<00:08,  9.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/071.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  50%|█████     | 72/143 [00:32<00:10,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/072.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/073.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  52%|█████▏    | 74/143 [00:33<00:16,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/074.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  52%|█████▏    | 75/143 [00:33<00:19,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/075.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  53%|█████▎    | 76/143 [00:34<00:22,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/076.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  54%|█████▍    | 77/143 [00:34<00:24,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/078.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  55%|█████▍    | 78/143 [00:35<00:27,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/077.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  55%|█████▌    | 79/143 [00:36<00:33,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/079.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  57%|█████▋    | 81/143 [00:37<00:42,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/080.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  57%|█████▋    | 82/143 [00:38<00:42,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/082.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  58%|█████▊    | 83/143 [00:39<00:38,  1.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/084.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  59%|█████▉    | 85/143 [00:40<00:34,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/085.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  60%|██████    | 86/143 [00:40<00:32,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/086.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  61%|██████    | 87/143 [00:41<00:31,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/087.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  62%|██████▏   | 88/143 [00:41<00:30,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/088.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  62%|██████▏   | 89/143 [00:42<00:29,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/089.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  63%|██████▎   | 90/143 [00:42<00:29,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/090.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  64%|██████▎   | 91/143 [00:43<00:28,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/091.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  64%|██████▍   | 92/143 [00:44<00:27,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/092.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  65%|██████▌   | 93/143 [00:44<00:27,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/093.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  66%|██████▌   | 94/143 [00:45<00:27,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/094.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  66%|██████▋   | 95/143 [00:45<00:26,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/095.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  67%|██████▋   | 96/143 [00:46<00:25,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/096.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  68%|██████▊   | 97/143 [00:46<00:25,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/097.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  69%|██████▉   | 99/143 [00:47<00:23,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/098.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  70%|██████▉   | 100/143 [00:48<00:23,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/100.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  71%|███████   | 101/143 [00:49<00:26,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/102.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  71%|███████▏  | 102/143 [00:50<00:28,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/101.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  72%|███████▏  | 103/143 [00:50<00:30,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/103.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  73%|███████▎  | 104/143 [00:51<00:29,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/105.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  73%|███████▎  | 105/143 [00:52<00:26,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/104.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  74%|███████▍  | 106/143 [00:52<00:24,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/106.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  75%|███████▍  | 107/143 [00:53<00:22,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/108.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  76%|███████▌  | 108/143 [00:53<00:20,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/107.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  76%|███████▌  | 109/143 [00:54<00:19,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/109.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  77%|███████▋  | 110/143 [00:55<00:19,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/111.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  78%|███████▊  | 111/143 [00:55<00:18,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/110.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  78%|███████▊  | 112/143 [00:58<00:39,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/112.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  79%|███████▉  | 113/143 [00:59<00:31,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/113.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  80%|███████▉  | 114/143 [00:59<00:26,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/114.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  80%|████████  | 115/143 [01:00<00:22,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/115.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  81%|████████  | 116/143 [01:00<00:19,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/116.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  82%|████████▏ | 117/143 [01:01<00:17,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/117.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  83%|████████▎ | 118/143 [01:01<00:16,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/118.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  83%|████████▎ | 119/143 [01:02<00:17,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/120.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  84%|████████▍ | 120/143 [01:03<00:17,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/119.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  85%|████████▍ | 121/143 [01:04<00:17,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/121.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  85%|████████▌ | 122/143 [01:05<00:15,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/123.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  86%|████████▌ | 123/143 [01:05<00:13,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/122.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  87%|████████▋ | 124/143 [01:06<00:12,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/124.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  87%|████████▋ | 125/143 [01:06<00:11,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/125.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  88%|████████▊ | 126/143 [01:07<00:10,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/126.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  89%|████████▉ | 127/143 [01:07<00:09,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/127.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  90%|████████▉ | 128/143 [01:08<00:08,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/128.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  90%|█████████ | 129/143 [01:08<00:07,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/129.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  91%|█████████ | 130/143 [01:09<00:07,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/130.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  92%|█████████▏| 131/143 [01:10<00:06,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/132.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  92%|█████████▏| 132/143 [01:10<00:06,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/131.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  93%|█████████▎| 133/143 [01:11<00:05,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/133.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  94%|█████████▎| 134/143 [01:11<00:04,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/135.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  94%|█████████▍| 135/143 [01:11<00:03,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/134.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull:  96%|█████████▌| 137/143 [01:12<00:02,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/136.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  97%|█████████▋| 138/143 [01:13<00:02,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/138.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  97%|█████████▋| 139/143 [01:13<00:01,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/139.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  98%|█████████▊| 140/143 [01:14<00:01,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/140.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  99%|█████████▊| 141/143 [01:14<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/141.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Slaty_Backed_Gull:  99%|█████████▉| 142/143 [01:15<00:00,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/142.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Slaty_Backed_Gull: 100%|██████████| 143/143 [01:16<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Slaty_Backed_Gull/143.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Processing 124 images in class 'Glaucous_Winged_Gull'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:   1%|          | 1/124 [00:00<01:49,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/001.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:   2%|▏         | 2/124 [00:01<01:49,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/005.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:   5%|▍         | 6/124 [00:02<00:36,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/006.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/007.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/008.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:   8%|▊         | 10/124 [00:02<00:18,  6.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/010.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/011.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/009.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/014.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  12%|█▏        | 15/124 [00:03<00:10, 10.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/013.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/012.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/016.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/015.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/017.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  15%|█▌        | 19/124 [00:03<00:07, 13.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/018.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/019.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/020.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/024.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/023.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  20%|██        | 25/124 [00:03<00:05, 18.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/022.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/026.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/025.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/030.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/028.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/033.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  25%|██▌       | 31/124 [00:04<00:11,  8.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/032.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/034.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/035.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/036.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  30%|██▉       | 37/124 [00:05<00:07, 12.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/040.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/039.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/041.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/043.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/042.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  35%|███▍      | 43/124 [00:05<00:04, 16.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/044.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/050.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/048.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/052.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  40%|███▉      | 49/124 [00:05<00:04, 18.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/053.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/051.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/054.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/055.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/058.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  42%|████▏     | 52/124 [00:05<00:03, 18.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/056.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/057.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/059.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/060.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  47%|████▋     | 58/124 [00:06<00:03, 19.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/062.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/063.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/061.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/066.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/064.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  49%|████▉     | 61/124 [00:06<00:03, 19.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/065.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/068.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/067.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/069.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/072.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  54%|█████▍    | 67/124 [00:06<00:02, 20.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/070.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/073.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/075.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/077.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  59%|█████▉    | 73/124 [00:06<00:02, 20.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/080.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/078.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/079.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/082.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/081.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  61%|██████▏   | 76/124 [00:06<00:02, 20.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/083.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/085.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/086.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/084.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/089.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  66%|██████▌   | 82/124 [00:07<00:02, 20.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/088.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/090.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/093.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/092.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  69%|██████▊   | 85/124 [00:07<00:01, 19.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/091.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/095.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/096.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/097.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  73%|███████▎  | 91/124 [00:07<00:01, 20.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/098.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/103.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/101.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/100.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/105.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  76%|███████▌  | 94/124 [00:07<00:01, 20.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/104.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/106.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/107.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/108.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  78%|███████▊  | 97/124 [00:08<00:03,  7.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/110.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/109.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  80%|███████▉  | 99/124 [00:09<00:04,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/113.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  83%|████████▎ | 103/124 [00:10<00:04,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/112.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/116.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/115.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  86%|████████▋ | 107/124 [00:11<00:02,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/114.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/118.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/120.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/117.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/122.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  91%|█████████ | 113/124 [00:11<00:01, 10.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/121.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/123.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/125.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/124.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/129.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  94%|█████████▎| 116/124 [00:11<00:00, 12.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/126.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/127.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/131.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/130.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/134.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[final_model_vit_20250213] Class: Glaucous_Winged_Gull:  98%|█████████▊| 122/124 [00:11<00:00, 16.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/133.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/132.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/135.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/137.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n",
            "Error processing /content/drive/My Drive/FYP/Black Background/Glaucous_Winged_Gull/138.jpg with model InterpretableViT: chunk(): argument 'input' (position 1) must be Tensor, not tuple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[final_model_vit_20250213] Class: Glaucous_Winged_Gull: 100%|██████████| 124/124 [00:11<00:00, 10.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics saved to /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739/final_model_vit_20250213/final_model_vit_20250213_statistics.csv\n",
            "Confusion matrix saved to: /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739/final_model_vit_20250213/final_model_vit_20250213_confusion_matrix.png\n",
            "Model 'final_model_vit_20250213' (loaded as InterpretableViT) accuracy: 0.00%\n",
            "Completed processing with model: final_model_vit_20250213\n",
            "\n",
            "All processing complete. Results saved to /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739\n",
            "Generated combined model comparison reports in /content/drive/My Drive/FYP/Results/ViT_Interpretability/20250309_065739\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import timm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Model Architectures\n",
        "# ----------------------------\n",
        "\n",
        "# (a) InterpretableViT: Uses the [CLS] token and an attention over patch tokens.\n",
        "class InterpretableViT(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3, hidden_dim=512):\n",
        "        \"\"\"\n",
        "        This model uses a pre-trained ViT backbone and removes its original classification head.\n",
        "        It then:\n",
        "          - Extracts the [CLS] token (which is already well-trained)\n",
        "          - Computes a learned attention over patch tokens (the remaining tokens)\n",
        "          - Aggregates the patch tokens via a weighted sum\n",
        "          - Concatenates the CLS token and the weighted patch summary\n",
        "          - Feeds the combined representation through a custom MLP classifier\n",
        "        The attention weights are returned for later visualization.\n",
        "        \"\"\"\n",
        "        super(InterpretableViT, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()  # Remove the original classification head\n",
        "\n",
        "        # Determine embedding dimension (most ViT models have an attribute 'embed_dim')\n",
        "        self.embed_dim = self.vit.embed_dim if hasattr(self.vit, 'embed_dim') else 768\n",
        "\n",
        "        # Attention layer on patch tokens (ignoring the CLS token)\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Classifier head: use both the CLS token and a weighted average of patch tokens.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim * 2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.embed_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get token embeddings from ViT; output shape: (B, N+1, embed_dim)\n",
        "        tokens = self.vit.forward_features(x)\n",
        "        # The first token is the [CLS] token\n",
        "        cls_token = tokens[:, 0, :]  # Shape: (B, embed_dim)\n",
        "        # Remaining tokens are patch tokens\n",
        "        patch_tokens = tokens[:, 1:, :]  # Shape: (B, N, embed_dim)\n",
        "        # Compute attention scores over patch tokens\n",
        "        attn_scores = self.attention_layer(patch_tokens)  # (B, N, 1)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)    # (B, N, 1)\n",
        "        # Weighted average of patch tokens\n",
        "        weighted_patch = torch.sum(attn_weights * patch_tokens, dim=1)  # (B, embed_dim)\n",
        "        # Combine the CLS token and weighted patch representation\n",
        "        combined = torch.cat([cls_token, weighted_patch], dim=1)  # (B, 2*embed_dim)\n",
        "        logits = self.classifier(combined)  # (B, 2)\n",
        "        return logits, attn_weights  # Return both logits and attention weights\n",
        "\n",
        "# (b) EnhancedViT: Pools patch tokens with an attention mechanism then classifies.\n",
        "class EnhancedViT(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3, hidden_dim=512):\n",
        "        \"\"\"\n",
        "        Initializes the enhanced ViT model.\n",
        "          - Loads a pre-trained ViT backbone.\n",
        "          - Removes the original classification head.\n",
        "          - Adds an attention mechanism to pool patch tokens.\n",
        "          - Adds a custom MLP classifier head.\n",
        "        \"\"\"\n",
        "        super(EnhancedViT, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()\n",
        "\n",
        "        # Get the embedding dimension (most timm ViT models have 'embed_dim')\n",
        "        if hasattr(self.vit, 'embed_dim'):\n",
        "            self.embed_dim = self.vit.embed_dim\n",
        "        else:\n",
        "            self.embed_dim = self.vit.head.in_features\n",
        "\n",
        "        # Attention mechanism: compute an attention score for each token (patch)\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Custom classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, 2)  # Binary classification (2 classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = self.vit.forward_features(x)  # Shape: [B, num_tokens, embed_dim]\n",
        "        attn_scores = self.attention_layer(tokens)  # Shape: [B, num_tokens, 1]\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        weighted_feature = torch.sum(attn_weights * tokens, dim=1)  # Shape: [B, embed_dim]\n",
        "        out = self.classifier(weighted_feature)\n",
        "        return out\n",
        "\n",
        "# (c) ViTModified: A simple modification that replaces the classification head.\n",
        "class ViTModified(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTModified, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        num_ftrs = self.vit.head.in_features\n",
        "        self.vit.head = nn.Linear(num_ftrs, 2)  # Output 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Utility Functions for Image Processing\n",
        "# ----------------------------\n",
        "def preprocess_image(image_path, device, normalize=True):\n",
        "    \"\"\"\n",
        "    Loads an image and applies transformations.\n",
        "    Returns a tensor of shape (1, 3, 224, 224) on the specified device.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "def get_original_image(image_path, size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Loads and resizes an image without normalization for visualization.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize(size, Image.LANCZOS)\n",
        "    return np.array(image)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Interpretability Analyzers (DeepLIFT, LRP, Saliency)\n",
        "# ----------------------------\n",
        "from captum.attr import DeepLift, LayerIntegratedGradients\n",
        "\n",
        "class DeepLIFTAnalyzer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.deep_lift = DeepLift(self.model)\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        if target_class is None:\n",
        "            with torch.no_grad():\n",
        "                output = self.model(image_tensor)\n",
        "                # Handle tuple output (e.g. from InterpretableViT)\n",
        "                if isinstance(output, tuple):\n",
        "                    logits = output[0]\n",
        "                else:\n",
        "                    logits = output\n",
        "                target_class = logits.argmax().item()\n",
        "        baseline = torch.zeros_like(image_tensor).to(self.device)\n",
        "        attribution = self.deep_lift.attribute(\n",
        "            image_tensor,\n",
        "            baselines=baseline,\n",
        "            target=target_class\n",
        "        )\n",
        "        attr = attribution.detach().cpu().numpy()[0]\n",
        "        if attr.ndim == 3:\n",
        "            if attr.shape[0] == 1:\n",
        "                attr = attr.squeeze(0)\n",
        "            else:\n",
        "                attr = np.transpose(attr, (1, 2, 0))\n",
        "                if attr.shape[2] not in [1, 3]:\n",
        "                    attr = np.mean(attr, axis=2)\n",
        "                else:\n",
        "                    attr = np.mean(np.abs(attr), axis=2)\n",
        "        attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "        return attr\n",
        "\n",
        "class LRPAnalyzer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.layer_ig = LayerIntegratedGradients(self.model, self.model.vit.patch_embed)\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        if target_class is None:\n",
        "            with torch.no_grad():\n",
        "                output = self.model(image_tensor)\n",
        "                if isinstance(output, tuple):\n",
        "                    logits = output[0]\n",
        "                else:\n",
        "                    logits = output\n",
        "                target_class = logits.argmax().item()\n",
        "        attribution, _ = self.layer_ig.attribute(\n",
        "            image_tensor,\n",
        "            target=target_class,\n",
        "            n_steps=50,\n",
        "            return_convergence_delta=True\n",
        "        )\n",
        "        attr = attribution.detach().cpu().numpy()[0]\n",
        "        if attr.ndim == 3:\n",
        "            if attr.shape[0] == 1:\n",
        "                attr = attr.squeeze(0)\n",
        "            else:\n",
        "                attr = np.transpose(attr, (1, 2, 0))\n",
        "                if attr.shape[2] not in [1, 3]:\n",
        "                    attr = np.mean(attr, axis=2)\n",
        "                else:\n",
        "                    attr = np.mean(np.abs(attr), axis=2)\n",
        "        attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "        return attr\n",
        "\n",
        "class SaliencyMapAnalyzer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def generate_attribution(self, image_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        image_tensor_with_grad = image_tensor.clone().detach().requires_grad_(True)\n",
        "        output = self.model(image_tensor_with_grad)\n",
        "        if isinstance(output, tuple):\n",
        "            logits = output[0]\n",
        "        else:\n",
        "            logits = output\n",
        "        if target_class is None:\n",
        "            target_class = logits.argmax().item()\n",
        "        self.model.zero_grad()\n",
        "        logits[0, target_class].backward()\n",
        "        gradients = image_tensor_with_grad.grad.data\n",
        "        saliency, _ = torch.max(gradients.abs(), dim=1)\n",
        "        saliency = saliency.squeeze().cpu().numpy()\n",
        "        saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "        return saliency\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Visualization & Saving Functions\n",
        "# ----------------------------\n",
        "def save_visualization(original_image, attribution_maps, class_name, predicted_class, confidence, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    n_panels = 1 + len(attribution_maps)\n",
        "    fig, axes = plt.subplots(1, n_panels, figsize=(n_panels * 4, 4))\n",
        "    axes[0].imshow(original_image)\n",
        "    axes[0].set_title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n",
        "    axes[0].axis('off')\n",
        "    for i, (name, attr_map) in enumerate(attribution_maps.items(), 1):\n",
        "        if name == 'Saliency':\n",
        "            axes[i].imshow(attr_map, cmap='hot')\n",
        "            axes[i].set_title(f\"{name} Map\")\n",
        "        else:\n",
        "            heatmap = cv2.applyColorMap(np.uint8(255 * attr_map), cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "            if original_image.shape[:2] != heatmap.shape[:2]:\n",
        "                original_resized = cv2.resize(original_image, (heatmap.shape[1], heatmap.shape[0]))\n",
        "            else:\n",
        "                original_resized = original_image\n",
        "            overlay = cv2.addWeighted(original_resized, 0.6, heatmap, 0.4, 0)\n",
        "            axes[i].imshow(overlay)\n",
        "            axes[i].set_title(f\"{name} Overlay\")\n",
        "        axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_statistics_csv(stat_list, csv_path):\n",
        "    df = pd.DataFrame(stat_list)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Statistics saved to {csv_path}\")\n",
        "\n",
        "def generate_confusion_matrix(stats_list, save_path, class_names):\n",
        "    try:\n",
        "        df = pd.DataFrame(stats_list)\n",
        "        if not df.empty:\n",
        "            confusion = pd.crosstab(df['true_class'], df['predicted_class'],\n",
        "                                    rownames=['True'], colnames=['Predicted'])\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "            plt.title(\"Confusion Matrix\")\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Confusion matrix saved to: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating confusion matrix: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Main Processing Function with Dynamic Architecture Selection\n",
        "# ----------------------------\n",
        "def process_dataset_with_models(dataset_path, model_paths, output_dir_base,\n",
        "                                class_names=['Glaucous_Winged_Gull', 'Slaty_Backed_Gull']):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    main_output_dir = Path(output_dir_base) / timestamp\n",
        "    main_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Main output directory: {main_output_dir}\")\n",
        "\n",
        "    for model_idx, model_path in enumerate(model_paths, 1):\n",
        "        model_name = Path(model_path).stem\n",
        "        print(f\"\\n[{model_idx}/{len(model_paths)}] Processing with model file: {model_name}\")\n",
        "        # Try loading the checkpoint with each architecture until one succeeds.\n",
        "        model_loaded = False\n",
        "        for arch in [ViTModified, InterpretableViT, EnhancedViT]:\n",
        "            try:\n",
        "                model = arch().to(device)\n",
        "                model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "                print(f\"Successfully loaded checkpoint with {arch.__name__}\")\n",
        "                model_loaded = True\n",
        "                arch_used = arch.__name__\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model from {model_path} with {arch.__name__}: {e}\")\n",
        "        if not model_loaded:\n",
        "            print(f\"Skipping model {model_path} as it could not be loaded with any known architecture.\")\n",
        "            continue\n",
        "\n",
        "        # Set up output directories for this model\n",
        "        model_output_dir = main_output_dir / model_name\n",
        "        deeplift_dir = model_output_dir / \"deeplift_visualizations\"\n",
        "        lrp_dir = model_output_dir / \"lrp_visualizations\"\n",
        "        saliency_dir = model_output_dir / \"saliency_visualizations\"\n",
        "        correct_images_dir = model_output_dir / \"correct_images\"\n",
        "        os.makedirs(deeplift_dir, exist_ok=True)\n",
        "        os.makedirs(lrp_dir, exist_ok=True)\n",
        "        os.makedirs(saliency_dir, exist_ok=True)\n",
        "        os.makedirs(correct_images_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize interpreters\n",
        "        deeplift_analyzer = DeepLIFTAnalyzer(model, device)\n",
        "        lrp_analyzer = LRPAnalyzer(model, device)\n",
        "        saliency_analyzer = SaliencyMapAnalyzer(model, device)\n",
        "        model_stats = []\n",
        "\n",
        "        # Process images from each class folder\n",
        "        for class_folder in Path(dataset_path).iterdir():\n",
        "            if not class_folder.is_dir():\n",
        "                continue\n",
        "            true_class = class_folder.name\n",
        "            if true_class not in class_names:\n",
        "                print(f\"Skipping folder '{true_class}' as it is not in class_names\")\n",
        "                continue\n",
        "\n",
        "            class_deeplift_dir = deeplift_dir / true_class\n",
        "            class_lrp_dir = lrp_dir / true_class\n",
        "            class_saliency_dir = saliency_dir / true_class\n",
        "            class_correct_dir = correct_images_dir / true_class\n",
        "\n",
        "            os.makedirs(class_deeplift_dir, exist_ok=True)\n",
        "            os.makedirs(class_lrp_dir, exist_ok=True)\n",
        "            os.makedirs(class_saliency_dir, exist_ok=True)\n",
        "            os.makedirs(class_correct_dir, exist_ok=True)\n",
        "\n",
        "            image_files = list(class_folder.glob(\"*.[jJ][pP][gG]\")) + \\\n",
        "                          list(class_folder.glob(\"*.[jJ][pP][eE][gG]\")) + \\\n",
        "                          list(class_folder.glob(\"*.[pP][nN][gG]\"))\n",
        "            print(f\"Processing {len(image_files)} images in class '{true_class}'...\")\n",
        "\n",
        "            for image_path in tqdm(image_files, desc=f\"[{model_name}] Class: {true_class}\"):\n",
        "                try:\n",
        "                    image_tensor = preprocess_image(str(image_path), device)\n",
        "                    original_image = get_original_image(str(image_path))\n",
        "                    with torch.no_grad():\n",
        "                        output = model(image_tensor)\n",
        "                        # Handle models that return a tuple (e.g., InterpretableViT)\n",
        "                        if isinstance(output, tuple):\n",
        "                            logits = output[0]\n",
        "                        else:\n",
        "                            logits = output\n",
        "                        probs = F.softmax(logits, dim=1)\n",
        "                        predicted_class_idx = torch.argmax(probs, dim=1).item()\n",
        "                        confidence = probs[0, predicted_class_idx].item()\n",
        "                    is_correct = (predicted_class_idx == class_names.index(true_class))\n",
        "                    if is_correct:\n",
        "                        deeplift_attr = deeplift_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                        lrp_attr = lrp_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                        saliency_attr = saliency_analyzer.generate_attribution(image_tensor, predicted_class_idx)\n",
        "                        attributions = {\n",
        "                            'DeepLIFT': deeplift_attr,\n",
        "                            'LRP': lrp_attr,\n",
        "                            'Saliency': saliency_attr\n",
        "                        }\n",
        "                        # Save visualizations\n",
        "                        save_visualization(\n",
        "                            original_image,\n",
        "                            {'DeepLIFT': deeplift_attr},\n",
        "                            true_class,\n",
        "                            class_names[predicted_class_idx],\n",
        "                            confidence,\n",
        "                            str(class_deeplift_dir / f\"{image_path.stem}_deeplift.png\")\n",
        "                        )\n",
        "                        save_visualization(\n",
        "                            original_image,\n",
        "                            {'LRP': lrp_attr},\n",
        "                            true_class,\n",
        "                            class_names[predicted_class_idx],\n",
        "                            confidence,\n",
        "                            str(class_lrp_dir / f\"{image_path.stem}_lrp.png\")\n",
        "                        )\n",
        "                        save_visualization(\n",
        "                            original_image,\n",
        "                            {'Saliency': saliency_attr},\n",
        "                            true_class,\n",
        "                            class_names[predicted_class_idx],\n",
        "                            confidence,\n",
        "                            str(class_saliency_dir / f\"{image_path.stem}_saliency.png\")\n",
        "                        )\n",
        "                        combined_dir = model_output_dir / \"combined_visualizations\" / true_class\n",
        "                        os.makedirs(combined_dir, exist_ok=True)\n",
        "                        save_visualization(\n",
        "                            original_image,\n",
        "                            attributions,\n",
        "                            true_class,\n",
        "                            class_names[predicted_class_idx],\n",
        "                            confidence,\n",
        "                            str(combined_dir / f\"{image_path.stem}_combined.png\")\n",
        "                        )\n",
        "                        shutil.copy2(str(image_path), str(class_correct_dir / image_path.name))\n",
        "                    model_stats.append({\n",
        "                        \"model\": arch_used,\n",
        "                        \"file\": str(image_path),\n",
        "                        \"true_class\": true_class,\n",
        "                        \"predicted_class\": class_names[predicted_class_idx],\n",
        "                        \"confidence\": confidence,\n",
        "                        \"correct\": is_correct\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {image_path} with model {arch_used}: {e}\")\n",
        "\n",
        "        stats_csv_path = model_output_dir / f\"{model_name}_statistics.csv\"\n",
        "        save_statistics_csv(model_stats, str(stats_csv_path))\n",
        "        confusion_matrix_path = model_output_dir / f\"{model_name}_confusion_matrix.png\"\n",
        "        generate_confusion_matrix(model_stats, str(confusion_matrix_path), class_names)\n",
        "        df = pd.DataFrame(model_stats)\n",
        "        accuracy = df['correct'].mean() * 100\n",
        "        print(f\"Model '{model_name}' (loaded as {arch_used}) accuracy: {accuracy:.2f}%\")\n",
        "        with open(str(model_output_dir / f\"{model_name}_summary.txt\"), \"w\") as f:\n",
        "            f.write(f\"Model: {model_name} (Architecture: {arch_used})\\n\")\n",
        "            f.write(f\"Processed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Overall Accuracy: {accuracy:.2f}%\\n\\n\")\n",
        "            f.write(\"Per-Class Metrics:\\n\")\n",
        "            for class_name in class_names:\n",
        "                class_df = df[df['true_class'] == class_name]\n",
        "                class_acc = class_df['correct'].mean() * 100 if len(class_df) > 0 else 0\n",
        "                f.write(f\"  {class_name}: {class_acc:.2f}% ({len(class_df[class_df['correct']])}/{len(class_df)} correct)\\n\")\n",
        "        print(f\"Completed processing with model: {model_name}\")\n",
        "\n",
        "    print(f\"\\nAll processing complete. Results saved to {main_output_dir}\")\n",
        "\n",
        "    # Optionally, generate a combined report across models.\n",
        "    try:\n",
        "        all_stats = []\n",
        "        for model_path in model_paths:\n",
        "            model_name = Path(model_path).stem\n",
        "            stats_path = main_output_dir / model_name / f\"{model_name}_statistics.csv\"\n",
        "            if stats_path.exists():\n",
        "                df = pd.read_csv(stats_path)\n",
        "                all_stats.append(df)\n",
        "        if all_stats:\n",
        "            combined_df = pd.concat(all_stats)\n",
        "            combined_stats_path = main_output_dir / \"all_models_comparison.csv\"\n",
        "            combined_df.to_csv(combined_stats_path, index=False)\n",
        "            model_summary = combined_df.groupby('model')['correct'].agg(['mean', 'count']).reset_index()\n",
        "            model_summary['accuracy'] = model_summary['mean'] * 100\n",
        "            model_summary = model_summary.drop('mean', axis=1)\n",
        "            model_summary = model_summary.rename(columns={'count': 'total_images', 'accuracy': 'accuracy_percent'})\n",
        "            model_summary = model_summary.sort_values('accuracy_percent', ascending=False)\n",
        "            model_summary.to_csv(main_output_dir / \"model_accuracy_comparison.csv\", index=False)\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(x='model', y='accuracy_percent', data=model_summary)\n",
        "            plt.title('Model Accuracy Comparison')\n",
        "            plt.xlabel('Model')\n",
        "            plt.ylabel('Accuracy (%)')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(main_output_dir / \"model_accuracy_comparison.png\", dpi=300)\n",
        "            plt.close()\n",
        "            print(f\"Generated combined model comparison reports in {main_output_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating combined summary: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Google Colab Main Execution (if applicable)\n",
        "# ----------------------------\n",
        "# Install required packages (if needed)\n",
        "!pip install timm captum\n",
        "\n",
        "# Mount Google Drive (specific to Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your dataset and output paths (update these as needed)\n",
        "DATASET_PATH = \"/content/drive/My Drive/FYP/Black Background\"  # Contains class folders\n",
        "OUTPUT_BASE = \"/content/drive/My Drive/FYP/Results/ViT_Interpretability\"  # Base output directory\n",
        "CLASS_NAMES = [\"Glaucous_Winged_Gull\", \"Slaty_Backed_Gull\"]\n",
        "\n",
        "# List your model checkpoint paths (update these as needed)\n",
        "MODEL_PATHS = [\n",
        "    # \"/content/drive/My Drive/FYP/MODELS/VIT/VIT2_HQ2_20241222/latest_model_vit_20241222.pth\",\n",
        "    \"/content/drive/My Drive/FYP/MODELS/VIT/VIT2_HQ3_20250208/final_model_vit_20250208.pth\",\n",
        "    \"/content/drive/My Drive/FYP/MODELS/VIT/InterpretableViT_20250213/final_model_vit_20250213.pth\",\n",
        "]\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    print(f\"ERROR: Dataset path not found: {DATASET_PATH}\")\n",
        "else:\n",
        "    os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
        "    valid_models = []\n",
        "    for model_path in MODEL_PATHS:\n",
        "        if os.path.exists(model_path):\n",
        "            valid_models.append(model_path)\n",
        "        else:\n",
        "            print(f\"WARNING: Model not found: {model_path}\")\n",
        "    if not valid_models:\n",
        "        print(\"ERROR: No valid models found. Please check your model paths.\")\n",
        "    else:\n",
        "        print(f\"Found {len(valid_models)} valid models. Starting processing...\")\n",
        "        process_dataset_with_models(\n",
        "            dataset_path=DATASET_PATH,\n",
        "            model_paths=valid_models,\n",
        "            output_dir_base=OUTPUT_BASE,\n",
        "            class_names=CLASS_NAMES\n",
        "        )\n"
      ]
    }
  ]
}