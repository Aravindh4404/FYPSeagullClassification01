{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLNgvs-79TJv",
        "outputId": "84016085-ed4c-4988-c41d-e3e5212f4af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved with validation accuracy: 95.61%\n",
            "New best validation loss: 0.081745\n",
            "Epoch [1/30] - Train Loss: 0.331784, Val Loss: 0.081745, Val Acc: 95.61%, Val F1: 0.9741, Val ROC-AUC: 0.9933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-995e747a950a>:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [2/30] - Train Loss: 0.116234, Val Loss: 0.567935, Val Acc: 95.61%, Val F1: 0.9749, Val ROC-AUC: 0.9406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved with validation accuracy: 98.25%\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [3/30] - Train Loss: 0.190538, Val Loss: 0.191043, Val Acc: 98.25%, Val F1: 0.9897, Val ROC-AUC: 0.9660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [4/30] - Train Loss: 0.017226, Val Loss: 0.110120, Val Acc: 97.37%, Val F1: 0.9845, Val ROC-AUC: 0.9945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [5/30] - Train Loss: 0.057150, Val Loss: 0.102353, Val Acc: 95.61%, Val F1: 0.9749, Val ROC-AUC: 0.9982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved with validation accuracy: 99.12%\n",
            "New best validation loss: 0.018944\n",
            "Epoch [6/30] - Train Loss: 0.099987, Val Loss: 0.018944, Val Acc: 99.12%, Val F1: 0.9949, Val ROC-AUC: 0.9994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, average_precision_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Define the device for computation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive to save and load the model (if using Google Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = '/content/drive/My Drive/FYP'\n",
        "except ImportError:\n",
        "    base_path = '.'  # Use current directory if not in Colab\n",
        "\n",
        "# Define the folder to save model checkpoints\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "checkpoint_folder = f'{base_path}/VGGModel/HQ3latest_{date_str}/_valloss'\n",
        "os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "\n",
        "# Data Augmentation for Training Set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # VGG expects 224x224 input size\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Flip left/right\n",
        "    transforms.RandomVerticalFlip(p=0.3),  # Flip up/down\n",
        "    transforms.RandomRotation(degrees=10),  # Small rotation\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),  # Slight shifts\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),  # Small color changes\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Simple resizing for validation and test sets\n",
        "transform_val_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "data_path = f'{base_path}/Dataset/HQ3/train'\n",
        "test_data_path = f'{base_path}/Dataset/HQ3/test'\n",
        "train_dataset = datasets.ImageFolder(data_path, transform=transform_train)\n",
        "test_dataset = datasets.ImageFolder(test_data_path, transform=transform_val_test)\n",
        "\n",
        "# Split the dataset into 70% training and 30% validation\n",
        "train_size = int(0.85 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Set validation set transformation explicitly\n",
        "val_dataset.dataset.transform = transform_val_test\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Use Pre-trained VGG-16 model and modify it for binary classification\n",
        "class VGG16Modified(nn.Module):\n",
        "    def __init__(self, num_classes=2, dropout_rate=0.4):\n",
        "        super(VGG16Modified, self).__init__()\n",
        "        from torchvision.models import vgg16, VGG16_Weights\n",
        "\n",
        "        # Load pre-trained VGG16 model\n",
        "        self.vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Replace the classifier with a custom classification layer\n",
        "        num_ftrs = self.vgg.classifier[6].in_features\n",
        "        self.vgg.classifier[6] = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(num_ftrs, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vgg(x)\n",
        "\n",
        "# Initialize the VGG model\n",
        "model = VGG16Modified().to(device)\n",
        "\n",
        "# Define loss function and optimizer with L2 regularization (weight decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_scores):\n",
        "    \"\"\"Calculate various classification metrics.\"\"\"\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred) * 100\n",
        "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred, average='binary')\n",
        "    metrics['recall'] = recall_score(y_true, y_pred, average='binary')\n",
        "    metrics['f1'] = f1_score(y_true, y_pred, average='binary')\n",
        "    metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    # For ROC-AUC and PR-AUC, we need probabilities\n",
        "    # Ensure y_scores are probabilities of the positive class\n",
        "    metrics['roc_auc'] = roc_auc_score(y_true, y_scores)\n",
        "    metrics['pr_auc'] = average_precision_score(y_true, y_scores)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    \"\"\"Evaluate model and return predictions, true labels, losses and metrics.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predicted class and probability scores\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            pos_probs = probs[:, 1].cpu().numpy()  # Probability of positive class\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(pos_probs)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(all_labels, all_preds, all_scores)\n",
        "    metrics['loss'] = total_loss / len(loader)\n",
        "\n",
        "    return metrics, all_preds, all_labels, all_scores\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=30):\n",
        "    \"\"\"Train the model and track metrics with early stopping based on validation loss.\"\"\"\n",
        "    # Initialize lists to store metrics\n",
        "    metrics_df = pd.DataFrame(columns=['epoch', 'training_loss', 'val_loss', 'val_accuracy',\n",
        "                                       'val_balanced_accuracy', 'val_precision', 'val_recall',\n",
        "                                       'val_f1', 'val_mcc', 'val_roc_auc', 'val_pr_auc'])\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')  # Initialize best validation loss\n",
        "    checkpoint_model_path = os.path.join(checkpoint_folder, f\"checkpoint_model_vgg_{date_str}.pth\")\n",
        "    best_model_path = os.path.join(checkpoint_folder, f\"best_model_vgg_{date_str}.pth\")\n",
        "    early_stopping_path = os.path.join(checkpoint_folder, f\"early_stopping_model_vgg_{date_str}.pth\")\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5  # Number of epochs to wait for improvement\n",
        "    counter = 0\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if early_stop:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate training loss\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics, _, _, _ = evaluate(model, val_loader, criterion)\n",
        "        val_loss = val_metrics['loss']\n",
        "        val_acc = val_metrics['accuracy']\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc\n",
        "        }, checkpoint_model_path)\n",
        "\n",
        "        # Save best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        # Early stopping check based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            # Save the model with the best validation loss\n",
        "            torch.save(model.state_dict(), early_stopping_path)\n",
        "            print(f\"New best validation loss: {val_loss:.6f}\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"EarlyStopping counter: {counter} out of {patience}\")\n",
        "            if counter >= patience:\n",
        "                early_stop = True\n",
        "                print(f\"Early stopping triggered: validation loss didn't improve for {patience} epochs\")\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
        "              f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, \"\n",
        "              f\"Val Acc: {val_acc:.2f}%, \"\n",
        "              f\"Val F1: {val_metrics['f1']:.4f}, \"\n",
        "              f\"Val ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Add metrics to dataframe\n",
        "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
        "            'epoch': [epoch + 1],\n",
        "            'training_loss': [train_loss],\n",
        "            'val_loss': [val_loss],\n",
        "            'val_accuracy': [val_metrics['accuracy']],\n",
        "            'val_balanced_accuracy': [val_metrics['balanced_accuracy']],\n",
        "            'val_precision': [val_metrics['precision']],\n",
        "            'val_recall': [val_metrics['recall']],\n",
        "            'val_f1': [val_metrics['f1']],\n",
        "            'val_mcc': [val_metrics['mcc']],\n",
        "            'val_roc_auc': [val_metrics['roc_auc']],\n",
        "            'val_pr_auc': [val_metrics['pr_auc']]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "    # Early stopping information\n",
        "    if early_stop:\n",
        "        print(f\"Training stopped early at epoch {epoch}\")\n",
        "        early_stopping_info = pd.DataFrame({\n",
        "            'stopped_epoch': [epoch],\n",
        "            'best_val_loss': [best_val_loss],\n",
        "            'patience': [patience]\n",
        "        })\n",
        "        early_stopping_info.to_csv(os.path.join(checkpoint_folder, f\"early_stopping_info_{date_str}.csv\"), index=False)\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_csv_path = os.path.join(checkpoint_folder, f\"metrics_{date_str}.csv\")\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"Training metrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Plot and save training curves\n",
        "    plot_training_curves(metrics_df, checkpoint_folder)\n",
        "\n",
        "    # Plot confusion matrix for best model (both accuracy-based and loss-based)\n",
        "    print(\"\\nEvaluating best accuracy model:\")\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    val_metrics_acc, val_preds_acc, val_labels_acc, _ = evaluate(model, val_loader, criterion)\n",
        "    plot_confusion_matrix(val_labels_acc, val_preds_acc, ['Normal', 'Tumor'], checkpoint_folder, \"validation_best_acc\")\n",
        "\n",
        "    print(\"\\nEvaluating early stopping model (best validation loss):\")\n",
        "    model.load_state_dict(torch.load(early_stopping_path))\n",
        "    val_metrics_loss, val_preds_loss, val_labels_loss, _ = evaluate(model, val_loader, criterion)\n",
        "    plot_confusion_matrix(val_labels_loss, val_preds_loss, ['Normal', 'Tumor'], checkpoint_folder, \"validation_early_stop\")\n",
        "\n",
        "    # Compare the models\n",
        "    print(f\"\\nComparison - Best Accuracy Model vs Early Stopping Model:\")\n",
        "    print(f\"Best Accuracy Model - Val Acc: {val_metrics_acc['accuracy']:.2f}%, Val Loss: {val_metrics_acc['loss']:.6f}\")\n",
        "    print(f\"Early Stopping Model - Val Acc: {val_metrics_loss['accuracy']:.2f}%, Val Loss: {val_metrics_loss['loss']:.6f}\")\n",
        "\n",
        "    return metrics_df, early_stop, epoch\n",
        "\n",
        "def plot_training_curves(metrics_df, save_folder):\n",
        "    \"\"\"Plot and save training curves.\"\"\"\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Plot 1: Training vs Validation Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['training_loss'], 'b-', label='Training Loss')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], 'r-', label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Validation Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_accuracy'], 'g-')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: Validation F1, Precision, Recall\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_f1'], 'c-', label='F1 Score')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_precision'], 'm-', label='Precision')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_recall'], 'y-', label='Recall')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation F1, Precision, Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 4: Validation ROC-AUC and PR-AUC\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_roc_auc'], 'b-', label='ROC-AUC')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_pr_auc'], 'r-', label='PR-AUC')\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['val_mcc'], 'g-', label='MCC')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation ROC-AUC, PR-AUC, and MCC')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_folder, f\"training_curves_{date_str}.png\"), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, save_folder, dataset_name):\n",
        "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f'Confusion Matrix - {dataset_name.capitalize()} Set')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_folder, f\"confusion_matrix_{dataset_name}_{date_str}.png\"), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def test_model(model, test_loader, criterion, model_name=\"best\"):\n",
        "    \"\"\"Evaluate the model on the test set and visualize results.\"\"\"\n",
        "    print(f\"\\nEvaluating {model_name} model on test set...\")\n",
        "    test_metrics, test_preds, test_labels, test_scores = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # Print test metrics\n",
        "    print(f\"\\nTest Set Metrics ({model_name} model):\")\n",
        "    print(f\"Loss: {test_metrics['loss']:.6f}\")\n",
        "    print(f\"Accuracy: {test_metrics['accuracy']:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {test_metrics['balanced_accuracy']:.4f}\")\n",
        "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"MCC: {test_metrics['mcc']:.4f}\")\n",
        "    print(f\"ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"PR-AUC: {test_metrics['pr_auc']:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix for test set\n",
        "    plot_confusion_matrix(test_labels, test_preds, ['Normal', 'Tumor'], checkpoint_folder, f\"test_{model_name}\")\n",
        "\n",
        "    # Save test metrics to CSV\n",
        "    test_metrics_df = pd.DataFrame({metric: [value] for metric, value in test_metrics.items()})\n",
        "    test_metrics_df.to_csv(os.path.join(checkpoint_folder, f\"test_metrics_{model_name}_{date_str}.csv\"), index=False)\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "# ------------------- Main Training & Testing Routine -------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model and collect metrics\n",
        "    metrics_df, early_stopped, stopped_epoch = train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=30)\n",
        "\n",
        "    # Load best accuracy model for testing\n",
        "    best_model_path = os.path.join(checkpoint_folder, f\"best_model_vgg_{date_str}.pth\")\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    test_metrics_acc = test_model(model, test_loader, criterion, \"best_accuracy\")\n",
        "\n",
        "    # Load early stopping model for testing (if different from best accuracy model)\n",
        "    early_stopping_path = os.path.join(checkpoint_folder, f\"early_stopping_model_vgg_{date_str}.pth\")\n",
        "    model.load_state_dict(torch.load(early_stopping_path))\n",
        "    test_metrics_es = test_model(model, test_loader, criterion, \"early_stopping\")\n",
        "\n",
        "    # Compare test results between models\n",
        "    print(\"\\nTest Set Comparison - Best Accuracy Model vs Early Stopping Model:\")\n",
        "    print(f\"Best Accuracy Model - Test Acc: {test_metrics_acc['accuracy']:.2f}%, Test Loss: {test_metrics_acc['loss']:.6f}\")\n",
        "    print(f\"Early Stopping Model - Test Acc: {test_metrics_es['accuracy']:.2f}%, Test Loss: {test_metrics_es['loss']:.6f}\")\n",
        "\n",
        "    if early_stopped:\n",
        "        print(f\"\\nEarly stopping was triggered after {stopped_epoch} epochs\")\n",
        "    else:\n",
        "        print(\"\\nTraining completed without early stopping\")\n",
        "\n",
        "    print(f\"\\nTraining and evaluation complete. Results saved to {checkpoint_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIARAkiO-9ev",
        "outputId": "df5541b5-d2b4-41a7-a9aa-f4cdd2080026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: sympy 1.13.1\n",
            "Uninstalling sympy-1.13.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/isympy\n",
            "    /usr/local/lib/python3.11/dist-packages/isympy.py\n",
            "    /usr/local/lib/python3.11/dist-packages/sympy-1.13.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/sympy/*\n",
            "    /usr/local/share/man/man1/isympy.1\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled sympy-1.13.1\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Installing collected packages: sympy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.14.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 torch-2.7.0 torchvision-0.22.0 triton-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall sympy\n",
        "!pip install sympy --upgrade\n",
        "!pip install torch torchvision --upgrade"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}