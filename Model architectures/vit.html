<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 20px;
      margin: 0;
    }
    
    .architecture-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
      margin-bottom: 60px;
      border-bottom: 1px solid #ddd;
    }
    
    .diagram {
      display: flex;
      flex-direction: column;
      align-items: center;
      margin-top: 20px;
      width: 100%;
    }
    
    .model-row {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
      flex-wrap: wrap;
      justify-content: center;
    }
    
    .block {
      margin: 0 5px;
      text-align: center;
    }
    
    .block-label {
      font-size: 12px;
      margin-bottom: 5px;
      color: #333;
      max-width: 80px;
      text-align: center;
    }
    
    .arrow {
      width: 20px;
      text-align: center;
      font-size: 20px;
    }
    
    /* Common components */
    .basic-block {
      width: 50px;
      height: 70px;
      border: 2px solid #000;
      background-color: white;
    }
    
    .vit-block {
      width: 70px;
      height: 100px;
      border: 2px solid #084;
      background-color: white;
    }
    
    .attention-block {
      width: 70px;
      height: 100px;
      border: 2px solid #00c;
      background-color: white;
      position: relative;
    }
    
    .attention-internal {
      position: absolute;
      width: 40px;
      height: 60px;
      border: 1px solid #00c;
      left: 15px;
      top: 20px;
      background-color: #e6e6ff;
    }
    
    .cls-token {
      width: 20px;
      height: 100px;
      border: 2px solid #f80;
      background-color: white;
    }
    
    .patch-tokens {
      width: 50px;
      height: 100px;
      border: 2px solid #f80;
      background-color: white;
    }
    
    .dropout {
      width: 50px;
      height: 70px;
      border: 2px dashed #909;
      background-color: white;
    }
    
    .ln {
      width: 50px;
      height: 70px;
      border: 2px solid #0a0;
      background-color: white;
    }
    
    .linear {
      width: 50px;
      height: 70px;
      border: 2px solid #d00;
      background-color: white;
    }
    
    .mlp {
      width: 50px;
      height: 100px;
      border: 2px solid #fa0;
      background-color: white;
    }
    
    .legend {
      display: flex;
      flex-wrap: wrap;
      margin-top: 20px;
      justify-content: center;
      width: 100%;
      max-width: 800px;
    }
    
    .legend-item {
      display: flex;
      align-items: center;
      margin: 8px 15px;
    }
    
    .legend-box {
      width: 20px;
      height: 20px;
      margin-right: 10px;
      border: 2px solid black;
      background-color: white;
    }
    
    .legend-box-green {
      border-color: #0a0;
    }
    
    .legend-box-blue {
      border-color: #00c;
    }
    
    .legend-box-orange {
      border-color: #f80;
    }
    
    .legend-box-purple {
      border: 2px dashed #909;
    }
    
    .legend-box-red {
      border-color: #d00;
    }
    
    .legend-box-yellow {
      border-color: #fa0;
    }
    
    .legend-box-teal {
      border-color: #084;
    }
    
    .legend-text {
      font-size: 14px;
    }
    
    .implementation-details {
      margin-top: 30px;
      font-size: 14px;
      max-width: 600px;
      border: 1px solid #ddd;
      padding: 15px;
      border-radius: 5px;
    }
    
    .implementation-details h3 {
      margin-top: 0;
    }
    
    .implementation-details table {
      width: 100%;
      border-collapse: collapse;
    }
    
    .implementation-details td {
      padding: 5px;
      vertical-align: top;
    }
    
    .implementation-details td:first-child {
      width: 40%;
      font-weight: bold;
    }
    
    .diagram-title {
      font-size: 22px;
      font-weight: bold;
      margin-bottom: 20px;
    }
    
    .diagrams-title {
      font-size: 24px;
      font-weight: bold;
      margin-bottom: 40px;
      text-align: center;
    }

    .divider {
      height: 60px;
    }
  </style>
</head>
<body>
  <h1 class="diagrams-title">Vision Transformer Architectures for Binary Classification</h1>
  
  <!-- 1. Modified Vision Transformer (ViT) -->
  <div class="architecture-container">
    <div class="diagram-title">1. Modified Vision Transformer (ViT)</div>
    
    <div class="diagram">
      <div class="model-row">
        <!-- Input Image -->
        <div class="block">
          <div class="block-label">Input Image</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Patch + Position Embeddings -->
        <div class="block">
          <div class="block-label">Patch + Position Embeddings</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- ViT Encoder -->
        <div class="block">
          <div class="block-label">Pre-trained ViT Encoder</div>
          <div class="vit-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- CLS Token -->
        <div class="block">
          <div class="block-label">[CLS] Token</div>
          <div class="cls-token"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Final Classifier -->
        <div class="block">
          <div class="block-label">Linear Classifier (2 classes)</div>
          <div class="linear"></div>
        </div>
      </div>
      
      <div class="legend">
        <div class="legend-item">
          <div class="legend-box"></div>
          <div class="legend-text">Basic Blocks</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-teal"></div>
          <div class="legend-text">Pre-trained ViT</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-orange"></div>
          <div class="legend-text">Tokens</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-red"></div>
          <div class="legend-text">Linear Layer</div>
        </div>
      </div>
      
      <div class="implementation-details">
        <h3>Implementation Details</h3>
        <table>
          <tr>
            <td>Architecture</td>
            <td>Modified Vision Transformer (ViT)</td>
          </tr>
          <tr>
            <td>Base model</td>
            <td>vit_base_patch16_224 (pretrained)</td>
          </tr>
          <tr>
            <td>Modification</td>
            <td>Classification head replaced with a binary classifier</td>
          </tr>
          <tr>
            <td>Classification head</td>
            <td>Linear layer mapping from embedding dimension to 2 classes</td>
          </tr>
          <tr>
            <td>Optimization</td>
            <td>AdamW with weight decay 1e-4, LR 0.0001</td>
          </tr>
          <tr>
            <td>LR scheduler</td>
            <td>ReduceLROnPlateau (patience=3, factor=0.1)</td>
          </tr>
        </table>
      </div>
    </div>
  </div>

  <div class="divider"></div>
  
  <!-- 2. Enhanced Vision Transformer -->
  <div class="architecture-container">
    <div class="diagram-title">2. Enhanced Vision Transformer with Attention Pooling</div>
    
    <div class="diagram">
      <div class="model-row">
        <!-- Input Image -->
        <div class="block">
          <div class="block-label">Input Image</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Patch + Position Embeddings -->
        <div class="block">
          <div class="block-label">Patch + Position Embeddings</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- ViT Encoder -->
        <div class="block">
          <div class="block-label">Pre-trained ViT Encoder</div>
          <div class="vit-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Patch Tokens -->
        <div class="block">
          <div class="block-label">Patch Token Embeddings</div>
          <div class="patch-tokens"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Attention Layer -->
        <div class="block">
          <div class="block-label">Attention Layer</div>
          <div class="attention-block">
            <div class="attention-internal"></div>
          </div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Weighted Sum -->
        <div class="block">
          <div class="block-label">Weighted Feature Vector</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
      </div>
      
      <div class="model-row">
        <!-- Classifier Head - Part 1 -->
        <div class="block">
          <div class="block-label">Layer Norm</div>
          <div class="ln"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Dropout</div>
          <div class="dropout"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Linear Layer</div>
          <div class="linear"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">ReLU</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Dropout</div>
          <div class="dropout"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Linear Layer (2 classes)</div>
          <div class="linear"></div>
        </div>
      </div>
      
      <div class="legend">
        <div class="legend-item">
          <div class="legend-box"></div>
          <div class="legend-text">Basic Blocks</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-teal"></div>
          <div class="legend-text">Pre-trained ViT</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-orange"></div>
          <div class="legend-text">Token Embeddings</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-blue"></div>
          <div class="legend-text">Attention Mechanism</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-green"></div>
          <div class="legend-text">Layer Normalization</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-purple"></div>
          <div class="legend-text">Dropout</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-red"></div>
          <div class="legend-text">Linear Layer</div>
        </div>
      </div>
      
      <div class="implementation-details">
        <h3>Implementation Details</h3>
        <table>
          <tr>
            <td>Architecture</td>
            <td>Enhanced Vision Transformer with Attention Pooling</td>
          </tr>
          <tr>
            <td>Base model</td>
            <td>vit_base_patch16_224 (pretrained)</td>
          </tr>
          <tr>
            <td>Token aggregation</td>
            <td>Learned attention over patch tokens for weighted feature extraction</td>
          </tr>
          <tr>
            <td>Classifier head</td>
            <td>LayerNorm → Dropout(0.3) → Linear(embed_dim→512) → ReLU → Dropout(0.3) → Linear(512→2)</td>
          </tr>
          <tr>
            <td>Optimization</td>
            <td>AdamW with weight decay 1e-4, LR 0.0001</td>
          </tr>
          <tr>
            <td>LR scheduler</td>
            <td>ReduceLROnPlateau (patience=3, factor=0.1)</td>
          </tr>
        </table>
      </div>
      
      <div class="implementation-details" style="margin-top: 20px;">
        <h3>Attention Mechanism Detail</h3>
        <table>
          <tr>
            <td>Input</td>
            <td>Patch token embeddings from ViT backbone</td>
          </tr>
          <tr>
            <td>Attention scoring</td>
            <td>Linear projection to scalar score per token</td>
          </tr>
          <tr>
            <td>Normalization</td>
            <td>Softmax along token dimension</td>
          </tr>
          <tr>
            <td>Feature aggregation</td>
            <td>Weighted sum of tokens based on attention scores</td>
          </tr>
        </table>
      </div>
    </div>
  </div>

  <div class="divider"></div>
  
  <!-- 3. Interpretable Vision Transformer -->
  <div class="architecture-container">
    <div class="diagram-title">3. Interpretable Vision Transformer</div>
    
    <div class="diagram">
      <div class="model-row">
        <!-- Input Image -->
        <div class="block">
          <div class="block-label">Input Image</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- Patch + Position Embeddings -->
        <div class="block">
          <div class="block-label">Patch + Position Embeddings</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <!-- ViT Encoder -->
        <div class="block">
          <div class="block-label">Pre-trained ViT Encoder</div>
          <div class="vit-block"></div>
        </div>
        <div class="arrow">→</div>
      </div>
      
      <div class="model-row">
        <!-- Branch 1: CLS Token -->
        <div class="block">
          <div class="block-label">[CLS] Token</div>
          <div class="cls-token"></div>
        </div>
        
        <!-- Branch 2: Patch Tokens + Attention -->
        <div style="display: flex; flex-direction: column; align-items: center; margin: 0 20px;">
          <div class="block">
            <div class="block-label">Patch Tokens</div>
            <div class="patch-tokens"></div>
          </div>
          <div style="height: 10px; font-size: 16px;">↓</div>
          <div class="block">
            <div class="block-label">Attention Layer</div>
            <div class="attention-block">
              <div class="attention-internal"></div>
            </div>
          </div>
          <div style="height: 10px; font-size: 16px;">↓</div>
          <div class="block">
            <div class="block-label">Weighted Patch Vector</div>
            <div class="basic-block"></div>
          </div>
        </div>
      </div>
      
      <div class="model-row" style="margin-top: -20px;">
        <div style="font-size: 16px; margin: 0 20px;">↓</div>
        <div style="font-size: 16px; margin: 0 20px;">↓</div>
      </div>
      
      <div class="model-row">
        <div class="block">
          <div class="block-label">Concatenated Representation</div>
          <div style="width: 100px; height: 30px; border: 2px solid #000; background-color: white;"></div>
        </div>
        <div class="arrow">→</div>
      </div>
      
      <div class="model-row">
        <!-- Classifier Head -->
        <div class="block">
          <div class="block-label">Layer Norm</div>
          <div class="ln"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Dropout</div>
          <div class="dropout"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Linear Layer</div>
          <div class="linear"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">ReLU</div>
          <div class="basic-block"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Dropout</div>
          <div class="dropout"></div>
        </div>
        <div class="arrow">→</div>
        
        <div class="block">
          <div class="block-label">Linear Layer (2 classes)</div>
          <div class="linear"></div>
        </div>
      </div>
      
      <div class="legend">
        <div class="legend-item">
          <div class="legend-box"></div>
          <div class="legend-text">Basic Blocks</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-teal"></div>
          <div class="legend-text">Pre-trained ViT</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-orange"></div>
          <div class="legend-text">Tokens</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-blue"></div>
          <div class="legend-text">Attention Mechanism</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-green"></div>
          <div class="legend-text">Layer Normalization</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-purple"></div>
          <div class="legend-text">Dropout</div>
        </div>
        <div class="legend-item">
          <div class="legend-box legend-box-red"></div>
          <div class="legend-text">Linear Layer</div>
        </div>
      </div>
      
      <div class="implementation-details">
        <h3>Implementation Details</h3>
        <table>
          <tr>
            <td>Architecture</td>
            <td>Interpretable Vision Transformer</td>
          </tr>
          <tr>
            <td>Base model</td>
            <td>vit_base_patch16_224 (pretrained)</td>
          </tr>
          <tr>
            <td>Feature extraction</td>
            <td>Dual-path: [CLS] token + attention-weighted patch tokens</td>
          </tr>
          <tr>
            <td>Feature fusion</td>
            <td>Concatenation of [CLS] token and weighted patch representation</td>
          </tr>
          <tr>
            <td>Classifier head</td>
            <td>LayerNorm → Dropout(0.3) → Linear(2*embed_dim→512) → ReLU → Dropout(0.3) → Linear(512→2)</td>
          </tr>
          <tr>
            <td>Interpretability</td>
            <td>Attention weights over patch tokens can be visualized as heatmaps</td>
          </tr>
          <tr>
            <td>Optimization</td>
            <td>AdamW with weight decay 1e-4, LR 0.0001</td>
          </tr>
          <tr>
            <td>LR scheduler</td>
            <td>ReduceLROnPlateau (patience=3, factor=0.1)</td>
          </tr>
        </table>
      </div>
      
      <div class="implementation-details" style="margin-top: 20px;">
        <h3>Attention Mechanism Detail</h3>
        <table>
          <tr>
            <td>Input</td>
            <td>Patch token embeddings from ViT backbone (excluding [CLS] token)</td>
          </tr>
          <tr>
            <td>Attention scoring</td>
            <td>Linear projection to scalar score per patch token</td>
          </tr>
          <tr>
            <td>Normalization</td>
            <td>Softmax along token dimension</td>
          </tr>
          <tr>
            <td>Feature aggregation</td>
            <td>Weighted sum of patch tokens based on attention scores</td>
          </tr>
          <tr>
            <td>Output</td>
            <td>Attention weights are returned alongside predictions for visualization</td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</body>
</html>