\contentsline {section}{\numberline {1}Introduction}{1}{section.1}%
\contentsline {section}{\numberline {2}Motivation}{3}{section.2}%
\contentsline {section}{\numberline {3}Related Works}{5}{section.3}%
\contentsline {section}{\numberline {4}Description of Work}{15}{section.4}%
\contentsline {section}{\numberline {5}Methodology}{16}{section.5}%
\contentsline {subsection}{\numberline {5.1}Google Colab Platform}{16}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Python and PyTorch Framework}{16}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Advantages of PyTorch in Our Implementation}{17}{subsubsection.5.2.1}%
\contentsline {section}{\numberline {6}Dataset Preparation and Refinement}{17}{section.6}%
\contentsline {subsection}{\numberline {6.1}Stage 1: Initial Dataset Collection}{17}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Stage 2: Refined Dataset - Focus on Adult In-flight Images}{18}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Stage 3: High-Quality Dataset}{18}{subsection.6.3}%
\contentsline {section}{\numberline {7}Transfer Learning Methodology}{19}{section.7}%
\contentsline {subsection}{\numberline {7.1}Theoretical Framework and Rationale}{19}{subsection.7.1}%
\contentsline {section}{\numberline {8}Transfer Learning Approach}{19}{section.8}%
\contentsline {section}{\numberline {9}Training Optimization Strategy}{20}{section.9}%
\contentsline {section}{\numberline {10}Model Architecture Modifications}{20}{section.10}%
\contentsline {section}{\numberline {11}Data Preparation and Augmentation}{21}{section.11}%
\contentsline {subsection}{\numberline {11.1}Optimization Strategy}{21}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}Learning Rate Scheduling}{22}{subsection.11.2}%
\contentsline {subsection}{\numberline {11.3}Gradient Clipping}{22}{subsection.11.3}%
\contentsline {subsubsection}{\numberline {11.3.1}Dataset Management}{22}{subsubsection.11.3.1}%
\contentsline {section}{\numberline {12}Regularization Techniques}{23}{section.12}%
\contentsline {subsection}{\numberline {12.1}Model Checkpointing and Evaluation}{23}{subsection.12.1}%
\contentsline {section}{\numberline {13}Evaluation Strategy}{23}{section.13}%
\contentsline {subsubsection}{\numberline {13.0.1}Addressing Class Imbalance in few models}{24}{subsubsection.13.0.1}%
\contentsline {subsection}{\numberline {13.1}Image Preprocessing}{24}{subsection.13.1}%
\contentsline {subsection}{\numberline {13.2}Reproducibility Considerations}{25}{subsection.13.2}%
\contentsline {subsection}{\numberline {13.3}VGG-16 Architecture}{25}{subsection.13.3}%
\contentsline {subsubsection}{\numberline {13.3.1}Theoretical Foundation}{25}{subsubsection.13.3.1}%
\contentsline {subsubsection}{\numberline {13.3.2}Model Adaptation for Fine-Grained Classification}{25}{subsubsection.13.3.2}%
\contentsline {section}{\numberline {14}Implementation Details}{26}{section.14}%
\contentsline {section}{\numberline {15}Vision Transformer (ViT) Architecture}{26}{section.15}%
\contentsline {subsection}{\numberline {15.1}Theoretical Framework}{26}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Standard Vision Transformer Implementation}{27}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}Enhanced Vision Transformer with Custom Attention}{27}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}Data Processing and Augmentation}{28}{subsection.15.4}%
\contentsline {subsection}{\numberline {15.5}Training Methodology}{29}{subsection.15.5}%
\contentsline {subsection}{\numberline {15.6}Residual Network (ResNet-50) Implementation}{31}{subsection.15.6}%
\contentsline {subsubsection}{\numberline {15.6.1}Architecture Adaptation}{31}{subsubsection.15.6.1}%
\contentsline {subsubsection}{\numberline {15.6.2}Image Enhancement}{32}{subsubsection.15.6.2}%
\contentsline {subsubsection}{\numberline {15.6.3}Training Strategy}{32}{subsubsection.15.6.3}%
\contentsline {subsubsection}{\numberline {15.6.4}Performance Implications}{32}{subsubsection.15.6.4}%
\contentsline {section}{\numberline {16}Custom CNN with Squeeze-and-Excitation Blocks}{33}{section.16}%
\contentsline {subsection}{\numberline {16.1}Architectural Design}{33}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}Addressing Class Imbalance}{33}{subsection.16.2}%
\contentsline {subsection}{\numberline {16.3}Optimization Strategy}{34}{subsection.16.3}%
\contentsline {section}{\numberline {17}Model Interpretability Methodologies}{34}{section.17}%
\contentsline {subsection}{\numberline {17.1}Gradient-weighted Class Activation Mapping (Grad-CAM)}{34}{subsection.17.1}%
\contentsline {subsubsection}{\numberline {17.1.1}Theoretical Foundation}{34}{subsubsection.17.1.1}%
\contentsline {subsubsection}{\numberline {17.1.2}Methodology for VGG16}{35}{subsubsection.17.1.2}%
\contentsline {subsection}{\numberline {17.2}Attention Rollout for Vision Transformers}{36}{subsection.17.2}%
\contentsline {subsubsection}{\numberline {17.2.1}Theoretical Foundation}{36}{subsubsection.17.2.1}%
\contentsline {subsubsection}{\numberline {17.2.2}Methodology for ViT}{36}{subsubsection.17.2.2}%
\contentsline {subsection}{\numberline {17.3}Grad-CAM for Vision Transformers}{37}{subsection.17.3}%
\contentsline {subsubsection}{\numberline {17.3.1}Implementation Approach}{37}{subsubsection.17.3.1}%
\contentsline {subsection}{\numberline {17.4}Comparison Framework}{38}{subsection.17.4}%
\contentsline {subsection}{\numberline {17.5}Implementation Details}{39}{subsection.17.5}%
\contentsline {subsubsection}{\numberline {17.5.1}Grad-CAM for VGG16}{39}{subsubsection.17.5.1}%
\contentsline {subsubsection}{\numberline {17.5.2}Attention Rollout for ViT}{39}{subsubsection.17.5.2}%
\contentsline {subsubsection}{\numberline {17.5.3}Grad-CAM for ViT}{39}{subsubsection.17.5.3}%
