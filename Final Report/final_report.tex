\documentclass[a4paper,12pt]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{titling}
\usepackage{lmodern}
\usepackage{lastpage}
\usepackage{natbib}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\renewcommand{\contentsname}{Table of Contents}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em} % Adjust spacing between paragraphs

% Customise hyperlink appearance (Optional)
\hypersetup{
    colorlinks=true,        % Enable colored links
    linkcolor=black,        % Color for internal links (TOC, sections, etc.)
    citecolor=black,        % Color for citation links
    filecolor=black,        % Color for file links
    urlcolor=blue          % Color for external URLs
}

\begin{document}

% Cover Page
\begin{titlepage}
    \begin{center}

        \textbf{\LARGE{School of Computer Science}}\\[0.5em]
        \textbf{\Large{Faculty of Science and Engineering}}\\[0.5em]
        \textbf{\Large{University of Nottingham}}\\[0.5em]
        \textbf{\Large{Malaysia}}\\[5em]

        \includegraphics[width=0.5\textwidth]{images/nottingham_logo.png}\\[3em]

        \textbf{\Large{UG FINAL YEAR DISSERTATION REPORT}}\\[6em]
        \textbf{\large{\textit{Interpretable Seagull classification}}}\\[6em]

    \end{center}

    \begin{center}
        \begin{minipage}{0.6\textwidth}  % adjust width as needed
            \raggedright
            \textbf{Student's Name} \hspace{1.5cm}: Name\\[1em]
            \textbf{Student Number} \hspace{1.4cm}: Student ID\\[1em]
            \textbf{Supervisor Name} \hspace{1.2cm}: Dr. Tomas Maul\\[1em]
            \textbf{Year} \hspace{3.8cm}: 2025\\[4em]
        \end{minipage}
    \end{center}

    \vfill

    \begin{center}
        \begin{minipage}{\textwidth}
            \centering
            {\fontsize{12}{10}\selectfont\textbf{SUBMITTED IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE AWARD OF}}
            {\fontsize{12}{10}\selectfont\textbf{BACHELOR OF SCIENCE IN COMPUTER SCIENCE (HONS)}}\\
            {\fontsize{12}{10}\selectfont\textbf{THE UNIVERSITY OF NOTTINGHAM}}
        \end{minipage}
    \end{center}
\end{titlepage}

% Title Page
\newpage
\begin{titlepage}
    \begin{center}
        \vspace{0.1em}
        \includegraphics[width=0.5\textwidth]{images/nottingham_logo.png}\\[6em]

        \textbf{\large Title}\\[6em]

        \fontsize{10}{10}{Submitted in May 2025, in partial fulfillment of the conditions of the award of the degrees B.Sc.}\\[4em]

        Name\\
        School of Computer Science\\
        Faculty of Science and Engineering\\
        University of Nottingham\\
        Malaysia\\[6em]

        I hereby declare that this dissertation is all my own work, except as indicated in the text:\\[4em]

        Signature \underline{\hspace{7cm}}\\[2em]
        Date \hspace{1cm} \underline{\hspace{1cm}} / \underline{\hspace{1cm}} / \underline{\hspace{2cm}}
    \end{center}
\end{titlepage}

% Change margins for Table of Contents and subsequent pages
\newgeometry{
    margin=1in
}

% Roman numbering for preliminary pages
\pagenumbering{roman}

% Acknowledgement
\newpage
\section*{\centering \normalsize{Acknowledgement}}

% Abstract
\newpage
\section*{\centering \normalsize{Abstract}}


% Table of Contents
\newpage
\tableofcontents

% List of Figures
\newpage
\listoffigures

% List of Tables
\newpage
\listoftables

% Switch to Arabic numbering starting from Introduction
\newpage
\cleardoublepage % Ensure proper page break before switching numbering style
\pagenumbering{arabic} % Switch to Arabic numerals
\setcounter{page}{1} % Restart page numbering at 1

% Introduction
\section{Introduction}
Accurate species identification is a key starting point for scientific research and conservation efforts. Determining whether two populaions can be consistently distinguished based on morphological traits is essential for establishing taxonomic boundaries and designing appropriate conservation strategies. Gulls (\textit{Laridae}) present a particularly challenging case for identification due to their recent evolutionary divergence and subtle morphological differences. As noted by ornithologists:

\begin{quote}
    ``Gulls can be a challenging group of birds to identify. To the untrained eye, they all look alike, yet, at the same time, in the case of the large gulls, one could say that no two birds look the same!'' \citep{ayyash2024}.
\end{quote}


The classification of gulls presents multiple challenges that make traditional identification methods problematic and inconsistent. These difficulties stem from several interrelated factors. First, certain gull species exhibit unusual levels of variation compared to other gull species.

\begin{quote}
``Glaucous-winged Gulls also exhibit variably pigmented wingtips... these differences are often chalked up to individual
variation, at least by this author, but they're inconveniently found in several hybrid zones, creating potential for much
confusion.\citep{adriaens2022}
\end{quote}

\begin{quote}
    ``The amount of variation here is disturbing because it is unmatched by any other gull species, and more so because it is not completely understood'' \citep{adriaens2022gulls}.
\end{quote}

Second, multiple confounding factors complicate identification:
\begin{itemize}
    \item \textbf{Hybridization:} Both species can interbreed in overlapping ranges, creating intermediate forms.
    \item \textbf{Age-related variations:} Juvenile and immature gulls display less distinct patterns than adults.
    \item \textbf{Environmental effects:} Feather bleaching from sun exposure, contamination, and wear can alter appearance.
    \item \textbf{Seasonal moulting:} Gulls undergo plumage changes throughout the year, affecting diagnostic features.
    \item \textbf{Viewing conditions:} Lighting, angle, and distance significantly impact observed coloration.
\end{itemize} \citep{adriaens2022}

These complications can make manual classification both time-consuming and subjective. While traditional taxonomic guides provide detailed descriptions of distinguishing features, the subtle nature of these differences can often lead to inconsistent identifications.


\section*{Motivation}

This project addresses the complex task of fine-grained classification between two closely related gull species: the Slaty-backed Gull and the Glaucous-winged Gull. These species, found primarily in eastern Russia and the Pacific Coast of the USA, display subtle and overlapping physical characteristics. 


DOUBTFUL to keep
The wing and wingtip patterns—particularly the colour, intensity, and pattern of the primary feathers—are crucial diagnostic features for identification, yet they exhibit considerable variation within each species.


Deep learning approaches offer promising solutions to this taxonomic challenge through their ability to automatically learn discriminative features from large datasets. Fine-grained image classification (FGIC), which focuses on identifying subtle differences between subclasses within the same category, has advanced rapidly over the past decade with the development of sophisticated deep neural network architectures \citet{karpathy2015}.

For species identification specifically, convolutional neural networks (CNNs) such as ResNet, Inception, and DenseNet have demonstrated exceptional capabilities, with recent studies achieving accuracy rates exceeding 97\% in bird species classification tasks \citep{he2016}. These architectures automatically learn hierarchical feature representations—from low-level edges and textures to high-level semantic concepts—that capture the subtle morphological differences between closely related species \citep{simonyan2014}.


Traditional feature extraction methods necessitate manually designed features, such as edge detection, color histograms, feature point matching, and visual word bags, which have limited expressive capabilities and require extensive annotation details like bounding boxes and key points. The drawback of these methods lies in the extensive manual intervention required for feature selection and extraction. Due to the impressive outcomes of deep learning, most recognition frameworks now depend on advanced convolutions for feature extraction (\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full#ref10}{Krizhevsky et al., 2012}).
Unlike traditional machine learning methods that rely on hand-engineered features, deep neural networks can detect complex patterns in high-dimensional data, making them well-suited for fine-grained visual classification tasks. Manual identification requires per specimen analysis by expert taxonomists, hindering large-scale surveys.

There are many advantages of using Deep Learning
Architectures for Image Classification. Getting good quality
results in Machine Learning models is dependent on how good
the data is labelled, whereas Deep Learning architectures don’t
necessarily require labelling, as Neural Networks are great at
learning without guidelines. One more advantage is that in
certain domains like speech, language and vision, Deep
Learning consistently produces excellent results that
significantly outperforms other alternatives. There are many
challenges that are involved too. Deep Learning requires an
abundant amount of data in order to produce accurate results.
Overfitting is a prevalent problem in Deep Learning and can
sometimes negatively affect the model performance in realtime scenarios.

This project therefore focuses not only on developing high-accuracy classification models but also on implementing robust interpretability techniques to visualize and understand which morphological features drive model decisions. By bridging computer vision and ornithological expertise, this work aims to contribute both to the technological advancement of interpretable fine-grained classification and to the biological understanding of gull taxonomy.



Yet the fine-grained bird classification task has greater challenges [\href{https://www.mdpi.com/2076-2615/13/2/264#B14-animals-13-00264}{14}], which are mainly reflected in the following three points: (1) High intraclass variance. Birds belonging to the same category usually present distinctly different postures and perspectives, (2) Low inter-class variance. Some of the different categories of birds may have only minor differences; for example, some of the differences are only in the color pattern on the head; and (3) Limited training data. Some bird data are limited in number, especially endangered species, for whom it is difficult to collect sufficient image data. Meanwhile, the labeling of bird categories usually requires a great deal of time by experts in the corresponding fields. These problems greatly increase the difficulty of acquiring training data.

   
% Related Work
\newpage
\section{Related Work}


\section{Deep Learning for Fine-Grained Image Classification}
Fine-grained image classification presents unique challenges compared to general image classification tasks. As Li et al. (2021) note, fine-grained classification "necessitates discrimination between semantic and instance levels, while considering the similarity and diversity among categories"\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. This is particularly challenging in bird classification due to three key factors: high intra-class variance (birds of the same species in different postures), low inter-class variance (different species with only minor differences), and limited training data availability, especially for rare species\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}.

Convolutional Neural Networks (CNNs) have revolutionized image classification through their ability to automatically learn hierarchical feature representations. For fine-grained tasks, traditional CNNs face limitations in capturing the subtle distinguishing features between closely related categories. This has led to the development of specialized architectures and techniques focused on identifying discriminative regions in images\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}.

Early approaches to fine-grained classification relied on fixed rectangular bounding boxes and part annotations to obtain visual differences, but these methods required extensive human annotation effort\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. Recent research has shifted toward weakly supervised approaches that only require image-level labels, developing localization subnetworks to identify critical parts followed by classification subnetworks\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. These models facilitate learning while maintaining high accuracy without needing pre-selected boxes, making them more practical for real-world applications.

Recent research emphasizes that effective fine-grained classification depends on identifying and integrating information from multiple discriminative regions rather than focusing on a single region. As highlighted in recent literature, "it is imperative to integrate information from various regions rather than relying on a singular region"\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. This insight has led to the development of methods combining features from different levels via attention modules, thereby enhancing the semantic and discriminative capacity of features for fine-grained classification\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}.

\section*{Transfer Learning for Image Classification}
Deep learning, while powerful, comes with two major constraints: dependency on extensive labeled data and high training costs\cite{iman2022review}. Transfer learning offers a solution to these limitations by enabling the reuse of knowledge obtained from a source task when training on a target task. In the context of deep learning, this approach is known as Deep Transfer Learning (DTL)\cite{iman2022review}.

Transfer learning is particularly valuable for fine-grained bird classification where obtaining large, labeled datasets is challenging. As noted in recent research, "when the sample data is small, transfer learning can help the deep neural network classifier to improve classification accuracy"\href{https://ijece.iaescore.com/index.php/IJECE/article/view/24833}{3}. This makes transfer learning an ideal approach for specialized tasks like distinguishing between closely related gull species.

LINITED DATASET Nitish
Srivastava, Hinton [21] [23] proposed the method of Dropout to prevent over-fitting, effectively reducing the
parameters of the full connection layer, and solve the problem of insufficient samples\cite{zhao2017transfer}

Several studies have demonstrated the efficacy of transfer learning for bird species classification. A study on automatic bird species identification using deep learning achieved a top-5 accuracy of 97.98\% by leveraging pretrained CNN networks with a base model to encode images\href{https://www.semanticscholar.org/paper/41b0718279f408654094557156d4eeeb0067b2c4}{10}. Similarly, research on bird species identification using modified deep transfer learning achieved 98.86\% accuracy using the pretrained EfficientNetB5 model\href{https://www.semanticscholar.org/paper/770ee68d1b136cd098a018a399d1f69af29faae0}{11}. These results demonstrate that transfer learning approaches can achieve high performance even with limited training data.

Various pretrained models have been evaluated for bird classification tasks, including VGG16, VGG19, ResNet, DenseNet, and EfficientNet architectures. Comparative studies have shown that while all these models can perform effectively, some consistently outperform others.\cite{santiago2024comparative}For example, research on drones-birds classification found that "the accuracy and F-Score of ResNet18 exceeds 98\% in all cases"\href{https://www.semanticscholar.org/paper/c16f57236555aae3f600ef8f1978eff10b410233}{7}, while another study on bird classification reported that "DenseNet201 achieves the best classification accuracy of 98.89\% for binary classification"\href{https://www.semanticscholar.org/paper/6529ad5f1094a8d9b0ab38db163c7fdaad2a1d9c}{14}.

The transfer learning process typically involves two phases: first freezing most layers of the pretrained model and training only the top layers, then fine-tuning a larger portion of the network while keeping early layers fixed\href{https://www.semanticscholar.org/paper/770ee68d1b136cd098a018a399d1f69af29faae0}{11}. This approach preserves the general feature extraction capabilities of the pretrained model while adapting it to the specific characteristics of the target dataset.

\section*{Interpretability Techniques for Deep Learning Models}
While deep learning models achieve impressive accuracy in classification tasks, their "black box" nature limits their usefulness in scientific contexts where understanding the basis for classifications is crucial. Interpretability techniques address this limitation by providing insights into model decision-making processes, making them essential tools for applications where transparency is as important as accuracy.

Gradient-weighted Class Activation Mapping (Grad-CAM) has emerged as a particularly valuable technique for visualizing regions of images that influence classification decisions. As described in recent literature, Grad-CAM "uses the gradients of each target that flows into the least convolutional layer to produce a bearish localization map, highlighting important regions in the image for concept prediction"\href{https://www.atlantis-press.com/article/125986223.pdf}{5}. This approach enables researchers to validate model decisions against expert knowledge and potentially discover new insights about morphological features.

Visualization studies comparing baseline models with enhanced architectures demonstrate that while basic models often focus on the most conspicuous parts of bird images (such as wings), more sophisticated approaches can discern more intricate features vital for species differentiation\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. As noted in recent research, enhanced models excel "in identifying not only the prominent features but also the subtle, fine-grained characteristics essential for distinguishing between different bird types"\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}.

These interpretability methods are particularly valuable in fine-grained classification tasks where the differences between categories are subtle and potentially unknown. By highlighting regions that drive model decisions, techniques like Grad-CAM can reveal discriminative features that might not be obvious even to expert observers, potentially advancing biological understanding alongside classification accuracy.

\section*{Justification for Deep Learning with Transfer Learning Approach}
The choice of deep learning with transfer learning for gull species classification is supported by several compelling factors derived from recent research. Traditional machine learning approaches, while effective for smaller datasets, face limitations when dealing with the complexity of fine-grained visual classification tasks. As demonstrated in comparative studies, "deep learning is more effective than traditional machine learning algorithms in image recognition as the number of bird species increases"\href{https://ijece.iaescore.com/index.php/IJECE/article/view/24833}{3}.

The advantages of deep learning architectures for image classification are significant. Unlike traditional machine learning models that require carefully labeled data, "Deep Learning architectures don't necessarily require labelling, as Neural Networks are great at learning without guidelines"1. Furthermore, in domains like vision, "Deep Learning consistently produces excellent results that significantly outperforms other alternatives"1.

Transfer learning addresses the primary challenges of deep learning: the need for large datasets and extensive computational resources. By leveraging pretrained models that have already learned general visual features from massive datasets, transfer learning enables the development of highly accurate classifiers with relatively small, domain-specific datasets\href{https://arxiv.org/abs/2201.09679}{6}. This is particularly valuable for this project, which focuses on distinguishing between two specific gull species with limited available data.

The effectiveness of transfer learning for fine-grained bird classification has been consistently demonstrated across multiple studies, with various pretrained models achieving accuracy rates exceeding 98\%\href{https://www.semanticscholar.org/paper/41b0718279f408654094557156d4eeeb0067b2c4}{10}\href{https://www.semanticscholar.org/paper/770ee68d1b136cd098a018a399d1f69af29faae0}{11}. These results indicate that transfer learning provides an optimal balance between accuracy and efficiency for the specific task of gull species classification.

The integration of interpretability techniques with transfer learning further strengthens this approach by addressing the "black box" limitation of deep neural networks. By implementing methods like Grad-CAM, the project can not only achieve high classification accuracy but also provide insights into the morphological features that drive model decisions, making the results more valuable for scientific applications\href{https://www.atlantis-press.com/article/125986223.pdf}{5}.

\section*{Aims and Objectives}

\subsection*{Primary Aims}
\begin{enumerate}
    \item To develop high-performance deep learning models capable of distinguishing between Slaty-backed and Glaucous-winged Gulls based on their morphological characteristics.
    \item To implement robust interpretability techniques that reveal which features influence model decisions, allowing validation against ornithological expertise.
    \item To analyze whether consistent morphological differences exist between the two species and identify key discriminative features.
\end{enumerate}

\subsection*{Specific Objectives}
The project will be carried out in four phases:
\begin{enumerate}
    \item Model Development and Evaluation
        \begin{itemize}
            \item Curate a high-quality dataset of adult in-flight gull images with clearly visible diagnostic features.
            \item Implement and compare multiple deep learning architectures (CNNs, Vision Transformers) for fine-grained classification.
            \item Optimize model performance through appropriate regularization techniques, data augmentation, and hyperparameter tuning.
            \item Evaluate models using appropriate metrics (accuracy, precision, recall, F1-score) on carefully constructed test sets.
        \end{itemize}
    \item Interpretability Implementation
        \begin{itemize}
            \item Implement Gradient-weighted Class Activation Mapping (Grad-CAM) for convolutional architectures.
            \item Develop or adapt interpretability techniques suitable for Vision Transformers.
            \item Visualize regions of images that most influence classification decisions.
            \item Compare model focus areas with known taxonomic features described in ornithological literature.
        \end{itemize}
    \item Feature Analysis
        \begin{itemize}
            \item Perform quantitative analysis of image regions highlighted by interpretability techniques.
            \item Compare intensity, texture, and pattern characteristics between species.
            \item Identify statistically significant morphological differences between correctly classified specimens.
        \end{itemize}
    \item Refinement and Validation
        \begin{itemize}
            \item Refine models and interpretability methods based on insights from feature analysis.
            \item Validate findings against expert ornithological knowledge.
            \item Document limitations, edge cases, and areas for future research.
        \end{itemize}
\end{enumerate}

% Description of Work
\newpage
\section{Description of Work}

% Methodology
\newpage
\section{Methodology}

\subsection{Google Colab Platform}

Google Colab was selected as the primary platform for developing and training deep learning models. As described by Anjum et al. \citet{anjum2021}, Google Colab offers significant advantages for machine learning research through its cloud-based environment with integrated GPU acceleration enabling fast model training. The platform's pre-installed libraries and integration with Google Drive provided an efficient workflow for model development, experimentation, and storage of datasets and trained models. This approach aligns with modern best practices in deep learning research where computational efficiency is crucial for iterative model development and refinement.

Despite its advantages, Google Colab presented a few challenges. The platform frequently disconnected during training sessions, interrupting the model training process before completing all epochs. These disconnections likely stemmed from limited RAM allocation, runtime timeouts, or resource constraints of the shared free GPU environment. As noted by \citet{carneiro2018}, while Colab provides robust GPU resources that can match dedicated servers for certain tasks, these free resources ``are far from enough to solve demanding real-world problems and are not scalable.''

To mitigate these issues, two strategies were implemented. First, the relatively small size of our dataset helped minimize resource demands. Second, checkpoint saving was implemented throughout the training process, allowing training to resume from the last saved state if disconnections were encountered. This approach ensured that progress wasn't lost when disconnections occurred, though it introduced some workflow inefficiencies.

\subsection{Python and PyTorch Framework}

The implementation was carried out using Python as the primary programming language, chosen for its extensive library support and widespread adoption in the machine learning community. Python's simple syntax and powerful libraries make it particularly suitable for rapid prototyping and experimentation in deep learning research \citep{geron2019}.

For the deep learning framework, PyTorch was selected over alternatives like TensorFlow or Keras due to its dynamic computational graph which allows for more flexible model development and easier debugging. PyTorch's intuitive design facilitates a more natural expression of deep learning algorithms while still providing the performance benefits of GPU acceleration. The framework's robust ecosystem for computer vision tasks, including pre-trained models and transformation pipelines, was particularly valuable for this fine-grained classification task.

\subsubsection{Advantages of PyTorch in Our Implementation}

PyTorch offered several key advantages that were particularly beneficial for our transfer learning approach with pre-trained models:

\begin{itemize}
    \item \textbf{Dynamic Computational Graph:} PyTorch's define-by-run approach allowed for more intuitive debugging and model modification during development. This was especially valuable when adapting pre-trained architectures like VGG16 for our specific classification task.

    \item \textbf{Flexible Model Customization:} The implementation benefited from PyTorch's object-oriented approach, which made it straightforward to modify pre-trained models, e.g., replacing classification layers while preserving feature extraction capabilities.

    \item \textbf{Efficient Data Loading and Augmentation:} PyTorch's DataLoader and transformation pipelines facilitated efficient batch processing and on-the-fly data augmentation, which was crucial for maximizing the utility of our limited dataset.

    \item \textbf{Gradient Visualization Tools:} PyTorch's native support for gradient computation and hooks made implementing Grad-CAM and other visualization techniques more straightforward, enabling better model interpretability.
\end{itemize}

Similar to approaches described by Raffel et al. \citet{raffel2023}, our implementation prioritized efficiency and optimization to work within the constraints of limited computational resources, allowing us to achieve high-quality results despite the limitations of the free cloud environment.

\section{Dataset Preparation and Refinement}

The dataset preparation followed a three-stage iterative refinement process, each addressing specific challenges identified during model development. This approach aligns with established methodologies in fine-grained bird classification research, where dataset quality has been shown to significantly impact model performance \citet{ghani2024}.

\subsection{Stage 1: Initial Dataset Collection}

The initial dataset was collected from public repositories including eBird and iNaturalist, comprising 451 images of Glaucous-winged Gulls and 486 images of Slaty-backed Gulls. This dataset included gulls of various ages (juveniles and adults) in different postures (sitting, standing, and flying). Initial model testing on this dataset yielded poor performance (below 50\% accuracy), highlighting the need for dataset refinement. Similar challenges with diverse postures and class imbalance have been documented by Kahl et al. in their work on BirdNET systems \citet{kahl2021}.




\subsection{Stage 2: Refined Dataset - Focus on Adult In-flight Images}

Consultation with Professor Gibbins, an ornithological expert, revealed that adult wingtip patterns are the most reliable distinguishing features between these species, and these patterns are most visible in flight. This expert-guided refinement approach parallels methods described by Wang et al. in their work on avian dataset construction, where domain expertise significantly improved classification accuracy for visually similar species. \citet{wang2022}. Consequently, the dataset was refined to focus exclusively on adult in-flight images, resulting in a curated collection of 124 Glaucous-winged Gull images and 127 Slaty-backed Gull images. This targeted approach significantly improved model performance, with accuracy increasing to approximately 70\%.

By focusing specifically on adult in-flight images where wingtip patterns are most visible, this project addresses the core taxonomic question while minimizing confounding variables. The resulting interpretable classification system aims to provide both a practical identification tool and a scientific instrument for exploring morphological variation within and between these closely related species.

\subsection{Stage 3: High-Quality Dataset}

To further enhance classification performance, 640 high-resolution images of in-flight Slaty-backed Gulls were obtained from Professor Gibbins. The Glaucous-winged Gull dataset was also carefully curated with expert guidance, reducing it to 135 high-quality images that clearly displayed critical wingtip features. Images showing birds in moulting stages, juveniles, or unclear wingtip patterns were systematically removed. This quality-focused approach aligns with findings from Zhou et al., who demonstrated that expert-curated datasets can achieve comparable or superior results with significantly smaller data volumes compared to larger uncurated collections \citet{zhou2022}.

For comparative analysis, an unrefined dataset containing 632 adult in-flight Glaucous-winged Gulls and 640 high-quality Slaty-backed Gull images was also tested. This multi-dataset evaluation approach follows best practices established in the BirdSet benchmark for avian classification studies \citet{birdset2023}.

\section{Transfer Learning Methodology}

\subsection{Theoretical Framework and Rationale}

Transfer learning is a powerful machine learning technique that involves reusing a pre-trained model developed for a specific task as a starting point for a new task. This approach significantly enhances learning efficiency by leveraging knowledge gained from solving previous problems, enabling a positive transfer learning effect and reducing the training time required. For fine-grained classification tasks like distinguishing between visually similar gull species, transfer learning is particularly valuable as it allows the model to build upon a foundation of general visual features already learned from diverse datasets.

As highlighted by Kahl et al. (2021), transfer learning addresses two critical challenges in specialized biological classification: data scarcity and feature abstraction \citet{kahl2021}. First, data scarcity is a common issue in specialized domains like ornithological image classification, where large-scale annotated datasets are rare. Transfer learning mitigates this by leveraging models pre-trained on massive datasets like ImageNet. Second, these pre-trained models have learned to extract hierarchical features that capture important visual patterns, which can significantly enhance the accuracy of fine-grained classification tasks.

In our implementation, transfer learning was employed to leverage the robust feature extraction capabilities of pre-trained models on ImageNet. This approach aligns with best practices in fine-grained classification tasks, where lower-level features learned from diverse datasets can be effectively repurposed for specialized domains. The pre-training on ImageNet's 1.2 million images across 1,000 classes provides the model with a strong foundation for recognizing a wide range of visual patterns, which can then be fine-tuned for the specific task of distinguishing between Glaucous-winged and Slaty-backed Gulls.

Several pre-trained architectures were evaluated for this task, with VGG-16. \citet{simonyan2014vgg} demonstrating superior performance in our specific classification context. The effectiveness of transfer learning was evident in the rapid convergence and high accuracy achieved even with our relatively limited dataset of gull images, demonstrating the potential of this approach for specialized biological classification tasks.

% Bibliography
\newpage
\bibliographystyle{apa}
\bibliography{references}

\end{document}