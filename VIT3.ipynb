{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravindh4404/FYPSeagullClassification01/blob/main/VIT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wsvm895wS1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1ab902-4505-4469-aff7-df2bb85b0f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3HgvGOcwDvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bb4d9d-23c4-4e55-9812-ebc71b644413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.1+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5aedd8f7694b4717a11c315d2aa71229",
            "99f8769f963049ab86b84107500f6ff0",
            "726adb23b0a247768f560b6784006c09",
            "a8231cdb3a1c4cc9a646283b84c6bbbd",
            "d1c11754182e4f97b832218a7627a21b",
            "3711966e8bde46818e632632093c1ad8",
            "076ea7f2dc7149dbba9993862794d482",
            "b6260293e84e4fe5a7130e21e80f2deb",
            "3b62163c2c414ecda0da1b5a09a86b13",
            "86564363ca99462bb3f38bb69c09ad8c",
            "c958346b4d024c9e937e726514d085d2"
          ]
        },
        "id": "7ooaxSuLwIfT",
        "outputId": "796afa56-22a1-46cf-f498-beaa71e4662d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aedd8f7694b4717a11c315d2aa71229",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.442076\n",
            "Validation Accuracy: 95.39%, Precision: 0.96, Recall: 0.98, F1 Score: 0.97\n",
            "Final model (epoch 1) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "New best model saved with accuracy: 95.39% at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/best_model_vit_20250207.pth\n",
            "Epoch [2/20], Loss: 0.242328\n",
            "Validation Accuracy: 96.05%, Precision: 0.95, Recall: 1.00, F1 Score: 0.98\n",
            "Final model (epoch 2) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "New best model saved with accuracy: 96.05% at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/best_model_vit_20250207.pth\n",
            "Epoch [3/20], Loss: 0.206850\n",
            "Validation Accuracy: 90.13%, Precision: 0.95, Recall: 0.93, F1 Score: 0.94\n",
            "Final model (epoch 3) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [4/20], Loss: 0.119347\n",
            "Validation Accuracy: 92.76%, Precision: 0.92, Recall: 1.00, F1 Score: 0.96\n",
            "Final model (epoch 4) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [5/20], Loss: 0.178137\n",
            "Validation Accuracy: 96.05%, Precision: 0.95, Recall: 1.00, F1 Score: 0.98\n",
            "Final model (epoch 5) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [6/20], Loss: 0.078148\n",
            "Validation Accuracy: 98.03%, Precision: 1.00, Recall: 0.98, F1 Score: 0.99\n",
            "Final model (epoch 6) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "New best model saved with accuracy: 98.03% at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/best_model_vit_20250207.pth\n",
            "Epoch [7/20], Loss: 0.065228\n",
            "Validation Accuracy: 91.45%, Precision: 0.91, Recall: 1.00, F1 Score: 0.95\n",
            "Final model (epoch 7) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [8/20], Loss: 0.057759\n",
            "Validation Accuracy: 96.71%, Precision: 0.98, Recall: 0.98, F1 Score: 0.98\n",
            "Final model (epoch 8) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [9/20], Loss: 0.081063\n",
            "Validation Accuracy: 92.76%, Precision: 0.92, Recall: 1.00, F1 Score: 0.96\n",
            "Final model (epoch 9) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [10/20], Loss: 0.074056\n",
            "Validation Accuracy: 97.37%, Precision: 0.98, Recall: 0.99, F1 Score: 0.98\n",
            "Final model (epoch 10) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [11/20], Loss: 0.029560\n",
            "Validation Accuracy: 97.37%, Precision: 0.97, Recall: 1.00, F1 Score: 0.98\n",
            "Final model (epoch 11) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [12/20], Loss: 0.012414\n",
            "Validation Accuracy: 98.03%, Precision: 0.98, Recall: 1.00, F1 Score: 0.99\n",
            "Final model (epoch 12) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "Epoch [13/20], Loss: 0.010707\n",
            "Validation Accuracy: 98.68%, Precision: 0.98, Recall: 1.00, F1 Score: 0.99\n",
            "Final model (epoch 13) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n",
            "New best model saved with accuracy: 98.68% at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/best_model_vit_20250207.pth\n",
            "Epoch [14/20], Loss: 0.011572\n",
            "Validation Accuracy: 97.37%, Precision: 0.97, Recall: 1.00, F1 Score: 0.98\n",
            "Final model (epoch 14) saved at /content/drive/My Drive/FYP/VIT2_HQ2_20250207/final_model_vit_20250207.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import timm  # For using pretrained Vision Transformer models\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Mount Google Drive if using Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the folder to save model checkpoints\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "checkpoint_folder = f'/content/drive/My Drive/FYP/VIT2_HQ2_{date_str}/'\n",
        "os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "\n",
        "# Data Augmentation for Training Set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Typical input size for ViT models\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalize to match pretrained models' expectations\n",
        "])\n",
        "\n",
        "# Simple resizing for validation and test sets\n",
        "transform_val_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "data_path = '/content/drive/My Drive/FYP/Dataset/HQ2/train'\n",
        "test_data_path = '/content/drive/My Drive/FYP/Dataset/HQ2/test'\n",
        "train_dataset = datasets.ImageFolder(data_path, transform=transform_train)\n",
        "test_dataset = datasets.ImageFolder(test_data_path, transform=transform_val_test)\n",
        "\n",
        "# Split the dataset into 80% training and 20% validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set the device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Enhanced Vision Transformer model with custom attention pooling and classifier head\n",
        "class EnhancedViT(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3, hidden_dim=512):\n",
        "        \"\"\"\n",
        "        Initializes the enhanced ViT model.\n",
        "        - Loads a pre-trained ViT backbone.\n",
        "        - Removes the original classification head.\n",
        "        - Adds an attention mechanism to pool patch tokens.\n",
        "        - Adds a custom MLP classifier head.\n",
        "        \"\"\"\n",
        "        super(EnhancedViT, self).__init__()\n",
        "        # Load a pre-trained ViT model from timm\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        # Remove the original classification head\n",
        "        self.vit.head = nn.Identity()\n",
        "\n",
        "        # Get the embedding dimension (most timm ViT models have 'embed_dim')\n",
        "        if hasattr(self.vit, 'embed_dim'):\n",
        "            self.embed_dim = self.vit.embed_dim\n",
        "        else:\n",
        "            # Fallback: use the in_features of the original head if available\n",
        "            self.embed_dim = self.vit.head.in_features\n",
        "\n",
        "        # Attention mechanism: compute an attention score for each token (patch)\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 1)  # Outputs a scalar score per token\n",
        "        )\n",
        "\n",
        "        # Custom classifier head: LayerNorm -> Dropout -> Linear -> ReLU -> Dropout -> Linear\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, 2)  # Binary classification (2 classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass:\n",
        "          1. Extract patch token embeddings via ViT's forward_features.\n",
        "          2. Compute attention scores for each token.\n",
        "          3. Aggregate tokens via a weighted sum.\n",
        "          4. Classify the aggregated feature vector.\n",
        "        \"\"\"\n",
        "        # Get patch token embeddings; expected shape: [batch, num_tokens, embed_dim]\n",
        "        tokens = self.vit.forward_features(x)\n",
        "        # Compute attention scores for each token; shape: [batch, num_tokens, 1]\n",
        "        attn_scores = self.attention_layer(tokens)\n",
        "        # Normalize attention scores using softmax along the token dimension\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        # Compute the weighted sum of token embeddings to form a global feature vector\n",
        "        weighted_feature = torch.sum(attn_weights * tokens, dim=1)  # Shape: [batch, embed_dim]\n",
        "        # Pass the aggregated features through the classifier head\n",
        "        out = self.classifier(weighted_feature)\n",
        "        return out\n",
        "\n",
        "# Initialize the enhanced ViT model and send it to the device\n",
        "model = EnhancedViT().to(device)\n",
        "\n",
        "# Define loss function and optimizer with L2 regularization (weight decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Training loop with learning rate scheduler and checkpoint saving\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20):\n",
        "    best_acc = 0.0\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    val_precisions = []\n",
        "    val_recalls = []\n",
        "    val_f1s = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Compute average training loss for this epoch\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.6f}\")\n",
        "\n",
        "        # Validate the model and compute metrics\n",
        "        val_acc, val_precision, val_recall, val_f1 = validate(model, val_loader, criterion)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_precisions.append(val_precision)\n",
        "        val_recalls.append(val_recall)\n",
        "        val_f1s.append(val_f1)\n",
        "\n",
        "        # Step the scheduler on the validation accuracy\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # Save the \"final\" model at the end of every epoch\n",
        "        final_model_path = os.path.join(checkpoint_folder, f\"final_model_vit_{date_str}.pth\")\n",
        "        torch.save(model.state_dict(), final_model_path)\n",
        "        print(f\"Final model (epoch {epoch+1}) saved at {final_model_path}\")\n",
        "        # ---------------------------------------------------------\n",
        "\n",
        "        # Save the best model if the accuracy improves\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_path = os.path.join(checkpoint_folder, f\"best_model_vit_{date_str}.pth\")\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved with accuracy: {best_acc:.2f}% at {best_model_path}\")\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(val_precisions, label='Validation Precision')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Validation Precision')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(val_recalls, label='Validation Recall')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.title('Validation Recall')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot F1 score separately\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(val_f1s, label='Validation F1 Score')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('Validation F1 Score')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "# Validation loop returning metrics\n",
        "def validate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
        "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
        "    print(f'Validation Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Test function to evaluate on the test set\n",
        "def test(model, loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Loss: {test_loss/len(loader):.6f}, Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Step 1: Train the model with learning rate scheduling\n",
        "train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)\n",
        "\n",
        "# Step 2: Evaluate the model on the test set\n",
        "test(model, test_loader, criterion)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UMUKq2JQiHyRjGuacqhUyBd_gK5BoOhf",
      "authorship_tag": "ABX9TyPARMjqMLm70g8b4KiC0r1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5aedd8f7694b4717a11c315d2aa71229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99f8769f963049ab86b84107500f6ff0",
              "IPY_MODEL_726adb23b0a247768f560b6784006c09",
              "IPY_MODEL_a8231cdb3a1c4cc9a646283b84c6bbbd"
            ],
            "layout": "IPY_MODEL_d1c11754182e4f97b832218a7627a21b"
          }
        },
        "99f8769f963049ab86b84107500f6ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3711966e8bde46818e632632093c1ad8",
            "placeholder": "​",
            "style": "IPY_MODEL_076ea7f2dc7149dbba9993862794d482",
            "value": "model.safetensors: 100%"
          }
        },
        "726adb23b0a247768f560b6784006c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6260293e84e4fe5a7130e21e80f2deb",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b62163c2c414ecda0da1b5a09a86b13",
            "value": 346284714
          }
        },
        "a8231cdb3a1c4cc9a646283b84c6bbbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86564363ca99462bb3f38bb69c09ad8c",
            "placeholder": "​",
            "style": "IPY_MODEL_c958346b4d024c9e937e726514d085d2",
            "value": " 346M/346M [00:05&lt;00:00, 56.6MB/s]"
          }
        },
        "d1c11754182e4f97b832218a7627a21b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3711966e8bde46818e632632093c1ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "076ea7f2dc7149dbba9993862794d482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6260293e84e4fe5a7130e21e80f2deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b62163c2c414ecda0da1b5a09a86b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86564363ca99462bb3f38bb69c09ad8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c958346b4d024c9e937e726514d085d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}