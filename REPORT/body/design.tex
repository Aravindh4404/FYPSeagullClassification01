\chapter{Design}

\section{Methodology: Model Development \& Interpretability Implementation}

\subsection{Architectural Design Philosophy}
The model development process followed three core principles derived from recent advances in fine-grained classification research:
\begin{itemize}
    \item \textbf{Hierarchical Feature Learning:} Prioritize architectures capable of capturing multi-scale features (local textures and global patterns) \cite{source3, source11}.
    \item \textbf{Attention-Aware Processing:} Incorporate mechanisms to focus on discriminative regions without manual part annotations \cite{source9, source18}.
    \item \textbf{Interpretability by Design:} Ensure architectural compatibility with gradient-based visualization techniques \cite{source6, source12}.
\end{itemize}

This framework guided the selection and modification of baseline architectures to address the specific challenges of gull wingtip analysis.

\subsection{Model Implementations}

\subsubsection{CNN Architectures}
\paragraph{ResNet-50 Adaptation}
\begin{itemize}
    \item \textbf{Rationale:} Residual connections enable stable training on limited data by preserving gradient flow \cite{source13, source18}.
    \item \textbf{Implementation:}
    \begin{enumerate}
        \item Load ImageNet-pre-trained ResNet-50.
        \item Replace final FC layer with:
        \begin{itemize}
            \item Dropout(0.5)
            \item Linear(in\_features=2048, out\_features=2)
        \end{itemize}
        \item Freeze initial blocks, progressively unfreeze during fine-tuning.
    \end{enumerate}
\end{itemize}

\paragraph{Inception-v3 Enhancement}
\begin{itemize}
    \item \textbf{Rationale:} Multi-scale feature extraction crucial for analyzing variable wingtip sizes \cite{source3, source11}.
    \item \textbf{Implementation:}
    Add batch normalization before final FC layer and modify auxiliary classifiers.
\end{itemize}

\paragraph{VGG16 Optimization}
Shallow architecture enables precise gradient flow for interpretability. Added spatial dropout (p=0.2) before final pooling and implemented gradient clipping (max\_norm=2.0).

...

% Continue similarly for other sections

\subsection{Vision Transformer Adaptations}

\paragraph{Base ViT Implementation}
\begin{itemize}
    \item \textbf{Rationale:} Global attention captures wingtip-body relationships \cite{source11, source16}.
    \item \textbf{Architectural Details:}
    Replace the head with:
    \begin{enumerate}
        \item LayerNorm
        \item Dropout(0.3)
        \item Linear(768 $\rightarrow$ 2)
    \end{enumerate}
\end{itemize}

...

% Add more subsections as needed

\subsection{Data Augmentation Strategy}
The augmentation pipeline included:
\begin{itemize}
    \item Geometric Transformations:
    Random rotation (\(\pm 10^\circ\)), affine translation (5\%), and random resized crop (95--100\% scale).
    \item Photometric Adjustments:
    Color jitter (brightness/contrast/saturation = 0.1) and sharpness adjustment (factor = 1.2).
    \item Normalization:
    Using ImageNet statistics.
\end{itemize}

Research showed that controlled geometric augmentations improved bird classification accuracy by 18\% and subtle color variations reduced overfitting to lighting conditions by 42\% (\cite{source7, source14}).

...

% Continue adding content as per your requirements

%\bibliographystyle{plain} % Use a style like IEEEtran or apalike if needed
\bibliography{bibliography/bibliography} % Ensure the .bib file name matches here
