%! Author = Aravindh P
%! Date = 07-04-2025

% Preamble


\section{ResNet50 Architecture}

\subsection{Theoretical Background}

Residual Networks (ResNet) represent a significant innovation in deep neural network architecture, introduced by He et al. (2016) to address the degradation problem that occurs when training very deep networks. The key innovation in ResNet is the introduction of skip connections or "shortcut connections" that bypass one or more layers, allowing gradients to flow more easily through the network during backpropagation\citep{residual2016}. This design enables the training of much deeper networks than was previously feasible, with ResNet-50 containing 50 layers organized in residual blocks\citep{resnet50blog}.

ResNet-50 architecture consists of five stages, each containing multiple residual blocks. Each residual block contains a "shortcut" that skips over the main path and rejoins it later, allowing the network to learn residual functions with reference to the layer inputs rather than learning unreferenced functions\citep{residual2016}\citep{exploratiojournal}. This approach enables ResNet to achieve high performance on image classification tasks while mitigating the vanishing gradient problem common in very deep networks.

The architecture's ability to effectively extract hierarchical features through its deep structure makes it particularly well-suited for fine-grained classification tasks where subtle differences must be detected. As noted by Ghani et al. (2023), ResNet architectures have demonstrated strong performance in avian classification tasks due to their capacity to learn discriminative features at multiple scales and levels of abstraction.

\subsection{Model Adaptation for Gull Species Classification}

For our gull classification task, we adapted the pre-trained ResNet-50 model using a focused transfer learning approach. The model was initialized with weights pre-trained on the ImageNet dataset, providing a strong foundation of general visual features. The adaptation process involved:

\begin{enumerate}
    \item Loading the pre-trained ResNet-50 model with ImageNet weights
    \item Preserving the convolutional backbone to maintain feature extraction capabilities
    \item Replacing the final fully connected layer (classifier) with a custom sequence:
    \begin{enumerate}
        \item Dropout layer with probability 0.5 to reduce overfitting
        \item Linear layer mapping from 2048 features to 2 output classes
    \end{enumerate}
\end{enumerate}

This adaptation strategy preserved ResNet-50's powerful feature extraction capabilities while customizing the classification head for our binary task. The relatively high dropout rate (0.5) was implemented to address potential overfitting, which is particularly important given the visual similarities between the target species and the limited size of our specialized dataset\citep{stackoverflow}.

\subsection{Image Preprocessing and Enhancement}

A distinctive aspect of our ResNet-50 implementation was the incorporation of image sharpening as a preprocessing step. This approach was motivated by research from Zhou et al. (2021) showing that enhancing edge definition can improve the detection of subtle morphological features in avian classification tasks. The image enhancement process applied a 3$\times$3 sharpening kernel through a custom preprocessing function:

\begin{center}
Sharpening Kernel: \\
$\begin{bmatrix}
0 & -1 & 0 \\
-1 & 5 & -1 \\
0 & -1 & 0
\end{bmatrix}$
\end{center}

This technique enhanced the visibility of critical diagnostic features like wingtip patterns while preserving the overall image content. To ensure consistency, image sharpening was applied across both training and evaluation pipelines\citep{jatit}.

\subsection{Data Augmentation Strategy}

The data augmentation pipeline for ResNet-50 was structured to enhance model robustness while preserving class-discriminative features:

\begin{itemize}
    \item Resize operations (300$\times$300 pixels) followed by sharpening
    \item Random horizontal flipping to simulate viewpoint variation
    \item Random rotation ($\pm$15 degrees) to account for flight angle variability
    \item Color jittering (brightness, contrast, saturation adjusted by $\pm$20\%)
    \item Random cropping with padding to vary focus regions
\end{itemize}

For validation and testing, a more conservative approach was employed with resizing, center cropping (256$\times$256 pixels), and the same sharpening preprocessing to maintain feature clarity without introducing variability\citep{jatit}.

\subsection{Training Approach and Optimization}

The ResNet-50 model was trained using the following methodological approach:

\begin{itemize}
    \item Adam optimizer with learning rate 0.001 and weight decay 1e-4 for regularization
    \item Adaptive learning rate scheduling using ReduceLROnPlateau with patience=3
    \item Early stopping with patience=5 to prevent overfitting
    \item Batch size of 16 for efficient GPU utilization
\end{itemize}

The implementation of early stopping was particularly valuable for the ResNet model, as it helped prevent overfitting to the training data while ensuring the model retained its generalization capabilities. As demonstrated by Huang et al. (2022), early stopping acts as an effective regularization technique for deep networks when working with specialized datasets of limited size\citep{mdpi}.

