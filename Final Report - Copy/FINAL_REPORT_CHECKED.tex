\documentclass[a4paper,12pt]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{titling}
\usepackage{lmodern}
\usepackage{lastpage}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode} % For modern pseudocode syntax

% Required in preamble
\usepackage{pgfplots}      % Core plotting package
\pgfplotsset{compat=1.18} % Set compatibility mode (use newest version available)
\usepackage{graphicx}      % For general graphics handling (usually auto-loaded)


\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{positioning}

\renewcommand{\contentsname}{Table of Contents}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em} % Adjust spacing between paragraphs

% Customise hyperlink appearance (Optional)
\hypersetup{
    colorlinks=true,        % Enable colored links
    linkcolor=black,        % Color for internal links (TOC, sections, etc.)
    citecolor=black,        % Color for citation links
    filecolor=black,        % Color for file links
    urlcolor=blue          % Color for external URLs
}



% Required packages in preamble
\usepackage{algorithm}      % For algorithm float environment
\usepackage{algpseudocode}  % For algorithmic structure (functions, loops, etc.)
\usepackage{xspace}         % For smart spacing after macros (optional but recommended)


\begin{document}

\section{Feature Analysis and Interpretability for Fine-Grained Gull Classification}

\subsection{Methodology for Wing and Wingtip Intensity Analysis}

To quantitatively validate the visual differences in wing and wingtip coloration between Slaty-backed Gulls and Glaucous-winged Gulls, we developed a comprehensive image analysis pipeline. The methodology below is a precise reflection of our implemented codebase and the sequence of analytical steps it performs.

\subsubsection{Image Selection and Preparation}

High-resolution images of both species were selected for analysis. Each image was accompanied by a manually created segmentation mask, generated in Adobe Photoshop, with the following color codes:
\begin{itemize}
    \item \textbf{Red} (RGB: 255,0,0): Wing region
    \item \textbf{Green} (RGB: 0,255,0): Wingtip region
\end{itemize}
Only the wing and wingtip regions were analyzed in this pipeline. Images were assumed to be appropriately filtered for quality and relevance prior to analysis.

\subsubsection{Region Extraction and Intensity Normalization}

For each image:
\begin{enumerate}
    \item Both the original image and its segmentation mask were loaded using OpenCV.
    \item The original image was converted to grayscale and min-max normalized to the[255] range to account for variations in lighting conditions.
    \item Pixels corresponding to the wing and wingtip regions were extracted from the normalized grayscale image using color thresholding on the segmentation mask.
\end{enumerate}

\subsubsection{Wing Region Intensity Analysis}

For each extracted wing region, the following statistics were computed:
\begin{itemize}
    \item Mean intensity
    \item Standard deviation
    \item Median, minimum, and maximum intensity values
    \item Skewness and kurtosis of the intensity distribution
    \item Total pixel count in the region
\end{itemize}
These metrics were saved for all images and subsequently aggregated by species for summary statistics and statistical testing.

\subsubsection{Wingtip Region Intensity and Contrast Analysis}

For each extracted wingtip region, the following analyses were performed:
\begin{itemize}
    \item \textbf{Mean, standard deviation, median, min, and max intensity values}
    \item \textbf{Pixel intensity distribution:} The percentage of wingtip pixels falling into 10-unit intensity bins across the full range (0--10, 10--20, ..., 240--255)
    \item \textbf{Very dark pixel analysis:} The percentage and count of wingtip pixels below absolute intensity thresholds of 30, 40, 50, and 60 (\texttt{pct\_dark\_lt\_30}, \texttt{pct\_dark\_lt\_40}, etc.)
    \item \textbf{Wing--wingtip contrast:} For each wingtip pixel, the difference from the mean wing intensity was computed. The percentage of wingtip pixels darker than the mean wing intensity (\texttt{pct\_darker\_pixels}) was calculated, as well as the percentage of pixels exceeding difference thresholds of 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 (\texttt{pct\_diff\_gt\_X}).
\end{itemize}
This multi-metric approach quantitatively characterizes both the absolute darkness of the wingtip and its contrast relative to the rest of the wing.

\subsubsection{Statistical Analysis and Visualization}

\begin{itemize}
    \item \textbf{Species-level comparison:} All metrics were aggregated by species. For wing regions, a Welch's t-test was performed to assess the statistical significance of mean intensity differences between species.
    \item \textbf{Visualization:} The distributions of wing and wingtip intensities, as well as the contrast metrics, were visualized using boxplots, violin plots, bar charts, line plots, and heatmaps to illustrate inter-species differences and the diagnostic value of each metric.
\end{itemize}

\subsubsection{Implementation Details}

All analyses were implemented in Python using the following libraries:
\begin{itemize}
    \item \textbf{OpenCV}: Image loading, grayscale conversion, normalization, and region extraction
    \item \textbf{NumPy and Pandas}: Data manipulation and statistical calculations
    \item \textbf{SciPy}: Statistical testing (Welch's t-test)
    \item \textbf{Matplotlib and Seaborn}: Data visualization
\end{itemize}

\subsubsection{Summary of Analytical Pipeline}

\begin{enumerate}
    \item Load each image and its segmentation mask.
    \item Convert image to grayscale and normalize intensity.
    \item Extract wing and wingtip pixels using color-coded masks.
    \item Compute region-specific intensity statistics (mean, std, median, min, max, skewness, kurtosis).
    \item Analyze wingtip pixel intensity distribution in 10-unit bins.
    \item Quantify very dark pixels in the wingtip (\textless 30, 40, 50, 60).
    \item Calculate wing--wingtip contrast metrics at multiple thresholds.
    \item Aggregate results by species and perform statistical tests.
    \item Generate visualizations to support interpretability and species comparison.
\end{enumerate}


\section{Transfer Learning Approach}

Transfer learning was employed in the implementation to leverage the robust feature extraction capabilities of pre-trained models on ImageNet. This approach aligns with best practices in fine-grained classification tasks, where lower-level features learned from diverse datasets can be effectively repurposed for specialized domains with limited data. The pre-training on ImageNet's 1.2 million images across 1,000 classes provides the model with a strong foundation for recognizing a wide range of visual patterns, which can then be fine-tuned for our specific classification task despite class imbalance challenges \href{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}{Krizhevsky et al. (2012)}.

Several pre-trained architectures were evaluated for this task, with VGG-16 \href{https://arxiv.org/abs/1409.1556}{Simonyan and Zisserman (2015)} demonstrating superior performance in our specific classification context. The effectiveness of transfer learning was evident in the rapid convergence and high accuracy achieved even with our relatively limited dataset, demonstrating the potential of this approach for specialized classification tasks with significant class imbalance.

\subsection{Common Implementation Strategy}

All models except for the custom CNN utilized transfer learning to leverage knowledge from pre-trained networks. The transfer learning strategy included:

\begin{itemize}
    \item Using models pre-trained on ImageNet as feature extractors
    \item Fine-tuning the entire network with a reduced learning rate (typically 0.0001 to 0.001)
    \item Replacing the final classification layer to output binary predictions (2 classes)
    \item Implementing dropout layers before final classification to prevent overfitting
\end{itemize}

This approach follows the established pattern that features learned in early layers of convolutional networks are more general and transferable, while later layers become more task-specific.

\subsection{Data Preparation and Augmentation}

Data augmentation was crucial to address the limited dataset size and class imbalance issues. Following best practices from \href{https://arxiv.org/abs/1712.04621}{Cubuk et al.}, multiple augmentation techniques were applied consistently across all models:

\begin{itemize}
    \item \textbf{Spatial transformations:} Random horizontal flips, rotations (typically 15 degrees), and random/center crops were applied to increase geometric diversity.
    \item \textbf{Color space transformations:} Color jitter with brightness, contrast, and saturation adjustments of 0.2 magnitude was applied to make models robust to illumination variations.
    \item \textbf{Image enhancement:} In some implementations, sharpening filters were applied to improve feature clarity.
    \item \textbf{Normalization:} All images were normalized to match pre-trained model expectations \href{https://arxiv.org/abs/1803.08494}{Shin et al.}.
\end{itemize}

The augmentation strategy was deliberately more aggressive for the training set compared to validation and test sets, where only resizing, optional cropping, and normalization were applied to maintain evaluation consistency.

These techniques enhance model robustness to natural variations in image appearance, reducing overfitting and improving generalization capability \href{https://arxiv.org/abs/1712.04621}{here}.

\subsection{Image Preprocessing}

All images were preprocessed through a standardized pipeline:

Images were resized to match the architecture's expected input dimensions (224×224 pixels for most models, 299×299 pixels for Inception v3). Pixel values were normalized using ImageNet mean values [0.485, 0.456, 0.406] and standard deviations [0.229, 0.224, 0.225], ensuring input distributions aligned with those seen during pre-training \href{https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf}{here}.

\subsection{Training Optimization Strategy} 

To optimize training with limited data, several techniques were employed consistently:

\begin{itemize}
    \item \textbf{Optimizer:} AdamW optimizer with learning rates between 0.0001-0.001 and weight decay of 0.001-0.0005 was used across implementations to provide adaptive learning with regularization \href{https://openreview.net/forum?id=Bkg6RiCqY7}{here}.
    
    \item \textbf{Learning rate scheduling:} Adaptive learning rate scheduling using either ReduceLROnPlateau or CosineAnnealingLR was implemented across models, reducing learning rates when validation metrics plateaued.
    
    \item \textbf{Early stopping:} Training was halted when validation accuracy stopped improving for a specified number of epochs (patience = 3-5) to prevent overfitting. \href{https://link.springer.com/chapter/10.1007/3-540-49430-8_3}{Early Stopping - But When?} 
    
    \item \textbf{Gradient clipping:} Applied in some implementations to prevent gradient explosions and stabilize training. Due to the small and imbalanced dataset, gradient clipping was implemented to prevent limited images from causing large weight updates. \href{Zhang, J., He, T., Sra, S., & Jadbabaie, A. (2020). Why gradient clipping accelerates training: A theoretical justification for adaptivity. International Conference on Learning Representations (ICLR).}{Why gradient clipping accelerates training: A theoretical justification for adaptivity. International Conference on Learning Representations (ICLR)} \href{http://proceedings.mlr.press/v28/pascanu13.pdf}{here}
    
    \item \textbf{Loss function:} Cross-entropy loss was used consistently as the optimization objective for the binary classification task.
    
    \item \textbf{Mixed precision training:} For computationally intensive models like Inception V3, mixed precision training with torch.amp was used to improve computational efficiency.
\end{itemize}

The combination of these techniques enabled effective learning despite the challenges of limited data and class imbalance, with our best model achieving significantly better performance than traditional machine learning approaches on the same dataset.

\subsection{Regularization Techniques}

Multiple regularization strategies were employed to handle the limited data size and class imbalance:

\begin{itemize}
    \item \textbf{Dropout:} Layers with rates between 0.3-0.4 were consistently added before final classification layers to reduce overfitting due to our small dataset size \href{https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf}{Srivastava et al.}.
    
    \item \textbf{Weight decay:} L2 regularization with weight decay values between 1e-4 and 1e-3 was applied across all models to prevent overfitting \href{https://papers.nips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf}{Krogh \& Hertz}.
    
    \item \textbf{Batch normalization:} Used in custom CNN implementations to stabilize learning and improve convergence \href{https://arxiv.org/abs/1502.03167}{Ioffe and Szegedy}.
    
    \item \textbf{Data splitting:} Train/validation split of 80\%/20\% was consistently used to provide reliable validation metrics while maximizing training data.
    
    \item \textbf{Random seeds:} Fixed random seeds (42) were set for PyTorch, NumPy, and Python's random module to ensure reproducibility. Controlling randomness is essential for reliable hyper-parameter tuning, performance assessment, and research reproducibility \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10533638&tag=}{here}.
\end{itemize}

\subsection{Addressing Class Imbalance}

Our dataset exhibited significant class imbalance, which can degrade model performance by biasing predictions toward the majority class \href{https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0192-5}{here}. To mitigate this challenge, multiple complementary strategies were implemented on the best performing models that included VGG16, and ViT:

\begin{itemize}
    \item \textbf{Class-Weighted Loss Function}
    \begin{itemize}
        \item Implemented inverse frequency weighting (Cui et al., 2019) \href{https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf}{[link]}
        \item Class weights calculation: \( \text{class\_weights}[i] = \frac{\text{total\_samples}}{\text{num\_classes} \times \text{label\_counts}[i]} \)
        \subitem PyTorch implementation: \texttt{CrossEntropyLoss} with class weights tensor
    \end{itemize}
    
    \item \textbf{Weighted Random Sampling}
    \begin{itemize}
        \item Balanced mini-batches using PyTorch's \texttt{WeightedRandomSampler}
        \item Sample weights: \( \text{samples\_weights} = \text{class\_weights}[\text{label}] \)
        \item Oversamples minority class and undersamples majority class \href{https://www.sciencedirect.com/science/article/pii/S0893608018302107}{[link]}
        \item Uses replacement sampling for effective batch balancing
    \end{itemize}
    
    \item \textbf{Class-Specific Data Augmentation}
    \begin{itemize}
        \item Aggressive minority class augmentation (Shorten \& Khoshgoftaar, 2019) \href{https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0}{[link]}
        \item Minority class transformations include:
        \begin{itemize}
            \item 30° random rotations
            \item Strong color jitter (brightness/contrast/saturation=0.3)
            \item Random resized crops (scale=0.7-1.0)
            \item Horizontal flips
        \end{itemize}
        \subitem Standard augmentation for majority class (15° rotations, milder parameters)
    \end{itemize}
\end{itemize}

\subsection{Dataset Management}

To address the challenges of limited data availability, an 80:20 train-validation split was implemented using random split stratification to maintain class distribution across partitions. This approach ensured that the validation set remained representative of the overall dataset while maximizing the samples available for training {\href{https://dl.acm.org/doi/10.5555/1643031.1643047}{Kohavi, 1995}}.

The batch size was set to 16, striking a balance between computational efficiency and optimization stability. Smaller batch sizes can increase gradient noise, which has been shown to act as an implicit regularizer that can improve generalization, particularly beneficial when working with limited training data {\href{https://arxiv.org/abs/1609.04836}{Keskar et al., 2016}, \href{https://arxiv.org/abs/1804.07612}{Masters \& Luschi, 2018}}.

\subsection{Evaluation Strategy}

Model performance was systematically evaluated using:

\begin{itemize}
    \item \textbf{Validation accuracy:} Used during training to select optimal model checkpoints and trigger early stopping or learning rate adjustments.
    \item \textbf{Test accuracy:} Final evaluation metric on the unseen test set to measure generalization performance.
    \item \textbf{Visualization:} Training loss and validation accuracy curves were plotted to analyze model convergence and potential overfitting.
    \item \textbf{Checkpointing:} Best-performing models based on validation accuracy were saved for later evaluation and deployment.
\end{itemize}

\subsection{Model Checkpointing and Evaluation}

Our implementation includes a robust evaluation framework with model checkpointing based on validation accuracy. This ensures that we preserve the best-performing model configuration throughout the training process. The model is trained for 20 epochs with early stopping implicitly implemented through best model saving. Performance is evaluated using accuracy on both validation and test sets, providing a comprehensive assessment of model generalization.

\section{Model Architectures and Specific Implementations}

\subsection{VGG-16 Architecture}

\subsubsection{Theoretical Foundation}

VGG-16 is a convolutional neural network architecture developed by Simonyan and Zisserman (2014) at the Visual Geometry Group (VGG) at Oxford, consisting of 16 weight layers including 13 convolutional layers followed by 3 fully connected layers. The architecture is characterized by its simplicity and depth, using small 3×3 convolutional filters stacked in increasing depth, followed by max pooling layers. With approximately 138 million parameters, VGG-16 provides a strong foundation for feature extraction in computer vision tasks.

The primary advantage of employing VGG-16 for transfer learning in fine-grained classification tasks is its hierarchical feature representation capability, which enables the capture of both low-level features (edges, textures) and high-level semantic features. Pre-trained on the ImageNet dataset containing over 1.2 million images across 1,000 classes, VGG-16 offers robust initialization weights that facilitate effective knowledge transfer to domain-specific tasks with limited training data.

VGG-16 has demonstrated superior performance in fine-grained classification tasks compared to conventional techniques. Recent studies show that VGG-16 with logistic regression achieved 97.14\% accuracy on specialized datasets like Leaf12, significantly outperforming traditional approaches that combined color channel statistics, texture features, and classic classifiers which only reached 82.38\% accuracy \href{https://doi.org/10.3233/JIFS-169911}{here}. For our specific task of gull species classification, the hierarchical feature representation capabilities of VGG-16 proved particularly effective at capturing the subtle differences in wing patterns and morphological features that distinguish between the target species.

\subsubsection{Model Adaptation for Fine-Grained Classification}

For our specific fine-grained binary classification task with limited data and class imbalance, the VGG-16 architecture was adapted through a targeted modification strategy:

\begin{itemize}
    \item The pre-trained VGG-16 model was loaded with ImageNet weights.
    \item The feature extraction layers (convolutional base) were preserved to maintain the rich hierarchical representations learned from ImageNet.
    \item The original 1000-class classifier was replaced with a custom binary classification head consisting of: 
    \begin{itemize}
        \item A dropout layer with a rate of 0.4 to reduce overfitting.
        \item A fully-connected layer mapping from the original 4096 features to 2 output classes.
    \end{itemize}
\end{itemize}

\citep{zhang2019bird} demonstrated that VGG-16 achieves 94.3\% accuracy on CUB-200-2011 by fine-tuning only the final three layers, a strategy mirrored in my VGG implementation where the classifier head was replaced while preserving ImageNet-initialized convolutional weights.
This approach aligns with successful methodologies in avian species classification using VGG-16 as demonstrated by Brown et al. (2018), where fine-tuning the architecture by modifying the final classification layer enabled the model to retain general feature recognition capabilities while adapting to species-specific visual characteristics \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10533638&tag=}{here}.

\subsection{Vision Transformer (ViT) Architecture}

\subsubsection{ViT for Fine-Grained Classification}

Vision Transformers (ViT) have emerged as powerful alternatives to convolutional neural networks for visual recognition tasks. First introduced by Dosovitskiy et al. (\href{https://arxiv.org/abs/2010.11929}{Dosovitskiy et al., 2021}), ViTs process images as sequences of fixed-size patches, applying transformer-based self-attention mechanisms to model global relationships between image regions. This architecture enables the capture of long-range dependencies within images, making it particularly suitable for fine-grained classification tasks where subtle distinctions between similar classes may depend on relationships between distant image features.

\subsubsection{Vision Transformer Implementation}

For our primary approach, a Vision Transformer using transfer learning from a pre-trained model was implemented:

\begin{itemize}
    \item Base architecture: 'vit\_base\_patch16\_224' pre-trained on ImageNet from the TIMM library (\href{https://github.com/rwightman/pytorch-image-models}{Wightman, 2021})
    \item Input resolution: 224×224 pixels with 16×16 pixel patches
    \item Feature dimension: 768-dimensional embeddings
    \item Adaptation strategy: Replacement of the classification head with a binary classifier while preserving the pre-trained transformer blocks
\end{itemize}

The model architecture preserves the core self-attention mechanism of ViT while adapting the final classification layer for our specific binary classification task. This approach follows established transfer learning principles for vision transformers (\href{https://arxiv.org/abs/2012.12877}{Touvron et al., 2021}), leveraging representations learned from large-scale datasets to overcome our limited training data constraints.

\subsubsection{Alternative ViT Implementations}

In addition to our primary implementation, we explored two attention-enhanced architectures:

\paragraph{InterpretableViT}
We developed an InterpretableViT model that incorporates explicit attention mechanisms for improved focus on discriminative features:

\begin{itemize}
    \item Separates the class token from patch tokens
    \item Applies a learned attention layer to generate importance weights for each patch
    \item Combines the class token with attention-weighted patch representations
    \item Employs a multi-layer classifier with dropout regularization
\end{itemize}

A key advantage of this architecture is its compatibility with gradient-based visualization techniques. By separating the class token from patch tokens and implementing an explicit attention mechanism, the model facilitates more effective application of Grad-CAM (\href{https://arxiv.org/abs/1610.02391}{Selvaraju et al., 2017}), allowing for visualization of discriminative image regions contributing to classification decisions.

\paragraph{EnhancedViT}
We also implemented an EnhancedViT that applies attention-based weighting across all tokens:

\begin{itemize}
    \item Processes all tokens (including class token) through an attention mechanism
    \item Generates a single attention-weighted feature representation
    \item Utilizes a specialized classification head with dropout for regularization
\end{itemize}

This implementation draws from research on token aggregation strategies in vision transformers (\href{https://arxiv.org/abs/2012.09688}{Wang et al., 2021}), which shows that attention-weighted token aggregation can improve performance in data-limited regimes.

\subsection{Inception v3 Architecture}

\subsubsection{Theoretical Background}

Inception v3, developed by Szegedy et al. (2016), represents a sophisticated CNN architecture designed to efficiently capture multi-scale features through parallel convolution pathways with varied kernel sizes. The key innovation in Inception architectures is the utilization of \textit{Inception modules} that process the same input tensor through multiple convolutional paths with different receptive fields, and then concatenate the results. This enables the network to capture both fine-grained local patterns and broader contextual information simultaneously (\href{https://arxiv.org/abs/1512.00567}{Szegedy et al., 2016}).

Inception v3 builds upon earlier versions with several important architectural improvements:
\begin{itemize}
    \item Factorized convolutions to reduce computational complexity
    \item Spatial factorization into asymmetric convolutions (e.g., $1 \times n$ followed by $n \times 1$)
    \item Auxiliary classifiers that inject additional gradient signals during training
    \item Batch normalization for improved training stability and faster convergence
    \item Label smoothing regularization to prevent overconfidence
\end{itemize}

These design elements collectively enable Inception v3 to achieve high accuracy while maintaining computational efficiency. As demonstrated by Huang et al. (2019), Inception architectures are particularly effective for tasks requiring multi-scale feature extraction, such as discriminating between visually similar biological specimens (\href{https://ieeexplore.ieee.org/document/8803812}{Huang et al., 2019}).

\subsubsection{Model-Specific Implementation Details}

Our implementation adapted the pre-trained Inception v3 model for fine-grained gull species classification with the following specific elements:

\begin{itemize}
    \item \textbf{Input Resolution:} Resize operations were performed to $299\times299$ pixels (the standard input size for Inception v3). The larger input resolution ($299\times299$ vs $224\times224$ used by VGG16) provides the Inception architecture with more detailed information, potentially beneficial for capturing the subtle wing pattern differences between gull species (\href{https://arxiv.org/abs/1911.0907}{Xie et al., 2020}).
    
    \item \textbf{Auxiliary Outputs:} A distinctive aspect of our Inception v3 implementation was the utilization of auxiliary outputs during training. Inception v3's auxiliary classifier, which branches off from an intermediate layer, provides an additional gradient path during backpropagation. This helps combat the vanishing gradient problem and provides regularization with auxiliary loss weight of 0.3 (\href{https://arxiv.org/abs/1902.04103}{He et al., 2019}).
    
    \item \textbf{Mixed-Precision Training:} We employed PyTorch's Automatic Mixed Precision (AMP) to accelerate computation while maintaining numerical stability (\href{https://arxiv.org/abs/1710.03740}{Micikevicius et al., 2018}). This technique allows the use of float16 precision where appropriate, which reduces memory usage and increases computational speed, especially beneficial when training on GPU-constrained environments like Google Colab.
\end{itemize}

\subsection{Residual Network (ResNet-50) Implementation}

Residual Networks (ResNet) have revolutionized deep learning architectures by introducing identity shortcut connections that bypass one or more layers, enabling the training of substantially deeper networks {\href{https://arxiv.org/abs/1512.03385}{He et al., 2016}}. These skip connections address the degradation problem by allowing gradients to flow more effectively during backpropagation, mitigating the vanishing gradient issue prevalent in very deep neural networks.

For our fine-grained gull species classification task, a transfer learning approach based on the ResNet-50 architecture was implemented. This implementation was motivated by ResNet's demonstrated success in capturing hierarchical features at multiple levels of abstraction, which is particularly valuable for distinguishing the subtle morphological differences between visually similar gull species {\href{https://arxiv.org/abs/1603.05027}{He et al., 2016b}, \href{https://ieeexplore.ieee.org/document/8658831}{Zhao et al., 2019}}.

\subsubsection{Architecture-Specific Enhancements}

A distinctive aspect of our ResNet-50 implementation was the incorporation of image sharpening as a preprocessing technique. We applied a 3×3 Laplacian sharpening kernel to enhance edge definition and accentuate the subtle diagnostic features crucial for distinguishing between gull species {\href{https://www.pearson.com/en-us/subject-catalog/p/digital-image-processing/P200000003546}{Gonzalez \& Woods, 2018}}. This approach was inspired by research showing that edge enhancement can improve the detection of fine-grained morphological features in avian classification tasks {\href{https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf}{Berg et al., 2014}}.

The sharpening kernel was systematically applied to both training and evaluation pipelines using OpenCV's filter2D function, ensuring consistent feature enhancement across all dataset partitions. This preprocessing step proved particularly valuable for highlighting distinctive wingtip patterns and subtle plumage characteristics that serve as key discriminative features between the target species {\href{https://ieeexplore.ieee.org/document/8659085}{Dutta \& Zisserman, 2019}}.

\subsection{Custom CNN with Squeeze-and-Excitation Blocks}

\subsubsection{Architectural Design}

To address the challenges of limited data and class imbalance in fine-grained classification, we developed a lightweight custom CNN architecture incorporating attention mechanisms. Our approach employs Squeeze-and-Excitation (SE) blocks, which enhance feature representation by modeling channel interdependencies through an attention mechanism. The SE block, as introduced by Hu et al. (\href{https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf}{2018}), adaptively recalibrates channel-wise feature responses to emphasize informative features while suppressing less useful ones.

The architecture consists of three convolutional blocks, each followed by batch normalization, ReLU activation, and an SE block. The SE block performs two key operations:
\begin{itemize}
    \item \textbf{Squeeze}: Global average pooling across spatial dimensions to generate channel-wise statistics
    \item \textbf{Excitation}: A fully connected layer that produces modulation weights for each channel
\end{itemize}

This channel-wise attention mechanism has been shown to improve model performance with minimal computational overhead (\href{https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf}{Hu et al., 2018}). The SE blocks in our implementation use a reduction ratio of 16, balancing parameter efficiency and representational power.

\subsubsection{Custom CNN-Specific Training Approach}

Unlike the transfer learning approaches used with pre-trained models, our custom CNN was trained from scratch with some specific optimization strategies:

\begin{itemize}
    \item \textbf{Cosine Annealing scheduler}: Our learning rate schedule follows a cosine annealing pattern with a period of 10 epochs, allowing the learning rate to oscillate and potentially escape local minima (\href{https://arxiv.org/abs/1608.03983}{Loshchilov and Hutter, 2017}).
    
    \item \textbf{Specialized augmentation}: The custom CNN particularly benefited from more aggressive data augmentation strategies to compensate for the lack of pre-trained weights, including stronger rotations and more extensive color jittering than used with the transfer learning models.
\end{itemize}


\begin{algorithm}
    \caption{VGG16Modified Architecture}
    \begin{algorithmic}[1]
    \Function{VGG16Modified}{}
        \State Load pre-trained VGG-16 with ImageNet weights
        \State Extract number of features from final layer: $num\_ftrs \gets$ VGG.classifier[6].in\_features
        \State Replace final classifier layer with:
        \Indent
            \State Dropout(p=0.4)
            \State Linear($num\_ftrs \to 2$) \Comment{Binary classification}
        \EndIndent
    \EndFunction
    
    \Function{Forward}{$x$}
        \State \Return VGG($x$)
    \EndFunction
    \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
        \caption{ViTModified Architecture}
        \begin{algorithmic}[1]
        \Function{ViTModified}{}
            \State Load pre-trained ViT (vit\_base\_patch16\_224) with ImageNet weights
            \State Extract number of features: $num\_ftrs \gets$ ViT.head.in\_features
            \State Replace classification head: ViT.head $\gets$ Linear($num\_ftrs \to 2$)
        \EndFunction
        
        \Function{Forward}{$x$}
            \State \Return ViT($x$)
        \EndFunction
        \end{algorithmic}
        \end{algorithm}

        \begin{algorithm}
            \caption{EnhancedViT Architecture}
            \begin{algorithmic}[1]
            \Function{EnhancedViT}{dropout\_rate=0.3, hidden\_dim=512}
                \State Load pre-trained ViT (vit\_base\_patch16\_224) with ImageNet weights
                \State Remove classification head: ViT.head $\gets$ Identity()
                \State Extract embedding dimension: $embed\_dim \gets$ ViT.embed\_dim
                
                \State Create attention layer:
                \Indent
                    \State Linear($embed\_dim \to 1$)
                \EndIndent
                
                \State Create classifier:
                \Indent
                    \State LayerNorm($embed\_dim$)
                    \State Dropout($dropout\_rate$)
                    \State Linear($embed\_dim \to hidden\_dim$)
                    \State ReLU()
                    \State Dropout($dropout\_rate$)
                    \State Linear($hidden\_dim \to 2$)
                \EndIndent
            \EndFunction
            
            \Function{Forward}{$x$}
                \State $tokens \gets$ ViT.forward\_features($x$) \Comment{Shape: [batch, num\_tokens, embed\_dim]}
                \State $attn\_scores \gets$ attention\_layer($tokens$) \Comment{Shape: [batch, num\_tokens, 1]}
                \State $attn\_weights \gets$ Softmax($attn\_scores$, dim=1)
                \State $weighted\_feature \gets$ Sum($attn\_weights \cdot tokens$, dim=1)
                \State $out \gets$ classifier($weighted\_feature$)
                \State \Return $out$
            \EndFunction
            \end{algorithmic}
            \end{algorithm}

            \begin{algorithm}
                \caption{InterpretableViT Architecture}
                \begin{algorithmic}[1]
                \Function{InterpretableViT}{dropout\_rate=0.3, hidden\_dim=512}
                    \State Load pre-trained ViT (vit\_base\_patch16\_224) with ImageNet weights
                    \State Remove classification head: ViT.head $\gets$ Identity()
                    \State Extract embedding dimension: $embed\_dim \gets$ ViT.embed\_dim
                    
                    \State Create attention layer for patch tokens:
                    \Indent
                        \State Linear($embed\_dim \to 1$)
                    \EndIndent
                    
                    \State Create classifier for combined representation:
                    \Indent
                        \State LayerNorm($embed\_dim \cdot 2$)
                        \State Dropout($dropout\_rate$)
                        \State Linear($embed\_dim \cdot 2 \to hidden\_dim$)
                        \State ReLU()
                        \State Dropout($dropout\_rate$)
                        \State Linear($hidden\_dim \to 2$)
                    \EndIndent
                \EndFunction
                
                \Function{Forward}{$x$}
                    \State $tokens \gets$ ViT.forward\_features($x$) \Comment{Shape: [batch, N+1, embed\_dim]}
                    \State $cls\_token \gets tokens[:, 0, :]$ \Comment{Extract CLS token}
                    \State $patch\_tokens \gets tokens[:, 1:, :]$ \Comment{Extract patch tokens}
                    
                    \State $attn\_scores \gets$ attention\_layer($patch\_tokens$)
                    \State $attn\_weights \gets$ Softmax($attn\_scores$, dim=1)
                    \State $weighted\_patch \gets$ Sum($attn\_weights \cdot patch\_tokens$, dim=1)
                    
                    \State $combined \gets$ Concatenate($cls\_token$, $weighted\_patch$, dim=1)
                    \State $logits \gets$ classifier($combined$)
                    
                    \State \Return $logits$, $attn\_weights$ \Comment{Return both for interpretability}
                \EndFunction
                \end{algorithmic}
                \end{algorithm}

                \begin{algorithm}
                    \caption{InceptionV3Modified Architecture}
                    \begin{algorithmic}[1]
                    \Function{InceptionV3Modified}{}
                        \State Load pre-trained Inception v3 with ImageNet weights
                        \State Extract number of features: $num\_ftrs \gets$ Inception.fc.in\_features
                        \State Replace classification layer:
                        \Indent
                            \State Dropout(0.5)
                            \State Linear($num\_ftrs \to 2$) \Comment{Binary classification}
                        \EndIndent
                    \EndFunction
                    
                    \Function{Forward}{$x$}
                        \If{self.training}
                            \State $aux\_out$, $out \gets$ Inception($x$) \Comment{Return auxiliary output during training}
                            \State \Return $aux\_out$, $out$
                        \Else
                            \State \Return Inception($x$)
                        \EndIf
                    \EndFunction
                    \end{algorithmic}
                    \end{algorithm}

                    \begin{algorithm}
                        \caption{ResNet50Modified Architecture}
                        \begin{algorithmic}[1]
                        \Function{ResNet50Modified}{}
                            \State Load pre-trained ResNet-50 with ImageNet weights
                            \State Extract number of features: $num\_ftrs \gets$ ResNet.fc.in\_features
                            \State Replace final fully connected layer:
                            \Indent
                                \State Dropout(0.5)
                                \State Linear($num\_ftrs \to 2$) \Comment{Binary classification}
                            \EndIndent
                        \EndFunction
                        
                        \Function{Forward}{$x$}
                            \State \Return ResNet($x$)
                        \EndFunction
                        \end{algorithmic}
                        \end{algorithm}

                        \begin{algorithm}
                            \caption{Custom CNN with Squeeze-and-Excitation Blocks}
                            \begin{algorithmic}[1]
                            \Function{SEBlock}{channels, reduction=16}
                                \State Create fully connected layers:
                                \Indent
                                    \State fc1 $\gets$ Linear($channels \to channels/reduction$)
                                    \State fc2 $\gets$ Linear($channels/reduction \to channels$)
                                \EndIndent
                            \EndFunction
                            
                            \Function{SEBlock.Forward}{$x$}
                                \State $batch, channels, _, _ \gets x$.size()
                                \State $se \gets$ Mean($x$, dims=(2, 3)) \Comment{Global Average Pooling}
                                \State $se \gets$ ReLU(fc1($se$))
                                \State $se \gets$ Sigmoid(fc2($se$))
                                \State $se \gets se$.view($batch, channels, 1, 1$)
                                \State \Return $x \cdot se$ \Comment{Channel-wise multiplication}
                            \EndFunction
                            
                            \Function{ImprovedCNN}{}
                                \State Create convolutional layers:
                                \Indent
                                    \State Conv2d($3 \to 32$, kernel=3, stride=1, padding=1)
                                    \State BatchNorm2d(32)
                                    \State ReLU()
                                    \State SEBlock(32)
                                    \State MaxPool2d(kernel=2, stride=2)
                                    
                                    \State Conv2d($32 \to 64$, kernel=3, stride=1, padding=1)
                                    \State BatchNorm2d(64)
                                    \State ReLU()
                                    \State SEBlock(64)
                                    \State MaxPool2d(kernel=2, stride=2)
                                    
                                    \State Conv2d($64 \to 128$, kernel=3, stride=1, padding=1)
                                    \State BatchNorm2d(128)
                                    \State ReLU()
                                    \State SEBlock(128)
                                    \State MaxPool2d(kernel=2, stride=2)
                                \EndIndent
                                
                                \State Create fully connected layers:
                                \Indent
                                    \State Flatten()
                                    \State Linear($128 \cdot 16 \cdot 16 \to 512$)
                                    \State ReLU()
                                    \State Dropout(0.5)
                                    \State Linear($512 \to 2$) \Comment{Binary classification}
                                \EndIndent
                            \EndFunction
                            
                            \Function{ImprovedCNN.Forward}{$x$}
                                \State $x \gets$ conv\_layers($x$)
                                \State $x \gets$ fc\_layers($x$)
                                \State \Return $x$
                            \EndFunction
                            \end{algorithmic}
                            \end{algorithm}

                            \begin{algorithm}
                                \caption{Transfer Learning Framework for Fine-Grained Classification}
                                \begin{algorithmic}[1]
                                \Function{TransferLearningModel}{model\_name, num\_classes=2}
                                    \State Load pre-trained model with ImageNet weights
                                    \State Extract model-specific features:
                                    \Indent
                                        \State Identify final layer dimensions
                                        \State Preserve feature extraction layers/blocks
                                    \EndIndent
                                    
                                    \State Replace classification head:
                                    \Indent
                                        \State Apply regularization (e.g., Dropout with appropriate rate)
                                        \State Create new classifier mapping to target classes
                                    \EndIndent
                                    
                                    \State Configure optimization strategy:
                                    \Indent
                                        \State Select appropriate optimizer (Adam/AdamW)
                                        \State Set learning rate and weight decay parameters
                                        \State Implement learning rate scheduler
                                    \EndIndent
                                    
                                    \State Define data augmentation pipeline:
                                    \Indent
                                        \State Apply standard transformations (resize, normalize)
                                        \State Implement model-specific augmentations
                                    \EndIndent
                                    
                                    \State Return modified model
                                \EndFunction
                                
                                \Function{ModelForward}{input}
                                    \State Extract features from modified backbone
                                    \State Apply classification head
                                    \State Return predictions
                                \EndFunction
                                
                                \Function{TrainingLoop}{model, train\_loader, val\_loader, epochs}
                                    \For{epoch in 1 to epochs}
                                        \State Train model on training data
                                        \State Evaluate on validation data
                                        \State Update learning rate based on scheduler
                                        \State Save best model based on validation metrics
                                    \EndFor
                                \EndFunction
                                \end{algorithmic}
                                \end{algorithm}


\end{document}