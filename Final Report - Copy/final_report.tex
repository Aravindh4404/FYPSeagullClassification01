\documentclass[a4paper,12pt]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{lastpage}
\usepackage{tabularx}

\usepackage[nottoc,numbib]{tocbibind}

  % Preamble packages:
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}




\usepackage{multicol}
\usepackage{caption}

% for apa with apa
% \usepackage{natbib} 
% for ieee with IEEEtran
\usepackage[numbers]{natbib}
\usepackage{float}
\usepackage{grffile}  % Add to preamble
\usepackage{titlesec} % Add this line for chapter formatting

\usepackage{graphicx}

\usepackage{float} % If you use [H] placement


% Required in preamble
\usepackage{pgfplots}      % Core plotting package
\pgfplotsset{compat=1.18} % Set compatibility mode (use newest version available) % For general graphics handling (usually auto-loaded)

% Required packages in preamble
\usepackage{algorithm}      % For algorithm float environment
\usepackage{algpseudocode}  % For algorithmic structure (functions, loops, etc.)
\usepackage{xspace}         % For smart spacing after macros (optional but recommended)

\usepackage{csquotes}

\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{positioning}

\renewcommand{\contentsname}{Table of Contents}


\usepackage{booktabs} % For professional quality tables
\usepackage{caption}  % For captions of figures and tables
\usepackage{subcaption} % For subfigures within a figure environment
% \usepackage[backend=biber, style=apa]{biblatex} % Example for bibliography (if needed, adapt style)
% \addbibresource{references.bib} % Link to your bibliography file
% \usepackage{hyperref} % Optional: for clickable links and references
% \usepackage[capitalise]{cleveref} % Optional: for smart cross-referencing (e.g., Figure 1, Table 2)

\usepackage{pgffor}
% MAKE Smaller
% make lists small
\usepackage{enumitem}
\setlist{nosep} % Removes extra space in itemize/enumerate

\titlespacing*{\section}{0pt}{0.5em}{0.5em}
\titlespacing*{\subsection}{0pt}{0.5em}{0.5em}


\setlength{\parindent}{1.5em}
\setlength{\parskip}{0.5em} % Adjust spacing between paragraphs

% Customise hyperlink appearance (Optional)
\hypersetup{
    colorlinks=true,        % Enable colored links
    linkcolor=black,        % Color for internal links (TOC, sections, etc.)
    citecolor=black,        % Color for citation links
    filecolor=black,        % Color for file links
    urlcolor=blue          % Color for external URLs
}

% \titleformat{\chapter}[hang]{\normalfont\huge\bfseries}{\thechapter.}{0.5em}{}

\titleformat{\chapter}[display]
  {\normalfont\Huge\bfseries}
  {}
  {0pt}
  {\vspace{-2em}}
\titlespacing*{\chapter}{0pt}{-2em}{1em}


\begin{document}

% % Cover Page
% \begin{titlepage}
%     \begin{center}

%         \textbf{\LARGE{School of Computer Science}}\\[0.5em]
%         \textbf{\Large{Faculty of Science and Engineering}}\\[0.5em]
%         \textbf{\Large{University of Nottingham}}\\[0.5em]
%         \textbf{\Large{Malaysia}}\\[5em]

%         \includegraphics[width=0.5\textwidth]{images/nottingham_logo.png}\\[3em]

%         \textbf{\Large{UG FINAL YEAR DISSERTATION REPORT}}\\[6em]
%         \textbf{\large{\textit{Interpretable Seagull classification}}}\\[6em]

%     \end{center}

%     \begin{center}
%         \begin{minipage}{0.6\textwidth}  % adjust width as needed
%             \raggedright
%             \textbf{Student's Name} \hspace{1.5cm}: Aravindh Palaniguru\\[1em]
%             \textbf{Student Number} \hspace{1.4cm}: 20511833\\[1em]
%             \textbf{Supervisor Name} \hspace{1.2cm}: Dr. Tomas Maul\\[1em]
%             \textbf{Year} \hspace{3.8cm}: 2025\\[4em]
%         \end{minipage}
%     \end{center}

%     \vfill

%     \begin{center}
%         \begin{minipage}{\textwidth}
%             \centering
%             {\fontsize{12}{10}\selectfont\textbf{SUBMITTED IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE AWARD OF}}
%             {\fontsize{12}{10}\selectfont\textbf{BACHELOR OF SCIENCE IN COMPUTER SCIENCE WITH ARTIFICIAL INTELLIGENCE (HONS)}}\\
%             {\fontsize{12}{10}\selectfont\textbf{THE UNIVERSITY OF NOTTINGHAM}}
%         \end{minipage}
%     \end{center}
% \end{titlepage}

% % Title Page
% \newpage
% \begin{titlepage}
%     \begin{center}
%         \vspace{0.1em}
%         \includegraphics[width=0.5\textwidth]{images/nottingham_logo.png}\\[6em]

%         \textbf{INTERPRETABLE SEAGULL CLASSIFICATION}\\[6em]

%         \fontsize{10}{10}{Submitted in May 2025, in partial fulfillment of the conditions of the award of the degrees B.Sc.}\\[4em]

%         Aravindh Palaniguru\\
%         School of Computer Science\\
%         Faculty of Science and Engineering\\
%         University of Nottingham\\
%         Malaysia\\[6em]

%         I hereby declare that this dissertation is all my own work, except as indicated in the text:\\[4em]

%         Signature \underline{\hspace{7cm}}\\[2em]
%         Date \hspace{1cm} \underline{\hspace{1cm}} / \underline{\hspace{1cm}} / \underline{\hspace{2cm}}
%     \end{center}
% \end{titlepage}

% Change margins for Table of Contents and subsequent pages
\newgeometry{
    margin=1in
}

% Roman numbering for preliminary pages
\pagenumbering{roman}

% % Acknowledgement
% \newpage
% \section*{\centering \normalsize{Acknowledgement}}

% % Abstract
% \newpage
% \section*{\centering \normalsize{Abstract}}


% Table of Contents
\newpage
\tableofcontents

% % List of Figures
% \newpage
% \listoffigures

% % List of Tables
% \newpage
% \listoftables

% Switch to Arabic numbering starting from Introduction
\newpage
\cleardoublepage % Ensure proper page break before switching numbering style
\pagenumbering{arabic} % Switch to Arabic numerals
\setcounter{page}{1} % Restart page numbering at 1

% Introduction
\chapter{Introduction}

Biodiversity is under unprecedented pressure due to climate change and human influence. The alarming rates at which species are disappearing indicate that the sixth mass extinction is underway \citep{Ceballos2017}. Understanding what biodiversity we have and what we stand to lose is crucial for convincing decision-makers to take appropriate conservation action. The gaps in taxonomic knowledge and shortage of experts constitute what is known as the "taxonomic impediment" \citep{taxonomicimpediment}, which hampers our ability to document and protect biodiversity effectively. Determining whether two populations can be consistently distinguished based on morphological traits remains essential for establishing taxonomic boundaries and designing appropriate conservation strategies.

Birds are frequently utilized to assess environmental quality due to their sensitivity to ecological changes and ease of observation during field studies. Researchers often rely on bird diversity as an indicator of the diversity within other species groups and the overall health of human environments. Examples include monitoring environmental changes through bird population shifts, tracking climate change via bird migration patterns, and evaluating biodiversity by counting bird species. Accurate identification of bird species is essential for detecting species diversity and conserving rare or endangered birds promoting scientific research and conservation efforts.\citep{ani13020264}

Among birds, gulls (\textit{Laridae}) present a particularly challenging case for identification due to their recent evolutionary divergence and subtle morphological differences. The wing and wingtip patterns—particularly the colour, intensity, and pattern of the primary feathers—are crucial diagnostic features for identification, yet they exhibit considerable variation within each species. 
%TODO CHECK FEATURES

The classification of gulls presents multiple challenges that make traditional identification methods problematic and inconsistent. Multiple confounding factors complicate identification \citep{adriaens2022}, including \textbf{hybridization} where species can interbreed in overlapping ranges, creating intermediate forms; \textbf{age-related variations} as juvenile and immature gulls display less distinct patterns than adults; \textbf{environmental effects} such as feather bleaching from sun exposure, contamination, and wear that can alter appearance; \textbf{seasonal moulting} where gulls undergo plumage changes throughout the year, affecting diagnostic features; and \textbf{viewing conditions} where lighting, angle, and distance significantly impact observed coloration.

As noted by \citep{ayyash2024} in \textit{The Gull Guide}:
\begin{quote}
    Gulls can be a challenging group of birds to identify. To the untrained eye, they all look alike, yet, at the same time, in the case of the large gulls, one could say that no two birds look the same.
\end{quote}

This project addresses the challenge of fine-grained analysis between two closely related gull species: the Slaty-backed Gull (\textit{Larus schistisagus}) and the Glaucous-winged Gull (\textit{Larus glaucescens}). These species, found primarily in eastern Russia and the Pacific Coast of North America, display subtle and overlapping physical characteristics that make accurate identification highly complex.

\citep{adriaens2022gulls} highlight the particular challenges with Glaucous-winged Gulls:
\begin{quote}
    Glaucous-winged Gulls also exhibit variably pigmented wingtips \ldots these differences are often chalked up to individual variation, at least by this author, but they're inconveniently found in several hybrid zones, creating potential for much confusion.
    
    The amount of variation here is disturbing because it is unmatched by any other gull species, and more so because it is not completely understood.
\end{quote}

There are many methods that can be used for classification. Manual identification to classify species requires per specimen analysis by expert taxonomists which is time consuming. Automated taxon identification systems (ATIs) could both handle routine identifications and potentially assist in identifying new species. Traditional ATIs, however, have been limited by their reliance on hand-crafted features \citep{valan}, are time-consuming hindering large-scale surveys, making them difficult to generalize across different taxonomic groups. As mentioned by \citep{Lu2024}, "While using machine learning techniques to solve the problem of fine-grained classification, traditional feature extraction methods necessitate manually designed features, such as edge detection, color histograms, feature point matching, and visual word bags, which have limited expressive capabilities and require extensive annotation details like bounding boxes and key points. The drawback of these methods lies in the extensive manual intervention required for feature selection and extraction."

Fine-grained image classification (FGIC), which focuses on identifying subtle differences between subclasses within the same category, has advanced rapidly over the past decade with the development of sophisticated deep neural network architectures. Deep learning approaches offer promising solutions to this taxonomic challenge through their ability to automatically learn discriminative features from large datasets\citep{source4}. 

Unlike traditional machine learning methods that rely on hand-engineered features, deep neural networks can detect complex patterns in high-dimensional data, making them well-suited for fine-grained visual classification tasks~\citep{valan}. Features extracted through convolution are learned automatically by multilayer convolutional neural networks, offering the model greater adaptability to various tasks and datasets, with features possessing enhanced expressive and abstract capabilities. The benefit of convolutional feature extraction is its ability to perform feature extraction and classification within the same network, with the quality and quantity of features adjustable through the network's structure and parameters \citep{source2} offering the model greater adaptability to various tasks and datasets \citep{Lu2024}. Getting good quality results in traditional machine Learning models is dependent on how good the data is labelled, whereas Deep Learning architectures don't necessarily require labelling, as Neural Networks are great at learning without guidelines~\cite{source5}.
%TODO check references

For species identification , convolutional neural networks (CNNs) such as ResNet, Inception, and VGG have demonstrated exceptional capabilities \cite{essay101313}, with studies such as \citep{transferln97} mentioned that "deep learning is more effective than traditional machine learning algorithms in image recognition as the number of bird species increases" achieving accuracy rates exceeding 97\% in bird species classification tasks. \citep{ALFATEMI2024558} compared deep learning and traditional machine learning algorithms and achieved an accuracy of 94\% tackle the challenge of classifying bird species with high visual similarity and subtle variations. These architectures automatically learn hierarchical feature representations—from low-level edges and textures to high-level semantic concepts—that capture the subtle morphological differences between closely related species.

Yet the fine-grained bird classification task has greater challenges even when using deep learning. Key issues include high intraclass variance, low inter-class variance, limited training data, intensity and pose variability, bird localization and background variability, occlusions, size variations, class imbalance \citep{10.1007/978-981-15-1387-9_3}, and limited samples\cite{source3} \citep{ani13020264}. Additionally, deep learning requires extensive data and is prone to overfitting. 

This dissertation aims to develop and evaluate deep learning models for the automated classification of the Slaty-backed Gull and the Glaucous-winged Gull, to use them to identify the most influential regions that distinguish these two species and analyse them to extract statistical features from these regions and assess their significance and validity in classification.

% Related Work
\newpage
\chapter{Literature Review}
\section*{Deep Learning for Fine-Grained Image Classification}
Fine-grained image classification presents unique challenges compared to general image classification tasks. Fine-grained classification \enquote{necessitates discrimination between semantic and instance levels, while considering the similarity and diversity among categories} \citep{source2}. This is particularly challenging in bird classification due to three key factors: high intra-class variance (birds of the same species in different postures), low inter-class variance (different species with only minor differences), and limited training data availability, especially for rare species\citep{source2}.

Early approaches to fine-grained classification relied on fixed rectangular bounding boxes and part annotations to obtain visual differences, but these methods required extensive human annotation effort. Recent research has shifted toward weakly supervised approaches that only require image-level labels, developing localization subnetworks to identify critical parts followed by classification subnetwork \citep{source2}. These models facilitate learning while maintaining high accuracy without needing pre-selected boxes, making them more practical for real-world applications.

% Recent research emphasizes that effective fine-grained classification depends on identifying and integrating information from multiple discriminative regions rather than focusing on a single region. As highlighted in recent literature, "it is imperative to integrate information from various regions rather than relying on a singular region"\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. This insight has led to the development of methods combining features from different levels via attention modules, thereby enhancing the semantic and discriminative capacity of features for fine-grained classification\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}.

% The effectiveness of Convolutional Neural Networks (CNNs) for bird species classification has been demonstrated in numerous studies. \citep{zhang2019bird} achieved 94.3\% accuracy on the Caltech-UCSD Birds (CUB-200-2011) dataset using a VGG-16 architecture, proving the viability of transfer learning for this domain. Similarly, \citep{marini2018bird} compared multiple CNN architectures for bird classification and found that deeper networks like ResNet and DenseNet consistently outperformed shallower alternatives.
% %TODO both ref above is wrong

% For extremely challenging cases with visually similar species, researchers have developed specialized techniques. \citep{he2022bird} proposed a multi-attention mechanism that dynamically focuses on discriminative regions, achieving 96.8\% accuracy on a dataset of visually similar bird species. This approach is particularly relevant to our study of gull species with subtle distinguishing characteristics.
% % TODO ref above is wrong
% USEFUL
% https://ijece.iaescore.com/index.php/IJECE/article/view/24833/15821 good


\section*{Transfer Learning for Image Classification}
Deep learning, while powerful, comes with two major constraints: dependency on extensive labeled data and high training costs\citep{iman2022review}. Transfer learning offers a solution to these limitations by enabling the reuse of knowledge obtained from a source task when training on a target task. In the context of deep learning, this approach is known as Deep Transfer Learning (DTL)\citep{iman2022review}.
%ref ok

Several studies have demonstrated the efficacy of transfer learning for bird species classification. A study on bird species identification using deep learning achieved accuracies of above 90\% by leveraging pretrained CNN networks with a base model to encode images\citep{Vo2023BirdDA}. Research on bird species identification by \citep{Mochurad2024ANE} using modified deep transfer learning achieved 98.86\% accuracy using the pretrained EfficientNetB5 model. The results with various pretrained models achieving high accuracy rates with few models exceeding 98\% demonstrate that transfer learning approaches can achieve high performance even with limited training data.
%ref double check

Various pretrained models have been evaluated for bird classification tasks, including VGG16, ResNet, DenseNet, and EfficientNet architectures. Comparative studies have shown that while all these models can perform effectively, some consistently outperform others. For example, research on drones-birds classification found that "the accuracy and F-Score of ResNet18 exceeds 98\% in all cases"\citep{Mohamed2023EfficientDC}, while another study on binary classification with the problem of small dataset reported that "DenseNet201 achieves the best classification accuracy of 98.89\%."\citep{Ren2023MonkeypoxDD}.
%ref ok

In a noteworthy study on medical image analysis, researchers evaluated the comparative performance of MobileNetV2 and Inception-v3 classification models. The investigation employed four distinct methodologies: implementing Inception-v3 both with and without transfer learning, and similarly applying MobileNetV2 with and without transfer learning techniques. The experimental results demonstrated that the MobileNetV2 architecture leveraging transfer learning capabilities achieved superior performance, reaching approximately 91.00\% accuracy in classification tasks \citep{Rahman2020}. An experiment conducted by \citep{9402686} conducted a comprehensive evaluation of different CNN architectures for identifying local bird species. With only 100 images per class before data augmentation high accuracies of above 90\% were achieved.

%  % This research approach aligns with the work presented by Al-antari et al. (\href{https://isic-challenge-2018.github.io/}{An automatic recognition of multiclass skin lesions via Deep Learning Convolutional Neural Networks}) at the ISIC2018: Skin Image Analysis Workshop and Challenge, which similarly explored advanced convolutional neural network applications for medical image classification.

% % Traditional approaches to fine-grained classification required extensive manual annotation of parts or regions of interest. As noted by \citep{zhang2022unsupervised} IRRELEVANT, earlier methods "localize object or parts in an image with object or part annotations, which are expensive and labor-consuming." To address this limitation, researchers have increasingly turned to deep learning approaches that can automatically extract relevant features without explicit part annotations.

Transfer learning addresses the primary challenges of deep learning: the need for large datasets and extensive computational resources. By leveraging pretrained models that have already learned general visual features from massive datasets, transfer learning enables the development of highly accurate classifiers with relatively domain-specific datasets. This is particularly valuable for this project, which focuses on distinguishing between two specific gull species with limited available data. \citep{iman2022review} emphasizes that recent transfer learning methods aim to reduce training process time and cost, and the necessity of extensive training datasets. \citep{transferln97} using CNN models pretrained from ImageNet achieved above 90\% accuracy in many CNN most that were tried for bird classification using transfer learning emphasize, "when the sample data is small, transfer learning can help the deep neural network classifier to improve classification accuracy." This makes transfer learning an ideal approach for specialized tasks like distinguishing between closely related gull species.

% \citep{tan2018survey}

% VERY USEFUL
% The experimental results showed that training the bird dataset with a pre-trained model requires no more than
% 10 epochs to obtain the best classification accuracy. Some models even require only 2 to 3 epochs, while
% training with an untrained model requires at least 40 to 50 epochs to achieve the highest classification
% accuracy. This is a big improvement on deep learning, which requires hardware support and a lot of training
% time.


% TODO \citep{kornblith2019better} 


% For extremely limited datasets, researchers have employed specialized transfer learning techniques. \citep{cui2018large} introduced a method called "transfer-learning by borrowing examples" that achieved state-of-the-art performance on small fine-grained datasets by selectively transferring knowledge from similar classes in larger datasets. 
% TODO NOT what i did above

% The transfer learning process typically follows a two-phase approach as described by \citep{sharif2014cnn}: first freezing most layers of the pre-trained model while training only the classification layers, then fine-tuning a larger portion of the network. 


% TODO \citep{guo2019spottune} refined this approach with their SpotTune method, which adaptively determines which layers to freeze or fine-tune on a per-instance basis, demonstrating improved performance for fine-grained classification tasks.



Working with limited datasets often introduces challenges related to class imbalance and overfitting. \citep{buda2018systematic} conducted a comprehensive analysis of class imbalance in convolutional neural networks and found that oversampling (duplicating samples from minority classes) generally outperforms undersampling for deep learning models. Standard data augmentation techniques such as resizing, random horizontal and vertical flipping, rotation, affine translation, and color jittering can improve model generalization and robustness \citep{yang2022augmentation,wu2024augmentation}.

% VERY USEFUL say too much oversampling caused overfitting that is y it was removed.

% For fine-grained bird classification specifically, \citep{chu2020fine} employed extensive data augmentation techniques including random cropping, rotation, flipping, and color jittering to improve model robustness. They demonstrated that such augmentations were particularly effective for classes with fewer samples, improving overall accuracy by up to 3.2\%.




% NOT USED
% More advanced techniques such as mixup \citep{zhang2018mixup}, which creates synthetic training examples by linearly interpolating between pairs of images and their labels, have shown effectiveness in fine-grained classification tasks. \citep{cui2019classbalancesd} integrated mixup with class-balanced loss to address imbalance in fine-grained datasets, achieving state-of-the-art performance on CUB-200-2011.


\section*{Interpretability Techniques for Deep Learning Models}
While deep learning models achieve impressive accuracy in classification tasks, their "black box" nature limits their usefulness in scientific contexts where understanding the basis for classifications is crucial. Interpretability techniques address this limitation by providing insights into model decision-making processes, making them essential tools for applications where transparency is as important as accuracy.

Gradient-weighted Class Activation Mapping (Grad-CAM) has emerged as a particularly valuable technique for visualizing regions of images that influence classification decisions. Selvaraju et al.\ \citep{Selvaraju_2019} introduced this technique as a generalization of CAM that "uses the gradients of any target concept flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image." Unlike earlier methods, Grad-CAM requires no architectural changes and can be applied to almost any CNN-based model. As described in recent literature, Grad-CAM "uses the gradients of each target that flows into the least convolutional layer to produce a bearish localization map, highlighting important regions in the image for concept prediction" \citep{Hasibuan2023Large}. This approach enables validating model decisions against expert knowledge and potentially discover new insights about morphological features with regards to this project.

Selvaraju et al.\ \citep{Selvaraju_2019} demonstrated that Guided Grad-CAM outperforms baseline approaches such as Guided Backpropagation and Deconvolution in generating class-discriminative visualizations. Their results show that Guided Grad-CAM not only produces high-resolution explanations but also clearly highlights the specific image regions that influence the model's classification decision---a property that is vital for our project, where understanding the distinguishing features between classes is essential. As illustrated in their paper, the visualizations allow us to see the exact areas that contributed to the class decision, thereby increasing transparency and trust in the classifier.

% VERY USEFUL: some gradcam biased like mentioned in paper above. can use for vit and other accuracy models

%NOT USEFUL
% Visualization studies comparing baseline models with enhanced architectures demonstrate that while basic models often focus on the most conspicuous parts of bird images (such as wings), more sophisticated approaches can discern more intricate features vital for species differentiation\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}. As noted in recent research, enhanced models excel "in identifying not only the prominent features but also the subtle, fine-grained characteristics essential for distinguishing between different bird types"\href{https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full}{4}.

% While deep learning models achieve impressive classification accuracy, their "black box" nature presents challenges for scientific applications where understanding decision mechanisms is crucial. As noted by \citep{montavon2018methods}, "black-box models that cannot be interpreted have limited applicability, especially in scientific contexts where understanding the basis for classifications is as important as the classifications themselves."


% For fine-grained classification, interpretability techniques can reveal whether models are focusing on biologically relevant features. \citep{zhang2018interpretable} demonstrated that CNN attention mechanisms often correspond to taxonomically important physical characteristics in birds. Their study showed that models trained only on image labels could automatically discover part-based attention patterns that aligned with expert knowledge.

% Beyond visualization, quantitative interpretability methods have been developed to measure feature importance. \citep{lundberg2017unified} proposed SHAP (SHapley Additive exPlanations), which assigns each feature an importance value for a particular prediction. In \citep{chen2019looks}, the authors applied SHAP to fine-grained bird classification models and found that the features deemed important by the model often matched field guide descriptions of distinguishing characteristics.

% These interpretability methods are particularly valuable in fine-grained classification tasks where the differences between categories are subtle and potentially unknown. By highlighting regions that drive model decisions, techniques like Grad-CAM can reveal discriminative features that can be useful to expert observers, potentially advancing biological understanding alongside classification accuracy.

% \href{https://www.atlantis-press.com/article/125986223.pdf}{5}.

\chapter{Aims and Objectives}

\section{Primary Aims}
\begin{enumerate}
    \item To develop high-performance deep learning models capable of distinguishing between Slaty-backed and Glaucous-winged Gulls based on their morphological characteristics.
    \item To implement robust interpretability techniques that reveal which features influence model decisions, allowing validation against ornithological expertise.
    \item To analyze whether consistent morphological differences exist between the two species. 
    \item Identify key discriminative features and perform analyses to get statistical information.
    \item To find out if the results of the analysis are statistically significant.
    \item Test whether the features can be used to identify the species of a gull with high confidence.
\end{enumerate}

\subsection*{Specific Objectives}
The project was carried out in four phases:
\begin{enumerate}
    \item Model Development and Evaluation
        \begin{itemize}
            \item Curate a high-quality dataset of adult in-flight gull images with clearly visible diagnostic features.
            \item Implement and compare multiple deep learning architectures (CNNs, Vision Transformers) for fine-grained classification.
            \item Evaluate models on unseen test sets.
        \end{itemize}
    \item Interpretability Implementation
        \begin{itemize}
            \item Implement suitable interpretability methods such Gradient-weighted Class Activation Mapping (Grad-CAM).
            \item Visualize regions of images that most influence classification decisions.
            \item Compare model focus areas with known taxonomic features described in ornithological literature/expert guidance.
        \end{itemize}
    \item Features Analyses
        \begin{itemize}
            \item Perform quantitative analysis of image regions highlighted by interpretability techniques.
            \item Compare intensity, texture, and pattern characteristics between species.
            \item Identify statistically significant morphological differences between correctly classified specimens.
            \item Validate the features that are significantly different to check if they can be used for classification
        \end{itemize}
\end{enumerate}


% Methodology
\newpage
\chapter{Methodology}

\section{Google Colab Platform}

Google Colab was selected as the primary platform for developing and training deep learning models for its cloud-based GPU acceleration, integration with Google Drive, and pre-installed libraries, facilitating efficient model development \citep{geron2019}.

% REMOVABLE
Despite its advantages, Google Colab presented a few challenges. The platform frequently disconnected during training sessions, interrupting the model training process before completing all epochs. As noted by \citep{carneiro2018}, while Colab provides robust GPU resources that can match dedicated servers for certain tasks, these free resources ``are far from enough to solve demanding real-world problems and are not scalable.''

The relatively small size of our dataset helped minimize resource demands. Second, checkpoint saving was implemented throughout the training process, allowing training to resume from the last saved state if disconnections were encountered. This approach ensured that progress wasn't lost when disconnections occurred, though it introduced some workflow inefficiencies.

\subsection{Python and PyTorch Framework}

The implementation utilized Python and PyTorch, chosen for their extensive machine learning ecosystem. The implementation utilized Python with PyTorch, capitalizing on its dynamic computation graphs for streamlined debugging and flexible model customization. The framework's efficient DataLoader pipelines enabled effective batch processing, while its native gradient support facilitated the integration of Grad-CAM for improved model interpretability.

% \subsection{Python and PyTorch Framework}

% The implementation was carried out using Python as the primary programming language, chosen for its extensive library support and widespread adoption in the machine learning community. Python's simple syntax and powerful libraries make it particularly suitable for rapid prototyping and experimentation in deep learning research \citep{geron2019}.

% For the deep learning framework, PyTorch was selected over alternatives like TensorFlow or Keras due to its dynamic computational graph which allows for more flexible model development and easier debugging. PyTorch's intuitive design facilitates a more natural expression of deep learning algorithms while still providing the performance benefits of GPU acceleration. The framework's robust ecosystem for computer vision tasks, including pre-trained models and transformation pipelines, was particularly valuable for this fine-grained classification task.

% \subsubsection{Advantages of PyTorch in Our Implementation}

% PyTorch offered several key advantages that were particularly beneficial for our transfer learning approach with pre-trained models:

% \begin{itemize}
%     \item \textbf{Dynamic Computational Graph:} PyTorch's define-by-run approach allowed for more intuitive debugging and model modification during development. This was especially valuable when adapting pre-trained architectures like VGG16 for our specific classification task.

%     \item \textbf{Flexible Model Customization:} The implementation benefited from PyTorch's object-oriented approach, which made it straightforward to modify pre-trained models, e.g., replacing classification layers while preserving feature extraction capabilities.

%     \item \textbf{Efficient Data Loading and Augmentation:} PyTorch's DataLoader and transformation pipelines facilitated efficient batch processing and on-the-fly data augmentation, which was crucial for maximizing the utility of our limited dataset.

%     \item \textbf{Gradient Visualization Tools:} PyTorch's native support for gradient computation and hooks made implementing Grad-CAM and other visualization techniques more straightforward, enabling better model interpretability.
% \end{itemize}

% Similar to approaches described by Raffel et al. \citet{raffel2023}, my implementation prioritized efficiency and optimization to work within the constraints of limited computational resources, allowing me to achieve high-quality results despite the limitations of the free cloud environment.

\section{Dataset Preparation and Refinement}

The dataset preparation followed a three-stage iterative refinement process, each addressing specific challenges identified during model development.
%  This approach aligns with established methodologies in fine-grained bird classification research, where dataset quality has been shown to significantly impact model performance \citet{ghani2024}.

\subsection{Stage 1: Initial Dataset Collection}

The initial dataset was collected from public repositories including eBird and iNaturalist, comprising 451 images of Glaucous-winged Gulls and 486 images of Slaty-backed Gulls. This dataset included gulls of various ages (juveniles and adults) in different postures (sitting, standing, and flying). Initial model testing on this dataset yielded poor performance (below 60\% accuracy), highlighting the need for dataset refinement. 
% Similar challenges with class imbalance have been documented by Kahl et al. in their work on BirdNET systems \citet{kahl2021}.


\subsection{Stage 2: Refined Dataset - Focus on Adult In-flight Images}

Consultation with Professor Gibbins, an ornithological expert, revealed that wing and wingtip patterns were the most reliable distinguishing features between these species. These patterns are most visible in flight. Juvenile images were removed due to their less defined wingtip features and differing plumage. Consequently, the dataset was refined to focus exclusively on adult in-flight images, resulting in a curated collection of 124 Glaucous-winged Gull images and 127 Slaty-backed Gull images. This targeted approach significantly improved model performance, with accuracy increasing to approximately 70\%.

By focusing specifically on adult in-flight images where wingtip patterns are most visible, this project addresses the core taxonomic question while minimizing confounding variables. The resulting interpretable classification system aims to provide both a practical identification tool and a scientific instrument for exploring morphological variation within and between these closely related species.

\subsection{Stage 3: High-Quality Dataset}

To further enhance classification performance, 640 high-resolution images of in-flight Slaty-backed Gulls were obtained from Professor Gibbins. More Glaucous-winged Gull images were collected and curated with expert guidance, reducing it to 135 high-quality images that clearly displayed critical wingtip features.  Images showing birds in moulting stages, juveniles, or unclear wingtip patterns were systematically removed.

For comparative analysis, an unrefined dataset containing 632 adult in-flight Glaucous-winged Gulls and 640 high-quality Slaty-backed Gull images was also tested which resulted in around 60\% model accuracy.

\subsubsection{Test Dataset}

All well-performing models were subsequently evaluated using a test dataset comprising unseen images during training, sourced from books, online, and from industry expert Professor Gibbins in order to obtain a precise assessment of model accuracy. In the final evaluation, the test set consisted of 14 images of Glaucous-winged Gulls and 33 images of Slaty-backed Gulls. Although this test set was imbalanced, the results were not adversely affected, as the models did not exhibit bias toward either class. This is further supported by the individual class metrics presented in the Results section, which demonstrate consistent performance across both classes.


% \begin{lstlisting}[language=Python]
% optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)
% \end{lstlisting}

% \begin{lstlisting}[language=Python]
% scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)
% \end{lstlisting}

% \begin{lstlisting}[language=Python]
% torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
% \end{lstlisting}

        % Class imbalance describes a dataset with a skewed ratio of majority to minority samples. Leevy et al. [27] describe many of the existing solutions to high-class imbalance across data types. Our survey will show how class-balancing oversampling in image data can be done with Data Augmentation.


% \begin{lstlisting}[language=Python]
% transform_val_test = transforms.Compose([
%     transforms.Resize((224, 224)),
%     transforms.ToTensor(),
%     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
% ])
% \end{lstlisting}

% \begin{lstlisting}[language=Python]
% transform_train = transforms.Compose([
%     transforms.Resize((224, 224)),
%     transforms.RandomHorizontalFlip(),
%     transforms.RandomRotation(15),
%     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
%     transforms.ToTensor(),
%     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
% ])
% \end{lstlisting}


% \begin{lstlisting}[language=Python]
% # Set random seeds for reproducibility
% torch.manual_seed(42)
% np.random.seed(42)
% random.seed(42)
% \end{lstlisting}

% \citep{simonyan2014vgg}

% \begin{lstlisting}[language=Python]
% class VGG16Modified(nn.Module):
%     def __init__(self):
%         super(VGG16Modified, self).__init__()
%         from torchvision.models import VGG16_Weights
%         self.vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)
%         # Replace the classifier with a custom binary classification layer
%         num_ftrs = self.vgg.classifier[6].in_features
%         self.vgg.classifier[6] = nn.Sequential(
%             nn.Dropout(0.4),
%             nn.Linear(num_ftrs, 2)
%         )

%     def forward(self, x):
%         return self.vgg(x)
% \end{lstlisting}


\section{Iterative Development Methodology and Debugging}

Initial implementations using ResNet50 with unrefined Stage 1 dataset yielded poor results (test accuracies below 60\%), indicating fundamental issues in either data quality or model implementation. To systematically address these challenges and improve performance for subsequent transfer learning approaches, a methodical debugging framework was employed following best practices outlined by \citep{karpathy2019recipe}.

\subsection{Pipeline Validation and Early Debugging}

To systematically address the challenges encountered with initial poor results, the following approach was employed with Stage 2 dataset before implementing current well-performing models in the upcoming sections:

\begin{itemize}
    \item \textbf{Data Inspection and Visualization:}
    \begin{itemize}
        \item Images with unclear image patterns were identified and removed. With an imbalanced and a small dataset that we had, it was important not to provide unclear images to the model to prevent it from learning incorrect features although the resulting dataset was small.
        \item Augmentation visualization confirmed that features critical for classification (particularly wingtip patterns) remained visible after transformation
    \end{itemize}
    
    \item \textbf{Pipeline Verification with Simple Models:}
    \begin{itemize}
        \item A simple, lightweight Custom CNN was implemented as an initial baseline before advancing to complex architectures
        \item This simplified model validated data loading procedures, augmentation effectiveness, and basic training operations
    \end{itemize}
    
    \item \textbf{Single-Batch Overfitting Test:}
    \begin{itemize}
        \item To verify gradient flow and learning capability, a single batch was deliberately overfitted with the simple CNN implemented
        \item Training loss reduction from 0.7072 (Epoch 1) to 0.0057 (Epoch 20) confirmed the pipeline's fundamental functionality
        \item This critical test established that confirmed that the training pipeline was functioning correctly, and with validation the model demonstrated reasonable generalization given the simplicity of the model.
    \end{itemize}
    
    \item \textbf{Controlled Experimentation:}
    \begin{itemize}
        \item Random seeds were fixed across all implementations (set to 42) to ensure reproducibility
        \item This approach eliminated training variability as a confounding factor when comparing architectural modifications
        \item Systematic adjustments to hyperparameters could be evaluated with confidence that performance differences were attributable to the specific changes rather than random initialization
    \end{itemize}
\end{itemize}
    
After establishing a robust development pipeline and refining the dataset, the transfer learning implementations described in the following sections achieved significantly improved results, with test accuracies exceeding 90\% for the best-performing models.

\section{Transfer Learning Approach}

Transfer learning was employed in the implementation to leverage the robust feature extraction capabilities of pre-trained models on ImageNet. This approach aligns with best practices in fine-grained classification tasks, where lower-level features learned from diverse datasets can be effectively repurposed for specialized domains with limited data. The pre-training on ImageNet's 1.2 million images across 1,000 classes provides the model with a strong foundation for recognizing a wide range of visual patterns \citep{krizhevsky2012imagenet}, which can then be fine-tuned for our specific classification task despite class imbalance challenges.
% \href{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}{Krizhevsky et al. (2012)}
% \href{https://arxiv.org/abs/1409.1556}{Simonyan and Zisserman (2015)} 

% USEFUL
The effectiveness of transfer learning was evident in the rapid convergence and high accuracy achieved even with our relatively limited dataset, demonstrating the potential of this approach for specialized classification tasks with significant class imbalance.

\subsection{Common Implementation Strategy} \label{sec:common}

All models except for the custom CNN utilized transfer learning to leverage knowledge from pre-trained networks. All the models mentioned in this section used the Stage 3 dataset. The transfer learning strategy included:

\begin{itemize}
    \item Using models pre-trained on ImageNet as feature extractors
    \item Fine-tuning the entire network with a reduced learning rate (typically 0.0001 to 0.001) across models.
    \item Replacing the final classification layer to output binary predictions (2 classes)
    \item Implementing a dropout layer (dropout=0.4) before final classification to prevent overfitting
\end{itemize}

This approach follows the established pattern that features learned in early layers of convolutional networks are more general and transferable, while later layers become more task-specific.

\subsection{Data Preparation and Augmentation}

Data augmentation was crucial to address the limited dataset size and class imbalance issues. Following best practices from \citep{perez2017effectivenessdataaugmentationimage}, multiple augmentation techniques were applied consistently across all models:

\begin{itemize}
    \item \textbf{Spatial transformations:} Random horizontal flips, rotations (typically 15 degrees), and random/center crops were applied to increase geometric diversity.
    \item \textbf{Color space transformations:} Color jitter with brightness, contrast, and saturation adjustments of 0.2 magnitude was applied to make models robust to illumination variations.
    \item \textbf{Image enhancement:} In some implementations, sharpening filters were applied to improve feature clarity.
    \item \textbf{Normalization:} All images were normalized to match pre-trained model expectations \citep{wu2018groupnormalization}.
\end{itemize}

The augmentation strategy was deliberately more aggressive for the training set compared to validation and test sets, where only resizing, optional cropping, and normalization were applied to maintain evaluation consistency.

These techniques enhance model robustness to natural variations in image appearance, reducing overfitting and improving generalization capability \citep{perez2017effectivenessdataaugmentationimage}.

\subsection{Image Preprocessing}

All images were preprocessed through a standardized pipeline:

Images were resized to match the architecture's expected input dimensions (224×224 pixels for most models, 299×299 pixels for Inception v3). Pixel values were normalized using ImageNet mean values [0.485, 0.456, 0.406] and standard deviations [0.229, 0.224, 0.225], ensuring input distributions aligned with those seen during pre-training.

% \href{https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf}{here}

\subsection{Training Optimization Strategy}

To optimize training with limited data, several techniques were employed consistently:

\begin{itemize}
    \item \textbf{Optimizer:} AdamW optimizer with learning rates between 0.0001-0.001 and weight decay of 0.0005-0.001 was used across implementations to provide adaptive learning with regularization.
    
    \item \textbf{Learning rate scheduling:} Adaptive learning rate scheduling using ReduceLROnPlateau was implemented across models, reducing learning rates by a factor of 0.1 (factor=0.1) when validation metrics plateaued for 3-5 consecutive epochs (patience=3-5). ReduceLROnPlateau with mode='max' was used to track validation accuracy improvements rather than minimizing loss.
    
    \item \textbf{Batch size:} The batch size was set to 16 for most of the models, striking a balance between computational efficiency and optimization stability. For a few trials batch size of 32 was also tested. Smaller batch sizes can increase gradient noise, which has been shown to act as an implicit regularizer that can improve generalization, particularly beneficial when working with limited training data \citep{keskar2016largebatch, masters2018revisiting}.
    
    \item \textbf{Early stopping:} The models were evaluated both with and without the application of early stopping. In experiments where early stopping was implemented, training was terminated if the validation accuracy did not improve for a predetermined number of epochs (patience = 3–5), or if the validation loss began to increase, thereby mitigating the risk of overfitting \citep{Prechelt1998}.

    \item \textbf{Gradient clipping:} Applied in some implementations to prevent gradient explosions and stabilize training. Due to the small and imbalanced dataset, gradient clipping was implemented to prevent limited images from causing large weight updates. 
    
%\citep{zhang2020gradientclippingacceleratestraining}

    \item \textbf{Loss function:} Cross-entropy loss was used consistently as the optimization objective for the binary classification task.
    
    \item \textbf{Data splitting:} Train/validation split of 80\%/20\% was consistently used to provide reliable validation metrics while maximizing training data.
    
    \item \textbf{Random seeds:} Fixed random seeds (42) were set for PyTorch, NumPy, and Python's random module to ensure reproducibility. Controlling randomness is essential for reliable hyper-parameter tuning, performance assessment, and research reproducibility.
    

\end{itemize}

The combination of these techniques enabled effective learning despite the challenges of limited data and class imbalance, with our best model achieving significantly better performance than traditional machine learning approaches on the same dataset.

\subsection{Regularization Techniques}

Multiple regularization strategies were employed to handle the limited data size and class imbalance:

\begin{itemize}
    \item \textbf{Dropout:} Layers with rates between 0.3-0.4 were consistently added before final classification layers to reduce overfitting due to our small dataset size \citep{srivastava2014dropout}.
    \item \textbf{Weight decay:} L2 regularization with weight decay values between 1e-4 and 1e-3 was applied across all models to prevent overfitting \citep{krogh1992simple}.
    

\end{itemize}

\subsection{Addressing Class Imbalance}

Our dataset exhibited significant class imbalance, which can degrade model performance by biasing predictions toward the majority class \citep{krawczyk2016learning}. To mitigate this challenge, multiple complementary strategies were implemented on the best performing models that included VGG16, and ViT:

% TODO check ref below

\begin{itemize}
    \item \textbf{Class-Weighted Loss Function}
    \begin{itemize}
        \item Implemented inverse frequency weighting \citep{cui2019classbalanced}
        \item Class weights calculation: \( \text{class\_weights}[i] = \frac{\text{total\_samples}}{\text{num\_classes} \times \text{label\_counts}[i]} \)
        \subitem PyTorch implementation: \texttt{CrossEntropyLoss} with class weights tensor
    \end{itemize}
    
    \item \textbf{Weighted Random Sampling}
    \begin{itemize}
        \item Balanced mini-batches using PyTorch's \texttt{WeightedRandomSampler}
        \item Sample weights: \( \text{samples\_weights} = \text{class\_weights}[\text{label}] \)
        \item Oversamples minority class and undersamples majority class \citep{buda2018systematic}
        \item Uses replacement sampling for effective batch balancing
    \end{itemize}
    
    \item \textbf{Class-Specific Data Augmentation}
    \begin{itemize}
        \item Aggressive minority class augmentation \citep{shorten2019survey}
        \item Minority class transformations include:
        \begin{itemize}
            \item 30° random rotations
            \item Strong color jitter (brightness/contrast/saturation=0.3)
            \item Random resized crops (scale=0.7-1.0)
            \item Horizontal flips
        \end{itemize}
        \subitem Standard augmentation for majority class (15° rotations, milder parameters)
    \end{itemize}
\end{itemize}

\subsection{Dataset Management}

To address the challenges of limited data availability, an 80:20 train-validation split was implemented using random split stratification to maintain class distribution across partitions. This approach ensured that the validation set remained representative of the overall dataset while maximizing the samples available for training. 
%TODO

% To address the challenges of limited data availability, an 80:20 train-validation split was implemented using random split stratification to maintain class distribution across partitions. This approach ensured that the validation set remained representative of the overall dataset while maximizing the samples available for training \citep{kohavi1995study}.

\subsection{Evaluation Strategy}

Model performance was systematically evaluated using:

\begin{itemize}
    \item \textbf{Validation accuracy:} Used during training to select optimal model checkpoints and trigger early stopping or learning rate adjustments.
    \item \textbf{Test accuracy:} The final evaluation metric was the accuracy on the unseen test set, which served to assess the generalization performance of the models. In addition to evaluating test accuracy, Grad-CAM visualizations were generated using the test set to further aid in the selection of the optimal model and make sure that the model learned relevant features.
    \item \textbf{Visualization:} Training loss and validation accuracy curves were plotted to analyze model convergence and potential overfitting.
    \item \textbf{Checkpointing:} Best-performing models based on validation accuracy were saved for later evaluation on test dataset.
\end{itemize}

\subsection{Model Checkpointing and Evaluation}

Our implementation includes a robust evaluation framework with model checkpointing based on validation accuracy. This ensures that we preserve the best-performing model configuration throughout the training process. The model is trained for 20 epochs with early stopping implicitly implemented through best model saving. Performance is evaluated using accuracy on both validation and test sets, providing a comprehensive assessment of model generalization.

This approach is well-aligned with established findings by \citep{tan2018survey}, who demonstrate that transfer learning with pre-trained models on bird classification tasks enables rapid convergence-often requiring as few as 2 to 10 epochs to achieve optimal accuracy. in stark contrast to the 40–50 epochs typically needed for training models from scratch. This efficiency not only accelerates the experimental workflow but also reduces computational demands, making it feasible to conduct extensive model comparisons and hyperparameter tuning. By leveraging pre-trained models and rigorous checkpointing, our methodology ensures that the selected models are both accurate and robust, reflecting best practices in modern deep learning for fine-grained image classification tasks such as bird species recognition.
%TODO

\section{Model Architectures and Specific Implementations}

\subsection{VGG-16 Architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/architecture/vgg.png}
    \caption{Architecture of the VGG model}
    \label{fig:vgg_architecture}
\end{figure}


% \tikzset{every picture/.style={line width=0.75pt}}
% \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]

% % Conv1-1 
% \draw  [fill=white  ,fill opacity=1 ] (44,95.69) -- (124.19,15.5) -- (136,15.5) -- (136,200.31) -- (55.81,280.5) -- (44,280.5) -- cycle ; \draw   (136,15.5) -- (55.81,95.69) -- (44,95.69) ; \draw   (55.81,95.69) -- (55.81,280.5) ;
% % Conv 1-2
% \draw  [fill=white  ,fill opacity=1 ] (64,96.19) -- (144.19,16) -- (156,16) -- (156,200.81) -- (75.81,281) -- (64,281) -- cycle ; \draw   (156,16) -- (75.81,96.19) -- (64,96.19) ; \draw   (75.81,96.19) -- (75.81,281) ;
% % Pooling 1
% \draw  [color=red  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (100,105.5) -- (154,51.5) -- (164,51.5) -- (164,174.5) -- (110,228.5) -- (100,228.5) -- cycle ; \draw  [color=red  ,draw opacity=1 ] (164,51.5) -- (110,105.5) -- (100,105.5) ; \draw  [color=red  ,draw opacity=1 ] (110,105.5) -- (110,228.5) ;
% % Conv 2-1
% \draw  [fill=white  ,fill opacity=1 ] (115,106.5) -- (169,52.5) -- (179,52.5) -- (179,175.5) -- (125,229.5) -- (115,229.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (179,52.5) -- (125,106.5) -- (115,106.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (125,106.5) -- (125,229.5) ;
% % Conv 2-2
% \draw  [fill=white  ,fill opacity=1 ] (131,107.5) -- (185,53.5) -- (195,53.5) -- (195,176.5) -- (141,230.5) -- (131,230.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (195,53.5) -- (141,107.5) -- (131,107.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (141,107.5) -- (141,230.5) ;
% % Pooling 2
% \draw  [color=red  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (161,113.5) -- (195,79.5) -- (205,79.5) -- (205,152.5) -- (171,186.5) -- (161,186.5) -- cycle ; \draw  [color=red  ,draw opacity=1 ] (205,79.5) -- (171,113.5) -- (161,113.5) ; \draw  [color=red  ,draw opacity=1 ] (171,113.5) -- (171,186.5) ;
% % Conv 3-1
% \draw  [fill=white  ,fill opacity=1 ] (179,113.5) -- (213,79.5) -- (223,79.5) -- (223,152.5) -- (189,186.5) -- (179,186.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (223,79.5) -- (189,113.5) -- (179,113.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (189,113.5) -- (189,186.5) ;
% % Conv 3-2
% \draw  [fill=white  ,fill opacity=1 ] (198,114.5) -- (232,80.5) -- (242,80.5) -- (242,153.5) -- (208,187.5) -- (198,187.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (242,80.5) -- (208,114.5) -- (198,114.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (208,114.5) -- (208,187.5) ;
% % Conv 3-3
% \draw  [fill=white  ,fill opacity=1 ] (217,115.5) -- (251,81.5) -- (261,81.5) -- (261,154.5) -- (227,188.5) -- (217,188.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (261,81.5) -- (227,115.5) -- (217,115.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (227,115.5) -- (227,188.5) ;
% % Pooling 3
% \draw  [color=red  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (248,127.5) -- (264,111.5) -- (280,111.5) -- (280,151.5) -- (264,167.5) -- (248,167.5) -- cycle ; \draw  [color=red  ,draw opacity=1 ] (280,111.5) -- (264,127.5) -- (248,127.5) ; \draw  [color=red  ,draw opacity=1 ] (264,127.5) -- (264,167.5) ;
% % Conv 4-1
% \draw  [fill=white  ,fill opacity=1 ] (270,128.5) -- (286,112.5) -- (302,112.5) -- (302,152.5) -- (286,168.5) -- (270,168.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (302,112.5) -- (286,128.5) -- (270,128.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (286,128.5) -- (286,168.5) ;
% % Conv 4-2
% \draw  [fill=white  ,fill opacity=1 ] (292,128.5) -- (308,112.5) -- (324,112.5) -- (324,152.5) -- (308,168.5) -- (292,168.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (324,112.5) -- (308,128.5) -- (292,128.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (308,128.5) -- (308,168.5) ;
% % Conv 4-3
% \draw  [fill=white  ,fill opacity=1 ] (314,128.5) -- (330,112.5) -- (346,112.5) -- (346,152.5) -- (330,168.5) -- (314,168.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (346,112.5) -- (330,128.5) -- (314,128.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (330,128.5) -- (330,168.5) ;
% % pooling 4
% \draw  [color=red  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (343,134.5) -- (352,125.5) -- (365,125.5) -- (365,143.5) -- (356,152.5) -- (343,152.5) -- cycle ; \draw  [color={rgb, 255:red, 251; green, 0; blue, 0 }  ,draw opacity=1 ] (365,125.5) -- (356,134.5) -- (343,134.5) ; \draw  [color={rgb, 255:red, 251; green, 0; blue, 0 }  ,draw opacity=1 ] (356,134.5) -- (356,152.5) ;
% % Conv 5-1
% \draw  [fill=white  ,fill opacity=1 ] (359,134.5) -- (368,125.5) -- (381,125.5) -- (381,143.5) -- (372,152.5) -- (359,152.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (381,125.5) -- (372,134.5) -- (359,134.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (372,134.5) -- (372,152.5) ;
% % Conv 5-2
% \draw  [fill=white  ,fill opacity=1 ] (375,134.5) -- (384,125.5) -- (397,125.5) -- (397,143.5) -- (388,152.5) -- (375,152.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (397,125.5) -- (388,134.5) -- (375,134.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (388,134.5) -- (388,152.5) ;
% % COnv 5-3
% \draw  [fill=white  ,fill opacity=1 ] (391,134.5) -- (400,125.5) -- (413,125.5) -- (413,143.5) -- (404,152.5) -- (391,152.5) -- cycle ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (413,125.5) -- (404,134.5) -- (391,134.5) ; \draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] (404,134.5) -- (404,152.5) ;
% % Pooling 5
% \draw  [color=red  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (412,133.5) -- (416,129.5) -- (434,129.5) -- (434,139.5) -- (430,143.5) -- (412,143.5) -- cycle ; \draw  [color=red  ,draw opacity=1 ] (434,129.5) -- (430,133.5) -- (412,133.5) ; \draw  [color=red  ,draw opacity=1 ] (430,133.5) -- (430,143.5) ;
% % dense 1
% \draw  [color=blue  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (438,134.07) -- (440.57,131.5) -- (496,131.5) -- (496,137.93) -- (493.43,140.5) -- (438,140.5) -- cycle ; \draw  [color=blue  ,draw opacity=1 ] (496,131.5) -- (493.43,134.07) -- (438,134.07) ; \draw  [color=blue  ,draw opacity=1 ] (493.43,134.07) -- (493.43,140.5) ;
% % dense 2
% \draw  [color=blue  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (497.43,134.07) -- (500,131.5) -- (555.43,131.5) -- (555.43,137.93) -- (552.86,140.5) -- (497.43,140.5) -- cycle ; \draw  [color=blue  ,draw opacity=1 ] (555.43,131.5) -- (552.86,134.07) -- (497.43,134.07) ; \draw  [color=blue  ,draw opacity=1 ] (552.86,134.07) -- (552.86,140.5) ;
% % dense 3
% \draw  [color=blue  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (555.86,134.07) -- (558.43,131.5) -- (582,131.5) -- (582,137.93) -- (579.43,140.5) -- (555.86,140.5) -- cycle ; \draw  [color=blue  ,draw opacity=1 ] (582,131.5) -- (579.43,134.07) -- (555.86,134.07) ; \draw  [color=blue  ,draw opacity=1 ] (579.43,134.07) -- (579.43,140.5) ;
% % softmax
% \draw  [color=orange  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (582.43,134.07) -- (585,131.5) -- (608.57,131.5) -- (608.57,137.93) -- (606,140.5) -- (582.43,140.5) -- cycle ; \draw  [color=orange  ,draw opacity=1 ] (608.57,131.5) -- (606,134.07) -- (582.43,134.07) ; \draw  [color=orange  ,draw opacity=1 ] (606,134.07) -- (606,140.5) ;
% % input
% \draw   (6,140) -- (24,140) -- (24,130) -- (36,150) -- (24,170) -- (24,160) -- (6,160) -- cycle ;
% % arrow
% \draw    (426,88.25) -- (425.54,124.5) ;
% \draw [shift={(425.5,127.5)}, rotate = 270.73] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
% % conv
% \draw  [fill=white  ,fill opacity=1 ] (296.19,223.25) -- (301.69,217.75) -- (314.5,217.75) -- (314.5,226.81) -- (309,232.31) -- (296.19,232.31) -- cycle ; \draw   (314.5,217.75) -- (309,223.25) -- (296.19,223.25) ; \draw   (309,223.25) -- (309,232.31) ;
% % max pool
% \draw  [color=red  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (296.19,242.31) -- (301.69,236.81) -- (314.5,236.81) -- (314.5,245.87) -- (309,251.37) -- (296.19,251.37) -- cycle ; \draw  [color=red  ,draw opacity=1 ] (314.5,236.81) -- (309,242.31) -- (296.19,242.31) ; \draw  [color=red  ,draw opacity=1 ] (309,242.31) -- (309,251.37) ;
% % fully connected
% \draw  [color=blue  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (296.19,261.25) -- (301.69,255.75) -- (314.5,255.75) -- (314.5,264.81) -- (309,270.31) -- (296.19,270.31) -- cycle ; \draw  [color=blue  ,draw opacity=1 ] (314.5,255.75) -- (309,261.25) -- (296.19,261.25) ; \draw  [color=blue  ,draw opacity=1 ] (309,261.25) -- (309,270.31) ;
% % softmax
% \draw  [color=orange  ,draw opacity=1 ][fill=white  ,fill opacity=1 ] (296.19,280.31) -- (301.69,274.81) -- (314.5,274.81) -- (314.5,283.87) -- (309,289.37) -- (296.19,289.37) -- cycle ; \draw  [color=orange  ,draw opacity=1 ] (314.5,274.81) -- (309,280.31) -- (296.19,280.31) ; \draw  [color=orange  ,draw opacity=1 ] (309,280.31) -- (309,289.37) ;

 
% \draw (1,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input};
 
% \draw (100.5,-2.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$224\times 224\times 64$};
 
% \draw (132.5,34.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$112\times 112\times 128$};
 
% \draw (261.5,94.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$28\times 28\times 512$};
 
% \draw (344.5,108.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$14\times 14\times 512$};
 
% \draw (193,62.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$56\times 56\times 256$};

% \draw (399,71) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$7\times 7\times 512$};

% \draw (465.5,114) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1\times 1\times 4096$};

% \draw (552.5,114) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$1\times 1\times 2$};


% \draw (319,214.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {Convolution + ReLU};

% \draw (319.5,235.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {Max pooling};

% \draw (319.5,253.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {Fully connected + ReLU};

% \draw (320,271.5) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {Softmax};

% % Dropout label with arrow
% \draw [->] (570,110) -- (570,118);
% \draw (570,105) node [anchor=south] [font=\scriptsize] [align=center] {Dropout\\0.4};

% \end{tikzpicture}


% \begin{algorithm}
%     \caption{VGG16Modified Architecture}
%     \begin{algorithmic}[1]
%     \Function{VGG16Modified}{}
%         \State Load pre-trained VGG-16 with ImageNet weights
%         \State Extract number of features from final layer: $num\_ftrs \gets$ VGG.classifier[6].in\_features
%         \State Replace final classifier layer with:
%             \State Dropout(p=0.4)
%             \State Linear($num\_ftrs \to 2$) \Comment{Binary classification}
%     \EndFunction
    
%     \Function{Forward}{$x$}
%         \State \Return VGG($x$)
%     \EndFunction
%     \end{algorithmic}
%     \end{algorithm}

\subsubsection{Theoretical Foundation}

VGG-16, developed by Simonyan and Zisserman (2014) \citep{simonyan2014vgg} at the Visual Geometry Group (VGG) at Oxford, is a 16 layer CNN with 13 convolutional layers followed by 3 fully connected layers known for its deep stack of 3×3 convolutional filters followed by max pooling layers. With approximately 138 million parameters, VGG-16 provides a strong foundation for feature extraction in computer vision tasks although it is computationally exprensive compared to other architectures. Pre-trained on the ImageNet dataset containing over 1.2 million images across 1,000 classes, VGG-16 offers robust initialization weights that facilitate effective knowledge transfer to domain-specific tasks with limited training data.

VGG-16 has demonstrated superior performance in fine-grained classification tasks compared to conventional techniques. Recent studies show that VGG-16 with logistic regression achieved 97.14\% accuracy on specialized datasets like Leaf12, significantly outperforming traditional approaches that combined color channel statistics, texture features, and classic classifiers which only reached 82.38\% accuracy \citep{pearline2019study}. For our specific task of gull species classification, the hierarchical feature representation capabilities of VGG-16 proved particularly effective at capturing the subtle differences in wing patterns and morphological features that distinguish between the target species.

\citep{zhang2019bird} demonstrated that VGG-16 achieves 94.3\% accuracy on CUB-200-2011 by fine-tuning only the final three layers, a strategy mirrored in my VGG implementation where the classifier head was replaced while preserving ImageNet-initialized convolutional weights.
This approach aligns with successful methodologies in avian species classification using VGG-16 as demonstrated by Brown et al. (2018), where fine-tuning the architecture by modifying the final classification layer enabled the model to retain general feature recognition capabilities while adapting to species-specific visual characteristics \citep{10533638}.

\subsubsection{Model Adaptation for Fine-Grained Classification}

The VGG architecture followed the common methodology approach mentioned in Section~\ref{sec:common} with a dropout and FC layer  mapping from the original 4096 features to 2 output classes replacing the original 1000-class classifier.

% USEFUL GOOD FOR ALL THE VALUES LIKE OPTIMISER ETC https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10533638&tag=&tag=1

\subsection{Vision Transformer (ViT) Architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/architecture/vit.png}
    \caption{Architecture of the Vision Transformer (ViT) model}
    \label{fig:vit_architecture}
\end{figure}

\subsection{ViT for Fine-Grained Classification}

Vision Transformers (ViT) have emerged as powerful alternatives to convolutional neural networks for visual recognition tasks. First introduced by Dosovitskiy et al. \citep{dosovitskiy2021imageworth16x16words}, ViTs process images as sequences of fixed-size patches, applying transformer-based self-attention mechanisms to model global relationships between image regions. This architecture enables the capture of long-range dependencies within images, making it particularly suitable for fine-grained classification tasks where subtle distinctions between similar classes may depend on relationships between distant image features.

\subsubsection{Vision Transformer Implementation}

For our primary approach, a Vision Transformer was implemented using transfer learning from a pre-trained model with base architecture 'vit\_base\_patch16\_224' pre-trained on ImageNet from the TIMM library \citep{rwightman2021pytorch}. The ViT architecture followed the common methodology approach mentioned in Section~\ref{sec:common}.

The model architecture preserves the core self-attention mechanism of ViT while adapting the final classification layer for our specific binary classification task. This approach follows established transfer learning principles for vision transformers \citep{touvron2021trainingdataefficientimagetransformers}, leveraging representations learned from large-scale datasets to overcome our limited training data constraints.

\subsubsection{Alternative ViT Implementations}

In addition to our primary implementation, we explored two attention-enhanced architectures:

\textbf{InterpretableViT}
We developed an InterpretableViT model that incorporates explicit attention mechanisms for improved focus on discriminative features:

\begin{itemize}
    \item Separates the class token from patch tokens
    \item Applies a learned attention layer to generate importance weights for each patch
    \item Combines the class token with attention-weighted patch representations
    \item Employs a multi-layer classifier with dropout regularization
\end{itemize}

A key advantage of this architecture is its compatibility with gradient-based visualization techniques. By separating the class token from patch tokens and implementing an explicit attention mechanism, the model facilitates more effective application of Grad-CAM, allowing for visualization of discriminative image regions contributing to classification decisions.

% check above referencea and cite as Selvaraju_2019

\paragraph{EnhancedViT}
We also implemented an EnhancedViT that applies attention-based weighting across all tokens:

\begin{itemize}
    \item Processes all tokens (including class token) through an attention mechanism
    \item Generates a single attention-weighted feature representation
    \item Utilizes a specialized classification head with dropout for regularization
\end{itemize}

% This implementation draws from research on token aggregation strategies in vision transformers (\href{https://arxiv.org/abs/2012.09688}{Wang et al., 2021}), which shows that attention-weighted token aggregation can improve performance in data-limited regimes.

% https://arxiv.org/pdf/2006.03677 TODO CHECK REF ABOVE

\subsection{Inception v3 Architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/architecture/inception.png}
    \caption{Architecture of the Inception v3 model}
    \label{fig:inception_architecture}
\end{figure}

\subsection*{Theoretical Background}

Inception v3 \citep{szegedy2015rethinkinginceptionarchitecturecomputer} employs a modular design centered around \textit{Inception modules} that process input tensors through multiple parallel convolution pathways with varying receptive fields. This design efficiently captures multi-scale features simultaneously, making it particularly effective for fine-grained classification tasks requiring detection of subtle morphological differences.

The key architectural innovations in Inception v3 that informed our implementation include:
\begin{itemize}
    \item Factorized convolutions that reduce computational complexity while maintaining representational power
    \item Auxiliary classifiers that inject additional gradient signals to mitigate vanishing gradients
    \item Batch normalization for training stability and faster convergence

\end{itemize}

\subsubsection{Model-Specific Implementation Details}

Our implementation adapted the pre-trained Inception v3 model for binary classification with auxiliary outputs. During training, we utilized Inception v3's auxiliary classifier to provide an additional gradient path during backpropagation, which is a distinctive feature of this architecture not found in our other implementations. Mixed precision training with torch.amp.GradScaler() was used to improve computational efficiency without sacrificing accuracy.

\subsection{Residual Network (ResNet-50) Architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/architecture/resnet.png}
    \caption{Architecture of the ResNet-50 model}
    \label{fig:resnet_architecture}
\end{figure}

\subsection*{Theoretical Background}

ResNet-50 \citep{he2016deep} addresses the degradation problem in deep neural networks through the introduction of residual connections (skip connections). These connections enable the network to learn residual mappings rather than direct mappings, allowing for substantially deeper architectures without suffering from vanishing gradients. The residual block can be formulated as:

\begin{equation}
y = \mathcal{F}(x, \{W_i\}) + x
\end{equation}

where $\mathcal{F}$ represents the residual mapping to be learned and $x$ is the identity connection. This formulation makes optimization easier as the network only needs to learn the residual with respect to the identity mapping.

The ResNet architecture followed the common methodology approach mentioned in Section~\ref{sec:common}.

\subsection{Custom CNN with Squeeze-and-Excitation Blocks}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/architecture/improvedcnn.png}
    \caption{Architecture of the Custom CNN with Squeeze-and-Excitation Blocks}
    \label{fig:improvedcnn_architecture}
\end{figure}

\subsubsection{Architectural Design}

To address the challenges of limited data and class imbalance in fine-grained classification, we developed a lightweight custom CNN architecture incorporating attention mechanisms. Our approach employs Squeeze-and-Excitation (SE) blocks, which enhance feature representation by modeling channel interdependencies through an attention mechanism. The SE block, as introduced by Hu et al. \citep{Hu_2018_CVPR}, adaptively recalibrates channel-wise feature responses to emphasize informative features while suppressing less useful ones.

The architecture consists of three convolutional blocks, each followed by batch normalization, ReLU activation, and an SE block. The SE block performs two key operations:
\begin{itemize}
    \item \textbf{Squeeze}: Global average pooling across spatial dimensions to generate channel-wise statistics
    \item \textbf{Excitation}: A fully connected layer that produces modulation weights for each channel
\end{itemize}

This channel-wise attention mechanism has been shown to improve model performance with minimal computational overhead \citep{Hu_2018_CVPR}. The SE blocks in our implementation use a reduction ratio of 16, balancing parameter efficiency and representational power.

\subsubsection{Custom CNN-Specific Training Approach}

Unlike the transfer learning approaches used with pre-trained models, our custom CNN was trained from scratch with some specific optimization strategies:

\begin{itemize}
    \item \textbf{Cosine Annealing scheduler}: Our learning rate schedule follows a cosine annealing pattern with a period of 20 epochs, allowing the learning rate to oscillate and potentially escape local minima.
    
% IRRELEVANT (\href{https://arxiv.org/abs/1608.03983}{Loshchilov and Hutter, 2017})
    \item \textbf{Batch normalization:} Used in custom CNN implementations to stabilize learning and improve convergence \citep{ioffe2015batch}.
    
    \item \textbf{Specialized augmentation}: The custom CNN particularly benefited from more aggressive data augmentation strategies to compensate for the lack of pre-trained weights, including stronger rotations and more extensive color jittering than used with the transfer learning models.
\end{itemize}

\section{Model Interpretability Methodologies}

Deep learning models, particularly Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), are often criticized for their lack of transparency, functioning as "black boxes" wherein the decision-making process remains opaque to human observers. For critical applications like wildlife species classification, understanding how these models arrive at their predictions is essential for establishing trust and validating results \citep{Selvaraju_2019}. This section outlines the methodologies implemented to visualize and interpret the classification decisions of both the CNN architecture and Vision Transformer (ViT) models used in this study.

\subsection{Gradient-weighted Class Activation Mapping (Grad-CAM)}

Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely adopted visualization technique that produces visual explanations for decisions made by CNN-based models without requiring architectural changes or retraining \citep{Selvaraju_2019}. It generates coarse localization maps highlighting the regions in the input image that significantly influenced the model's prediction for a specific class.

\subsubsection{Theoretical Foundation}

Grad-CAM extends the Class Activation Mapping (CAM) approach \citep{zhou2015learningdeepfeaturesdiscriminative} by utilizing the gradient information flowing into the final convolutional layer of a CNN. Unlike CAM, which requires modifications to the network architecture and retraining, Grad-CAM can be applied to any CNN-based architecture without architectural changes, making it more versatile.

The fundamental principle behind Grad-CAM is that the final convolutional layer in a CNN retains spatial information while encoding high-level semantics. By analyzing how the gradients of a specific class score flow into this layer, Grad-CAM can identify the regions in the input image that are most influential for the prediction.

% \begin{algorithm}
% \caption{Gradient-weighted Class Activation Mapping (Grad-CAM)}
% \begin{algorithmic}[1]
% \Input Input image $I$, CNN model $f$, target class $c$, final convolutional layer feature maps $A^k$
% \Output Heatmap $L_{Grad-CAM}^c$
% \State Perform forward pass on model $f$ with input image $I$ to obtain prediction score $y^c$
% \State Compute gradients of score $y^c$ with respect to feature maps $A^k$: $\frac{\partial y^c}{\partial A^k}$
% \State Apply global average pooling to gradients to obtain importance weights $\alpha_k^c$: 
%        $\alpha_k^c = \frac{1}{Z}\sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}$
% \State Calculate weighted combination of feature maps: 
%        $L_{Grad-CAM}^c = ReLU\left(\sum_k \alpha_k^c A^k\right)$
% \State Normalize $L_{Grad-CAM}^c$ to range [0, 1]
% \State Resize $L_{Grad-CAM}^c$ to input image dimensions
% \Return $L_{Grad-CAM}^c$
% \end{algorithmic}
% \end{algorithm}

The ReLU function is applied to the weighted combination of feature maps to focus only on features that have a positive influence on the class of interest, effectively eliminating features that suppress the class.

\subsubsection{Methodology for CNN models}

In this study, Grad-CAM was implemented for the VGG16 model by targeting the final convolutional layer (features[-1]). The implementation involves several key steps:

\begin{enumerate}
    \item \textbf{Hook Registration}: Forward and backward hooks are registered on the target layer to capture activations during the forward pass and gradients during the backward pass.
    
    \item \textbf{Forward Pass}: The input image is passed through the network to obtain the model's prediction.
    
    \item \textbf{Backpropagation}: The gradient of the score for the target class (either the predicted class or a specified class) with respect to the feature maps of the target layer is computed through backpropagation.
    
    \item \textbf{Global Average Pooling}: These gradients undergo global average pooling to obtain weights indicating the importance of each channel for the target class.
    
    \item \textbf{Weighted Combination}: The weights are applied to the activations of the target layer to create a weighted combination of feature maps.
    
    \item \textbf{ReLU Application}: A ReLU function is applied to the weighted combination to focus only on features that have a positive influence on the class of interest.
    
    \item \textbf{Normalization}: The resulting heatmap is normalized to the range [0, 1] for consistent visualization.
    
    \item \textbf{Visualization}: The heatmap is resized to match the input image dimensions and overlaid on the original image using a colormap (typically 'jet') to highlight regions the model focused on for its prediction.
\end{enumerate}



Similar implementation was implemented for other models: Inceptionv3, ResNet50, and CustomCNN. 

\subsection{Attention Rollout for Vision Transformers}

Vision Transformers process images differently from CNNs, using self-attention mechanisms rather than convolution operations to model relationships between image patches. Therefore, a different approach called Attention Rollout is used to visualize ViT decision-making \citep{abnar2020attention}.

\subsubsection{Theoretical Foundation}

The Attention Rollout method visualizes information flow through the layers of a Transformer model. In Vision Transformers, the input image is divided into fixed-size patches, and each patch is linearly embedded along with position embeddings. A special classification token ([CLS]) is added, and the sequence of embedded patches is processed through multiple layers of self-attention.

Attention Rollout computes a measure of how the [CLS] token attends to each image patch by propagating attention through all layers of the network. This provides insight into which parts of the image the model considers most relevant for classification.

\subsubsection{Methodology for ViT}

The implementation of Attention Rollout for the ViT model follows these steps:

\begin{enumerate}
    \item \textbf{Attention Map Collection}: Forward hooks are registered on each transformer block to collect attention maps during the forward pass of an input image.
    
    \item \textbf{QKV Processing}: For each attention head, the query (Q), key (K), and value (V) matrices are extracted and processed to compute the raw attention weights between different tokens.
    
    \item \textbf{Head Averaging}: Attention weights from all heads in each layer are averaged to get a single attention map per transformer block.
    
    \item \textbf{Attention Rollout Computation}: Starting with an identity matrix, attention maps from each layer are sequentially multiplied to account for how attention propagates through the entire network.
    
    \item \textbf{CLS Token Attention Extraction}: The attention weights from the classification ([CLS]) token to each image patch are extracted, which indicates the importance of each patch for the final classification.
    
    \item \textbf{Reshaping and Visualization}: These weights are reshaped to match the spatial dimensions of the input image (typically 14×14 for ViT-Base with patch size 16) and then upsampled to create a heatmap that can be overlaid on the original image.
\end{enumerate}

While the implementation includes a mechanism for filtering low-attention connections through a discard ratio parameter, this feature was not utilized in the final analysis (set to 0.0), focusing instead on the complete attention distribution across all patches.

\subsection{Grad-CAM for Vision Transformers}

In addition to Attention Rollout, this study also implements Grad-CAM for Vision Transformers to provide a more direct comparison with the CNN-based visualizations. Two variants of Grad-CAM for ViT were implemented: Enhanced ViT and Interpretable ViT, each with specific architectural modifications to facilitate interpretation.

\subsection{Enhanced ViT Implementation}

The Enhanced ViT approach modifies the standard ViT architecture to facilitate Grad-CAM visualization:

\begin{itemize}
\item \textbf{Model Architecture:} The Enhanced ViT model utilizes the base ViT by replacing the original classification head with an identity layer, preserving the full token representation from the transformer blocks.

\item \textbf{Attention Mechanism:} An attention layer computes scalar importance scores for all tokens (including both [CLS] and patch tokens), which are then normalized through a softmax operation to produce attention weights.

\item \textbf{Feature Integration:} These attention weights are applied to the token features through a weighted sum operation, creating a single feature vector that represents the entire image with emphasis on the most important regions.

\item \textbf{Gradient Recording:} The model registers hooks to capture token features during the forward pass and their gradients during backpropagation, essential for the Grad-CAM calculation.

\item \textbf{Grad-CAM Computation:} For visualization, the class token is excluded, and only patch tokens (corresponding to image regions) are considered. The tokens and gradients are reshaped from the sequence format back to a 2D spatial grid to recover the image structure.

\item \textbf{Activation Map Generation:} Following standard Grad-CAM procedure, channel-wise weights are computed from gradients and applied to token features to create the final activation map highlighting discriminative regions.
\end{itemize}

\subsection{Interpretable ViT with Feature Concatenation}

The Interpretable ViT variant employs a feature concatenation approach that enhances interpretability:

\begin{itemize}
\item \textbf{Feature Extraction:} The implementation first extracts features from the base ViT model, producing a sequence of token embeddings.

\item \textbf{Token Separation:} After feature extraction, the [CLS] token and patch tokens are explicitly separated for differential processing:
\begin{align}
\text{cls\_token} &= \text{tokens}[:, 0, :] \\
\text{patch\_tokens} &= \text{tokens}[:, 1:, :]
\end{align}

\item \textbf{Attention Weighting:} An attention layer computes importance weights specifically for the patch tokens, determining each patch's contribution to the final representation:
\begin{align}
\text{attn\_scores} &= \text{attention\_layer}(\text{patch\_tokens}) \\
\text{attn\_weights} &= \text{softmax}(\text{attn\_scores}, \text{dim}=1)
\end{align}

\item \textbf{Weighted Representation:} The patch tokens are combined using these attention weights to form a single weighted feature vector:
\begin{align}
\text{weighted\_patch} = \sum_{i} \text{attn\_weights}_i \cdot \text{patch\_tokens}_i
\end{align}

\item \textbf{Feature Concatenation:} The [CLS] token (providing global context) and the weighted patch features (providing localized information) are concatenated to form a comprehensive representation:
\begin{align}
\text{combined} = [\text{cls\_token}; \text{weighted\_patch}]
\end{align}

\item \textbf{Classifier Architecture:} To accommodate this concatenated representation, the classifier's first layer accepts double the embedding dimension as input:
\begin{align}
\text{classifier} = \text{LayerNorm}(\text{embed\_dim} \times 2) \rightarrow \text{Dropout} \rightarrow \text{Linear} \rightarrow \ldots
\end{align}

\item \textbf{Grad-CAM Visualization:} During Grad-CAM computation, gradients flow back through this architecture, capturing how both global context and local features influence the classification decision.
\end{itemize}

Both approaches enable visualization of the ViT's decision-making process using gradient information, but with different mechanisms for integrating token features and attention weights. The Enhanced ViT applies attention across all tokens before classification, while the Interpretable ViT explicitly separates and recombines different types of features to provide a more structured view of the classification process.

\subsection{Comparison Framework}

To facilitate a fair comparison between CNN model and ViT interpretability, the following standardized approach was implemented:

\begin{itemize}
\item \textbf{Consistent Processing:} All models process the same test images with identical preprocessing (resizing to $224 \times 224$ and normalization).
\item \textbf{Three-Panel Visualization:} Each result is presented with three panels:
\begin{enumerate}
\item Original image
\item Raw heatmap showing the attention or Grad-CAM output
\item Overlay of the heatmap on the original image
\end{enumerate}
\item \textbf{Classification Analysis:} Both correct and incorrect predictions are analyzed to understand model behavior in different scenarios.
\item \textbf{Visualization Standardization:} Similar color maps (jet) and overlay techniques are used for all methods to maintain visual consistency.
\item \textbf{Quantitative Assessment:} Confusion matrices are generated for all models to quantitatively assess their performance alongside the visual interpretations.
\end{itemize}

By implementing these complementary interpretability techniques, this research provides insights into how different neural network architectures—traditional CNNs and modern Transformers—approach the same classification task and what are the distinguishing features between the two classes. The visualizations reveal different feature priorities and decision strategies that each architecture employs, contributing to a deeper understanding of model behavior \cite{dosovitskiy2021imageworth16x16words}.


% \begin{itemize}
%     \item \textbf{Confidence Calculation}: Computing the softmax probability for the predicted class to indicate the model's confidence.
    
%     \item \textbf{Misclassification Analysis}: For incorrectly classified images, both the original image and Grad-CAM visualization are saved with annotations indicating the true and predicted classes.
    
%     \item \textbf{Three-Panel Visualization}: Creating a standardized visualization with the original image, the Grad-CAM heatmap, and the overlay for easy comparison.
% \end{itemize}



% \subsection{Implementation Details}

% The interpretability frameworks were implemented with the following technical considerations:

% \subsubsection{Grad-CAM for VGG16}
% \begin{itemize}
%     \item \textbf{Target Layer}: The last convolutional layer of VGG16 (features[-1])
%     \item \textbf{Gradient Calculation}: Using PyTorch's autograd functionality for backpropagation
%     \item \textbf{Output Format}: Three-panel visualization with original image, Grad-CAM heatmap, and overlay
%     \item \textbf{Confidence Annotation}: Each visualization includes the model's confidence percentage
%     \item \textbf{Organization}: Images organized by class and correctness of prediction
% \end{itemize}

% \subsubsection{Attention Rollout for ViT}
% \begin{itemize}
%     \item \textbf{Model Type}: Vision Transformer Base model with 16×16 patch size
%     \item \textbf{Attention Collection}: Forward hooks on all self-attention blocks
%     \item \textbf{QKV Extraction}: Capturing query, key, and value matrices for attention computation
%     \item \textbf{Discard Ratio}: Configurable threshold (set to 0.0) to filter out low-attention areas
%     \item \textbf{Rollout Computation}: Sequential multiplication of attention maps across layers
% \end{itemize}

% \subsubsection{Grad-CAM for ViT}
% \begin{itemize}
%     \item \textbf{Model Architecture}: Custom InterpretableViT with token feature extraction
%     \item \textbf{Gradient Hooks}: Custom hooks to capture token features and their gradients
%     \item \textbf{Attention Layer}: Additional attention mechanism to weight patch tokens
%     \item \textbf{Feature Combination}: Concatenation of class token and weighted patch features
%     \item \textbf{Visualization}: Consistent with the VGG16 Grad-CAM approach for comparison
% \end{itemize}

% \appendix
% \section{Additional Interpretability Techniques}
% \label{appendix:add_interpret}

% In addition to the Grad-CAM method discussed in the main sections of this report, several other interpretability techniques were explored during this research. While not featured prominently in the main analysis, these methods offer complementary perspectives on model decision-making.

% \subsection{DeepLIFT}

% DeepLIFT (Deep Learning Important FeaTures) was implemented using the Captum library for the VGG16 and ViT models. This technique compares activations against a reference baseline (typically zeros) to determine feature importance:

% \begin{itemize}
%     \item A zero baseline tensor is created as a reference point
%     \item Attribution scores are calculated comparing actual activations to this baseline
%     \item Channel-wise attributions are aggregated and normalized for visualization
%     \item Color maps are applied to create interpretable heatmaps
% \end{itemize}

% While DeepLIFT provides more theoretically grounded attributions than simple gradient methods, it was not extended to all models due to it not being used as frequently by others as other techniques such as grad cam/
% \subsection{Saliency Maps}

% Basic saliency maps were implemented for both VGG16 and Vision Transformer models:

% \begin{itemize}
%     \item Input tensors are configured to track gradients
%     \item Forward and backward passes compute gradients from prediction to input
%     \item Absolute values of gradients reveal input sensitivity
%     \item Channel-wise maximum operations create single-channel visualizations
% \end{itemize}


% % This paper examines saliency methods, which are explanation techniques designed to highlight relevant features in inputs (particularly images) that influence a machine learning model's predictions. The authors propose a methodology to evaluate whether these explanation methods are actually providing meaningful insights about the model and data. Their key findings include:

% % Many popular saliency methods fail basic "sanity checks" - they produce explanations that are independent of model parameters and training data.
% % The authors propose two randomization tests to evaluate explanation methods:

% % Model parameter randomization test: Compares explanations from a trained model vs. a randomly initialized model
% % Data randomization test: Compares explanations from a model trained on proper data vs. one trained on randomly permuted labels


% % Several widely used methods (Guided BackProp and Guided GradCAM) fail these tests, meaning they can't provide insights about the model or data relationships.
% % Some methods (standard Gradients and GradCAM) pass the tests, suggesting they are more reliable.
% % The authors demonstrate that some saliency methods produce outputs visually similar to simple edge detectors, which require neither training data nor model knowledge.

% % The paper warns that visual assessment alone can be misleading when evaluating explanation methods. Methods that fail their proposed tests are inadequate for tasks like finding data outliers, explaining model relationships, or debugging models. The authors support their findings with theoretical analysis of linear models and simple convolutional neural networks.
% \subsection{Integrated Gradients for Vision Transformers}

% For the Vision Transformer model specifically, Integrated Gradients was implemented using Captum:

% \begin{itemize}
%     \item Path integrals are approximated between baseline and input images
%     \item 50 interpolation steps were used for the approximation
%     \item Attributions are processed to align with spatial image dimensions
%     \item Results are normalized for consistent visualization
% \end{itemize}

% While theoretically sound, Integrated Gradients requires significantly more computation time than Grad-CAM. Additionally, the literature review indicated that Grad-CAM adaptations for Vision Transformers often provide more intuitive visualizations for classification tasks \cite{chefer2021transformer}.

% \subsection{Guided Backpropagation for Vision Transformers}

% Guided Backpropagation was adapted specifically for Vision Transformers:

% \begin{itemize}
%     \item Modified gradient flow during backpropagation highlights positive influences
%     \item Implementation uses Captum's GuidedBackprop with model-specific wrappers
%     \item Results are post-processed to create single-channel visualizations
% \end{itemize}

% This method produces visually cleaner results than standard saliency maps but lacks the localization capabilities of Grad-CAM. Additionally, research has shown that guided backpropagation may highlight input patterns rather than decision-relevant features \cite{nie2018theoretical}.

% \subsection{Conclusion on Additional Methods}

% These additional interpretability techniques were implemented primarily for exploration and comparison. Grad-CAM was selected as the primary visualization method for the main analysis due to its:

% \begin{itemize}
%     \item Widespread adoption in the literature
%     \item Consistent performance across model architectures
%     \item Ability to produce class-discriminative visualizations
%     \item Better localization of discriminative regions
%     \item Lower computational requirements
%     \item Stronger theoretical foundation compared to simple gradient methods
% \end{itemize}

% The implementation details and experimental results for these additional techniques are preserved here to document the comprehensive approach taken toward model interpretability in this research.

% Though straightforward to implement, saliency maps,  guided backpropagation were not highlighted in the main analysis as they can be noisy and lack the class-discriminative properties of Grad-CAM. Additionally, as noted by \cite{Adebayo2018}, simple gradient methods sometimes fail sanity checks that Grad-CAM passes.

% 3. Of the methods tested, Gradients and GradCAM pass the sanity checks, while Guided BackProp and
% Guided GradCAM are invariant to higher layer parameters; hence, fail.
% 4. Consequently, our findings imply that the saliency methods that fail our proposed tests are incapable
% of supporting tasks that require explanations that are faithful to the model or the data generating
% process.  \href{https://arxiv.org/pdf/1810.03292}{(Guided and saliency not good)}

% TODO
% \appendix
\section{Additional Interpretability Techniques}
\label{appendix:add_interpret}

In addition to the Grad-CAM and attention rollout methods discussed in the main sections of this report, several other interpretability techniques were explored during this research.

\subsection{DeepLIFT}

DeepLIFT (Deep Learning Important FeaTures) was implemented using the Captum library for the VGG16 and ViT models. This technique compares activations against a reference baseline (typically zeros) to determine feature importance:

\begin{itemize}
\item A zero baseline tensor is created as a reference point
\item Attribution scores are calculated comparing actual activations to this baseline
\item Channel-wise attributions are aggregated and normalized for visualization
\end{itemize}

While DeepLIFT provides theoretically grounded attributions, it was not adopted for the main analysis due to its limited adoption in the literature compared to prevalent techniques like Grad-CAM.

\subsection{Saliency Maps}

\begin{itemize}
\item Gradients of predictions with respect to input pixels are computed
\item Absolute values of gradients are visualized as sensitivity maps
\end{itemize}

Saliency maps, though computationally efficient, often produce noisy visualizations and lack class-discriminative properties. As shown in prior work \citep{Adebayo2018}, they may also fail basic sanity checks, making them unreliable for tasks requiring faithful explanations.

\subsection{Integrated Gradients for Vision Transformers}

Integrated Gradients was implemented for ViT using 50 interpolation steps:

\begin{itemize}
    \item Path integrals are approximated between baseline and input images
    \item 50 interpolation steps were used for the approximation
    \item Attributions are processed to align with spatial image dimensions
    \item Results are normalized for consistent visualization
\end{itemize}

Despite its theoretical soundness, Integrated Gradients requires significantly more computation than Grad-CAM. Additionally, the literature review indicated that Grad-CAM adaptations for Vision Transformers often provide more intuitive visualizations for classification tasks \cite{chefer2021transformer}

\subsection{Guided Backpropagation for ViT}


\begin{itemize}
    \item Modified gradient flow during backpropagation highlights positive influences
    \item Implementation uses Captum's GuidedBackprop with model-specific wrappers
    \item Results are post-processed to create single-channel visualizations
\end{itemize}

While visually cleaner than saliency maps, Guided Backpropagation has two key limitations: (1) it lacks localization capabilities, and (2) as demonstrated by \citep{Adebayo2018}, it fails parameter randomization tests, (3) it may highlight input patterns rather than model decision logic \cite{nie2018theoretical}.

\subsection{Justification for Grad-CAM}

Grad-CAM was chosen as the primary interpretability method for this study due to its unique advantages over the alternatives:

\begin{itemize}
\item \textbf{Theoretical Robustness}: Grad-CAM passes sanity checks (e.g., parameter randomization tests) that methods like Guided Backpropagation and Guided Grad-CAM fail \citep{Adebayo2018}.
\item \textbf{Class-Discriminative}: Unlike saliency maps or Integrated Gradients, Grad-CAM highlights regions relevant to specific predicted classes.
\item \textbf{Architecture Flexibility}: Works consistently across CNNs (e.g., VGG16, ResNet) and adapted Vision Transformers.
\item \textbf{Computational Efficiency}: Requires only a single backward pass per class, unlike path-based methods (e.g., Integrated Gradients).
\item \textbf{Empirical Validation}: Supported by extensive literature \citep{Selvaraju_2019, chefer2021generic} demonstrating its effectiveness for classification tasks.
\end{itemize}
% Add these to your bibliography
% \bibitem{adebayo2018sanity} Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., \& Kim, B. (2018). Sanity checks for saliency maps. In Advances in Neural Information Processing Systems.
% \bibitem{chefer2021transformer} Chefer, H., Gur, S., \& Wolf, L. (2021). Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
% \bibitem{nie2018theoretical} Nie, W., Zhang, Y., \& Patel, A. (2018). A theoretical explanation for perplexing behaviors of backpropagation-based visualizations. In International Conference on Machine Learning.

\section{Feature Analysis for Fine-Grained Gull Classification}

A multi-stage approach was employed to extract and analyze region-specific features.

\subsection{Image Selection}
From the normalized dataset, 200 high-resolution images (100 per species) were selected based on the following criteria:
\begin{itemize}
\item Correct classification by the best-performing VGG-16 model
\item Prominent Grad-CAM activations in wing or wingtip regions
\item Clear visibility of upperwing or wingtip in the image
\item High resolution image
\end{itemize}

\subsection{Manual Segmentation}
Using Adobe Photoshop, segmentation masks with color-coded regions were created with a specific color for each type of region: wing, wingtip, and head.

\subsection{Normalization}
Applied min-max normalization\citep{monzon_image_normalization} (0-255) to grayscale conversions using OpenCV's \texttt{NORM\_MINMAX}.
This helped standardize intensity values and mitigate the effects of differing lighting conditions ensuring that pixel intensity values were scaled consistently across the dataset, providing a reliable foundation for subsequent region-based analysis.

\subsection{Region Extraction}

Using OpenCV's \texttt{inRange} for color-based thresholding, pixels within $\pm$10 intensity values of each target color were extracted and used for further analysis.

% \section{Local Binary Pattern Gemini}

% \section{Methodology}

% \subsection{Local Binary Pattern Analysis}

% The texture analysis methodology employs Local Binary Patterns (LBP) to quantify the textural characteristics of gull feather regions. LBP is a powerful texture descriptor that captures local spatial structure by comparing each pixel with its neighboring pixels.



% We implemented uniform LBP patterns, which account for transitions between 0s and 1s in the binary representation, thereby capturing fundamental texture elements like edges and corners while reducing dimensionality.

% \subsubsection{Feature Extraction}

% From the LBP computation, three primary feature histograms were extracted:

% \begin{enumerate}
%     \item \textbf{LBP Histogram}: Distribution of LBP codes across the region of interest
    
%     \item \textbf{Ones Histogram}: Distribution of the number of 1s in each LBP code, calculated as:
%     \begin{equation}
%     H_{\text{ones}}(i) = \sum_{j=0}^{N-1} [b(j) = i]
%     \end{equation}
%     where $b(j)$ represents the number of 1s in pattern $j$, and $[\cdot]$ is the Iverson bracket
    
%     \item \textbf{Transitions Histogram}: Distribution of 0-to-1 or 1-to-0 transitions in each LBP code:
%     \begin{equation}
%     H_{\text{trans}}(i) = \sum_{j=0}^{N-1} [t(j) = i]
%     \end{equation}
%     where $t(j)$ counts the transitions in pattern $j$
% \end{enumerate}

% \subsection{Texture Feature Quantification}

% From these histograms, several texture properties were calculated to characterize the regions:

% \begin{enumerate}
%     \item \textbf{Entropy}: Measures randomness in texture patterns
%     \begin{equation}
%     E = -\sum_{i} h(i) \log_2 h(i)
%     \end{equation}
%     where $h(i)$ is the normalized histogram value at bin $i$
    
%     \item \textbf{Energy}: Measures uniformity of texture
%     \begin{equation}
%     U = \sum_{i} h(i)^2
%     \end{equation}
    
%     \item \textbf{Contrast}: Measures intensity variations
%     \begin{equation}
%     C = \sum_{i} (i - \mu)^2 h(i)
%     \end{equation}
%     where $\mu$ is the mean value of the histogram
    
%     \item \textbf{Homogeneity}: Measures closeness of distribution
%     \begin{equation}
%     H = \sum_{i} \frac{h(i)}{1 + |i - \mu|}
%     \end{equation}
% \end{enumerate}

\subsection{Local Binary Pattern Analysis}

Local Binary Patterns (LBP) constitute a robust and computationally efficient texture analysis technique, originally proposed by Ojala et al. \Citep{1017623}. LBP characterizes local image texture by comparing each pixel's intensity with its surrounding neighbors, generating a binary code that summarizes the local structure. This descriptor is particularly advantageous for biological image analysis due to several key properties:

\begin{itemize}
    \item Robustness to monotonic grayscale variations, enhancing resilience to changing illumination conditions.
    \item Rotation invariance when using specific LBP variants or derived features (1s and transition histograms)
    \item Ability to capture fine-scale micro-patterns potentially imperceptible to human vision.
\end{itemize}

To investigate subtle textural differences between Slaty-backed Gulls and Glaucous-winged Gulls, Local Binary Pattern (LBP) analysis was conducted on three anatomical regions: wing, wingtip, and head.

\subsubsection{LBP Computation}

For a given pixel at position $(x_c, y_c)$ with intensity $g_c$, the LBP value is computed by comparing it with $P$ neighbors at radius $R$ using the following equation:

\begin{equation}
\text{LBP}_{P,R}(x_c, y_c) = \sum_{p=0}^{P-1} s(g_p - g_c) \cdot 2^p
\end{equation}

where:
\begin{itemize}
    \item $g_p$ is the intensity of the neighbor pixel $p$
    \item $s(x)$ is the step function defined as:
\end{itemize}

\begin{equation}
s(x) = 
\begin{cases}
1, & \text{if } x \geq 0 \\
0, & \text{if } x < 0
\end{cases}
\end{equation}

\begin{equation}
\text{LBP}_{P,R} = \sum_{p=0}^{P-1} s(g_p - g_c) \cdot 2^p,\quad s(x) = 
\begin{cases}
1, & x \geq 0 \\
0, & x < 0
\end{cases}
\end{equation}

Two variants were implemented:
\begin{itemize}
    \item \textbf{Default LBP}: Standard implementation without uniformity constraints
    \item \textbf{Uniform LBP}: Considers patterns with $\leq 2$ bitwise transitions
\end{itemize}

For the final results, the default LBP variant was selected since default histogram retains all possible LBP codes, providing a comprehensive representation of local texture patterns. Consequently, it facilitates the direct calculation of derived features such as the number of ones and the number of transitions per pattern, as these statistics can be computed from the complete set of binary codes present in the default histogram. This approach ensures that no potentially discriminative texture information is excluded, thereby supporting a more detailed and flexible analysis of texture complexity and structure across the studied regions.

\subsection{Feature Extraction}
\label{subsec:feature_extraction}

Three feature histograms were derived from LBP codes:

\begin{enumerate}
    \item \textbf{LBP Histogram}: Raw distribution of LBP codes
    
    \item \textbf{Ones Histogram}: Frequency of patterns with $n$ ones:
    \begin{equation}
    H_{\text{ones}}(i) = \sum_{j=0}^{N-1} [b(j) = i]
    \end{equation}
    
    \item \textbf{Transitions Histogram}: Frequency of patterns with $m$ transitions:
    \begin{equation}
    H_{\text{trans}}(i) = \sum_{j=0}^{N-1} [t(j) = i]
    \end{equation}
\end{enumerate}

Texture properties were calculated from these histograms:

\begin{enumerate}
    \item \textbf{Entropy}: Measures randomness in texture patterns
    \begin{equation}
    E = -\sum_{i} h(i) \log_2 h(i)
    \end{equation}
    where $h(i)$ is the normalized histogram value at bin $i$
    
    \item \textbf{Energy}: Measures uniformity of texture
    \begin{equation}
    U = \sum_{i} h(i)^2
    \end{equation}
    
    \item \textbf{Contrast}: Measures intensity variations
    \begin{equation}
    C = \sum_{i} (i - \mu)^2 h(i)
    \end{equation}
    where $\mu$ is the mean value of the histogram
    
    \item \textbf{Homogeneity}: Measures closeness of distribution
    \begin{equation}
    H = \sum_{i} \frac{h(i)}{1 + |i - \mu|}
    \end{equation}
\end{enumerate}


\subsection{Comparative Analysis Implementation}

To quantify the differences between species, the implementation calculates several distribution similarity metrics:

\begin{enumerate}
    \item \textbf{KL Divergence}: A symmetric version of the Kullback-Leibler divergence that measures how one probability distribution diverges from another

    \item \textbf{Earth Mover's Distance}: Measures the minimum ``work'' required to transform one histogram into another, considering the distance between bins

    \item \textbf{Chi-Square Distance}: A statistical test that measures the difference between observed and expected frequency distributions

    \item \textbf{Jensen-Shannon Distance}: A symmetric and smoothed version of the KL divergence with better numerical properties
\end{enumerate}

Each metric captures different aspects of distribution similarity, providing a robust framework for comparing texture patterns between species.

\subsection{Discriminative Power Analysis}

A systematic assessment of discriminative power for different texture features and regions is performed by calculating percentage differences between species for each texture property and region. This analysis identifies which features and regions exhibit the most significant differences between species.

For each region and property combination, the implementation:
\begin{enumerate}
    \item Extracts the property value for each species
    \item Calculates the percentage difference
    \item Ranks the region-property pairs by their discriminative power
\end{enumerate}

This approach enables the identification of the most promising texture characteristics for species differentiation.

\subsection{Dimensionality Reduction and Visualization}

The implementation uses Principal Component Analysis (PCA) to visualize the high-dimensional LBP feature space in two dimensions:

\begin{enumerate}
    \item Standardize the feature data
    \item Apply PCA with 2 components
    \item Create scatter plots colored by species
    \item Calculate and display variance explained
\end{enumerate}

This dimensionality reduction approach provides intuitive visualization of class separability and validates the discriminative power of the extracted features.

\subsection{Implementation Workflow}

The complete analysis pipeline consists of two main modules:

\begin{enumerate}
    \item \textbf{Feature Extraction Module}:
    \begin{itemize}
        \item Loads original and segmentation images
        \item Extracts and processes each anatomical region
        \item Computes LBP features and abstract pattern features
        \item Saves features to CSV files for further analysis
    \end{itemize}

    \item \textbf{Analysis Modules}:
    \begin{itemize}
        \item Loads extracted features
        \item Calculates advanced texture statistics
        \item Performs comparative analysis between species
        \item Generates visualizations and statistical reports
        \item Identifies the most discriminative features
    \end{itemize}
\end{enumerate}

This modular approach facilitates efficient processing of large image datasets and enables iterative refinement of the analysis parameters.

\subsection{Validation Approach}

The implementation includes a few validation mechanisms:

\begin{enumerate}
    \item \textbf{Normalized histograms} to account for varying region sizes
    \item \textbf{Multiple comparison metrics} to ensure robust similarity assessment
    \item \textbf{Quantitative comparison metrics} (KL divergence, percentage differences) to assess feature differences
\end{enumerate}

\section{Wing and Wingtip Intensity Analysis}
% \href{https://doi.org/10.1111/j.1474-919X.2007.00703.x}{(Olsen and Larsson, 2007)}
The interpretability analysis of our VGG model indicated a strong focus on wing and wingtip regions when differentiating between Slaty-backed Gulls and Glaucous-winged Gulls. This aligns with ornithological knowledge, as these species exhibit distinct differences in wing coloration patterns, particularly in the wing and wingtip regions. To quantitatively validate these differences and check whether the differences are significant, an in-depth image analysis was conducted focusing on these critical regions.

% This aligns with known plumage differences documented in ornithological studies \cite{olsen2007gulls}.

% TODO
We employed a multi-stage approach to extract and analyze region-specific features:

\subsection{Feature Extraction Pipeline}
The Python pipeline executed these steps for each image:
\begin{enumerate}
    \item \textbf{Region-Specific Analysis}: Calculated intensity statistics (mean, median, std, skewness, kurtosis, minimum and maximum values) per region
 
    \item \textbf{Wingtip Characterization}:
    \begin{itemize}
        \item Analyzed intensity distribution across 25 bins (10-unit intervals):
        \begin{verbatim}
        intensity_ranges = [
            (0,10), (10,20), ..., (240,255)  # 25 total bins
        ]
        \end{verbatim}
        \item Quantified dark pixels using thresholds: <30, <40, <50, <60 intensity
    \end{itemize}
    \item \textbf{Wing-Wingtip Difference}
    \begin{itemize}
        \item Computed percentage of wingtip pixels darker than mean wing intensity of the bird.
        \item \textbf{Thresholds}: Calculated the proportion of wingtip pixels exceeding specific difference thresholds (10, 20, ..., 100) compared to the mean wing intensity, quantifying the contrast between regions.
    \end{itemize}
\end{enumerate}

\subsection{Statistical Comparison}
Statistical testing using SciPy was performed where t-test for unequal variances was implemented.


% Limitations
% \paragraph{Limitations and Future Work} While manual segmentation ensured biological accuracy, it introduced human variability. Future implementations could use U-Net models \cite{ronneberger2015unet} for automated segmentation. The 10-unit intensity bins provided granular analysis but may obscure broader patterns visible in 25-unit groupings used by field guides \cite{pyle2015identification}.

% \subsection{Statistical Comparison}
% To determine whether the observed differences between species were statistically significant, we performed:

% \begin{itemize}
%     \item t-tests comparing mean intensity values between species
%     \item Analysis of the percentage of pixels falling into each intensity range
%     \item Comparison of the wing-wingtip contrast metrics between species
% \end{itemize}

% \href{https://doi.org/10.1676/13-107.1}{(Howell and Dunn, 2014)}

These statistical analyses allowed us to find and objectively validate whether the differences found through the analysis in areas highlighted by Grad-CAM (wing and wingtip) between species are quantitatively significant.

The entire analysis pipeline was implemented in Python, utilizing libraries including OpenCV for image processing, NumPy and Pandas for data manipulation, and SciPy for statistical testing. Various visualization plots were created to represent the analysis results that include threshold-based differences, darkness metrics, pixel intensity distributions. per-image variability, and wing vs wingtip intensity relationships.

\section{Verification by Clustering for Species Differentiation}

\subsection{Overview of Clustering Approach}

To validate and complement the results achieved through the analysis of the region-specific features that were significantly different among the 2 species, a comprehensive clustering analysis framework was implemented that leverages traditional machine learning techniques to check whether the two species can be classified using the features. This approach helps validate the discriminative features identified by the deep learning models that were quantified.

\subsection{Feature Extraction and Preprocessing}

The clustering analysis utilizes three key morphological features extracted from the wingtip regions, specifically \textbf{Mean wing intensity}, \textbf{Mean wingtip intensity}, and \textbf{Count of darker pixels in wingtip regions}.

Prior to clustering analysis, the following preprocessing steps were implemented:
\begin{enumerate}
    \item \textbf{Data Loading}: The dataset was loaded from the CSV file (produced by intensity analysis script), with features and species labels separated for analysis.
    \item \textbf{Feature Standardization}: All features were standardized using \texttt{StandardScaler} to ensure each feature contributed equally to the clustering algorithms, preventing features with larger magnitudes from dominating the analysis.
    \item \textbf{Dimensionality Reduction}: Principal Component Analysis (PCA) was applied to reduce the three-dimensional feature space to two dimensions for visualization purposes. The explained variance ratio was calculated to assess how much information was preserved in the lower-dimensional representation.
\end{enumerate}

\subsection{Clustering Algorithms Implementation}
Three distinct clustering algorithms were implemented to provide a comprehensive evaluation of the feature space with Number of clusters/components set to 2 (corresponding to the two gull species).

\subsubsection{K-means Clustering}
K-means clustering was implemented with multiple initializations (\texttt{n\_init = 10}) to avoid local optima.
K-means partitions the data by minimizing the within-cluster sum of squares. The algorithm iteratively assigns data points to the nearest cluster centroid and then recalculates the centroids based on the new cluster assignments until convergence is achieved.

\subsubsection{Hierarchical Clustering}

Hierarchical clustering was implemented with Ward's linkage. This approach builds a hierarchy of clusters. Initially, each data point is treated as a single cluster.  The algorithm then iteratively merges the closest pairs of clusters until only the specified number of clusters remains. Ward's linkage was chosen to minimize the variance within the newly formed clusters at each merge, promoting the creation of compact, spherical clusters.

\subsubsection{Gaussian Mixture Model}

GMM assumes that the data is generated from a mixture of a finite number of Gaussian distributions with unknown parameters. The algorithm estimates the parameters of these distributions and assigns data points to clusters based on the probability that they belong to each distribution. This probabilistic approach allows for more flexibility in cluster shapes compared to K-means.

\subsection{Evaluation Metrics}
The effectiveness of each clustering algorithm was evaluated using multiple metrics:

\subsubsection{Unsupervised Evaluation}
\textbf{Silhouette Score}: Measures how similar a data point is to its own cluster compared to other clusters. Values range from -1 to 1, with higher values indicating better-defined clusters.

\subsubsection{Supervised Evaluation (Using Known Species Labels)}
\textbf{Adjusted Rand Index (ARI)}: Measures the similarity between the true species assignments and the clustering results, adjusted for chance. Values range from -1 to 1, with higher values indicating greater similarity.
\textbf{Clustering Accuracy}: Calculated as the percentage of specimens correctly classified after mapping clusters to the majority species within each cluster.
\textbf{Confusion Matrix}: Visualizes the number of correctly and incorrectly clustered specimens after mapping cluster labels to species labels.

\subsection{Cluster Mapping and Misclassification Analysis}
To interpret the clustering results in terms of species classification:
\begin{enumerate}
    \item \textbf{Cluster-to-Species Mapping}: For each cluster, the majority species was identified, creating a mapping from cluster labels to species labels.
    \item \textbf{Misclassification Analysis}: Using this mapping, specimens were identified as correctly or incorrectly clustered. Misclassified specimens were visualized in the PCA space and exported to CSV files for further analysis.
\end{enumerate}

\subsection{Visualization and Interpretation}
Multiple visualizations were generated to aid interpretation:
\begin{enumerate}
    \item \textbf{PCA Plots}: Showing the distribution of specimens in the reduced two-dimensional space with color-coding by cluster assignment.
    \item \textbf{Cluster Centers}: For K-means and GMM, cluster centers were plotted in the PCA space to visualize the centroids of each cluster.
    \item \textbf{Misclassification Visualization}: Highlighting correctly and incorrectly clustered specimens in the PCA space to identify areas of confusion.
    \item \textbf{Feature Importance Plots}: Bar charts showing the relative importance of each feature based on cluster center differences (for K-means).
    \item \textbf{Algorithm Comparison}: Bar charts comparing the silhouette scores across all clustering algorithms to assess their performance.
\end{enumerate}
This comprehensive clustering analysis provides valuable insights into the natural grouping of morphological features and helps validate the discriminative power of the significantly different features found from the analysis performed on regions of the birds highlighted by Grad-CAM. 

\subsection{Implementation Details}

The entire analytical workflow was implemented in Python using the following libraries:

\begin{itemize}
    \item \texttt{scikit-learn} for clustering algorithms, PCA, and evaluation metrics
    \item \texttt{pandas} for data management
    \item \texttt{NumPy} for numerical operations
    \item \texttt{Matplotlib} and \texttt{Seaborn} for visualization
\end{itemize}
A consistent random state (42) was used throughout the analysis to ensure reproducibility. Results, including visualizations and misclassification data, were automatically saved to an output directory for documentation and further analysis.

This comprehensive methodology allowed for a robust evaluation of whether the extracted statistical features could effectively distinguish between the two gull species through unsupervised clustering, providing valuable insights into feature significance for species classification.

\chapter{Results}

\section{Model Performance of top-performing models trained on latest Stage 3 dataset}

% Table 4: Validation and Test Accuracy Comparison
\begin{table}[htbp]
    \centering
    \caption{Validation and Test Accuracy Comparison}
    \label{tab:val_test_comparison}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{Val. Acc. (\%)} & \textbf{Test Acc. (\%)} \\
    \midrule
    VGG-16  & 97.37 & 95.74 \\
    ViT & 94.74 & 93.62 \\
    Inception v3 & 97.80 & 91.49  \\
    ResNet50 & 96.04 & 82.98 \\
    EnhancedViT & 94.52 & 95.74  \\
    InterpretableViT & 94.74 & 95.74 \\
    \bottomrule
    \end{tabular}
\end{table}


\begin{table}[htbp]
    \centering
    \caption{Comprehensive Model Performance Metrics on Test Dataset}
    \label{tab:combined_performance}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Glaucous Winged}} & \multicolumn{3}{c}{\textbf{Slaty Backed}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F1 (\%)} & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F1 (\%)} \\
    \midrule
    VGG-16 & 87.50 & 100.00 & 93.33 & 100.00 & 93.94 & 96.88 \\
    ViT & 100.00 & 78.57 & 88.00 & 91.67 & 100.00 & 95.65 \\
    Inception v3 & 77.78 & 100.00 & 87.50 & 100.00 & 87.88 & 93.55 \\
    ResNet50 & 66.67 & 85.71 & 75.00 & 93.10 & 81.82 & 87.10 \\
    EnhancedViT & 87.50 & 100.00 & 93.33 & 100.00 & 93.94 & 96.88 \\
    InterpretableViT & 92.86 & 92.86 & 92.86 & 96.97 & 96.97 & 96.97 \\
    \bottomrule
    \end{tabular}%
    }
\end{table}

Based on the results presented in Tables~\ref{tab:val_test_comparison} and~\ref{tab:combined_performance}, it is evident that several models achieved high validation and test accuracy although there were notable differences in their interpretability and localization capabilities. Specifically, qualitative analysis of Grad-CAM and interpretability visualizations (see Appendix~\ref{appendix:interpretability}) revealed that most models, with the exception of VGG-16, failed to consistently highlight the biologically relevant regions-namely, the wings and wingtip areas-of the gull images. Therefore, VGG-16 was selected as the best-performing model.

\section{Grad-CAM outputs of best performing VGG-16 model}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/interpretability/vgg/1.png}
    \caption{Grad-CAM visualizations for a selection of test images highlighting wing and wingtip regions of the VGG-16 model}
    \label{fig:vgg16_gradcam}
\end{figure}

As shown in Figure~\ref{fig:vgg16_gradcam}, the Grad-CAM outputs from the VGG-16 model highlight the wing and wingtip regions of the birds


\section{Intensity Analysis Results}
% images/REPORT_IMAGES_INTENSITY/I2/intensitydistribution.png

\subsection{Wing and Wingtip Intensity Analysis}

The intensity analysis revealed significant quantifiable differences in wing and wingtip intensities between Slaty-backed Gulls and Glaucous-winged Gulls, providing strong discriminative features for species identification.

Statistical analysis of both wing and wingtip intensity demonstrated clear and significant differences between the two gull species across multiple samples. Independent samples t-tests (Welch's t-tests, allowing for unequal variances) were performed for both wing and wingtip intensities, comparing the means of the two species. Both tests  confirmed that this difference is highly significant ($p < 0.001$), indicating that the observed differences in intensity for both wing and wingtip are extremely unlikely to have arisen by chance.

\begin{table}[H]
    \centering
    \caption{Wing and Wingtip Intensity Statistical Summary}
    \label{tab:intensity-stats-combined}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l l c c c c}
        \toprule
        Feature & Species & Mean Intensity & Std.\ Dev. & \% Difference & Statistical Significance \\
        \midrule
        \multirow{2}{*}{Wing} 
            & Slaty-backed Gull     & 73.98  & 21.90 & \multirow{2}{*}{108.3\%} & \multirow{2}{*}{$p\,<\,0.001$ (t-test)} \\
            & Glaucous-winged Gull  & 154.10 & 30.82 & & \\
        \midrule
        \multirow{2}{*}{Wingtip} 
            & Slaty-backed Gull     & 81.56  & 20.66 & \multirow{2}{*}{90.5\%} & \multirow{2}{*}{$p\,<\,0.001$ (t-test)} \\
            & Glaucous-winged Gull  & 155.36 & 32.52 & & \\
        \bottomrule
    \end{tabular}
    }
\end{table}

Test Statistic:
\begin{equation}
t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
\end{equation}

Degrees of freedom (Welch--Satterthwaite approximation):
\begin{equation}
\nu \approx \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}
\end{equation}

The combined figure (\autoref{fig:intensity_combined}) summarizes the intensity analysis results. Subfigures~\ref{fig:wing_intensity} and~\ref{fig:wing_intensity_means} show that Glaucous-winged Gulls have consistently higher and less variable wing intensity than Slaty-backed Gulls. Subfigures~\ref{fig:wingtip_intensity_violin} and~\ref{fig:wingtip_intensity_distribution} illustrate that, while wingtip intensity also differs significantly between species, there is greater variability and overlap, as seen in the broader range and outliers in the distributions.
While both wing and wingtip intensity are statistically significant distinguishing features, variations observed in the wingtip intensity distributions, potentially due to the presence of white spots and variations in darkness. For this reason the mean wing intensity may be considered a more consistently robust distinguishing feature between the two species.

\begin{figure}[H]
    \centering
    % First row
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/wing_intensity_analysis.png}
        \caption{Comparison of wing intensity values between species.}
        \label{fig:wing_intensity}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/WINGINTENSITY.png}
        \caption{Mean wing intensity across samples.}
        \label{fig:wing_intensity_means}
    \end{subfigure}
    % Vertical space between rows
    \par\vspace{1.5ex}
    % Second row
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/wingtip_intensity_violin_plot.png}
        \caption{Wingtip intensity violin plot.}
        \label{fig:wingtip_intensity_violin}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/wingtip_intensity_distribution.png}
        \caption{Wingtip intensity distribution.}
        \label{fig:wingtip_intensity_distribution}
    \end{subfigure}
    \caption{Wing and wingtip intensity metrics for Slaty-backed and Glaucous-winged Gulls, showing significant brightness differences.}
    \label{fig:intensity_combined}
\end{figure}

\subsection{Distributions of Wing and Wingtip Intensities}

A comprehensive analysis of pixel intensity distributions in both the wing and wingtip regions reveals clear, species-specific patterns that distinguish Slaty-backed Gulls from Glaucous-winged Gulls. This section presents and interprets the observed distributions using multiple complementary metrics.

\subsection{Wing Intensity Distributions}

Figure~\ref{fig:wing_intensity_distribution} displays the distribution of mean wing intensities for both species. Slaty-backed Gulls cluster at lower mean intensity values, indicating darker wings, while Glaucous-winged Gulls are concentrated at higher mean intensities, reflecting lighter wings. The distributions are largely non-overlapping, underscoring a strong differentiation in overall wing brightness between the two species.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/distribution.png}
    \caption{Distribution of mean wing intensity for Slaty-backed Gulls (blue) and Glaucous-winged Gulls (orange). Each species forms a distinct cluster, reflecting their characteristic wing brightness.}
    \label{fig:wing_intensity_distribution}
\end{figure}


\subsection{Wingtip Pixel Intensity Distributions}

The distributions of wingtip pixel intensities for both species are shown in Figure~\ref{fig:wingtip_intensity_distributions}. Slaty-backed Gulls (orange) have a higher proportion of darker pixels (lower intensity values), while Glaucous-winged Gulls (blue) show a greater proportion of lighter pixels (higher intensity values). Both grouped and fine-grained binning approaches reveal unique, largely non-overlapping patterns for each species.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/intensitydistribution.png}
        \caption{Grouped intensity bins}
        \label{fig:wingtip_intensity_distribution_grouped}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/TIPdistribution.png}
        \caption{Fine-grained intensity bins}
        \label{fig:wingtip_intensity_distribution_fine}
    \end{subfigure}
    \caption{Distributions of wingtip pixel intensities for Slaty-backed Gulls (orange) and Glaucous-winged Gulls (blue) using (a) grouped and (b) fine-grained intensity bins.}
    \label{fig:wingtip_intensity_distributions}
\end{figure}

\subsection{Presence of Very Dark Pixels}

A detailed analysis of wingtip pixel intensity reveals a striking difference in the prevalence of very dark pixels between Slaty-backed Gulls and Glaucous-winged Gulls. As shown in Figure~\ref{fig:pixels_below_thresh}, Slaty-backed Gulls have a much higher percentage of wingtip pixels below each intensity threshold examined ($<$30, $<$40, $<$50, $<$60), with values rising from 25\% to nearly 48\%. In contrast, Glaucous-winged Gulls have negligible proportions of such dark pixels at all thresholds.

Figure~\ref{fig:verydarkdist} further demonstrates that this difference is consistent across all sub-ranges within the very dark pixel spectrum (0--60), with Slaty-backed Gulls consistently showing a substantially greater percentage of pixels in each bin.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/pixelsbelowthresh.png}
        \caption{Percentage of wingtip pixels below each intensity threshold.}
        \label{fig:pixels_below_thresh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/verydarkdist.png}
        \caption{Distribution of very dark pixels (0--60) in wingtip regions.}
        \label{fig:verydarkdist}
    \end{subfigure}
    \caption{Comparison of very dark pixel proportions in wingtip regions for Slaty-backed and Glaucous-winged Gulls.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/darkpixels.png}
    \caption{Left: Percentage of very dark pixels ($<$30) by species. Right: Average count of very dark pixels ($<$30) per wingtip region, shown on a logarithmic scale.}
    \label{fig:darkpixels}
\end{figure}

The quantitative difference in very dark pixels between species is substantial, as shown in Figure~\ref{fig:darkpixels}, with Slaty-backed Gulls having on average 73,592 very dark pixels compared to just 8 in Glaucous-winged Gulls. This represents a critical diagnostic feature for species identification. This pronounced disparity represents a critical diagnostic feature for species identification. However, it is important to note that the presence of very dark pixels is not consistent across all images, so this feature cannot be used as a distinguishing criterion in every case.

\subsection{Relationship Between Wing and Wingtip Intensities}

To quantitatively assess the relative darkness of wingtip area compared to the rest of the wing which were the 2 critical areas highlighted by Grad-CAM, the proportion of wingtip pixels that are darker than the mean wing intensity for each species was calculated as suggested by industry expert.


\begin{table}[H]
    \centering
    \caption{Proportion of Wingtip Pixels Darker than Mean Wing Intensity}
    \label{tab:darknessproportion}
    \begin{tabular}{lcc}
        \toprule
        Species & Percentage \\
        \midrule
        Slaty-backed Gull & 56.69\% \\
        Glaucous-winged Gull & 47.71\% \\
        \bottomrule
    \end{tabular}
\end{table}

Figure~\ref{fig:wing_vs_wingtip_intensity} visualizes the relationship between mean wing intensity and mean wingtip intensity for individual birds. The dashed line indicates equal intensity between wing and wingtip. As illustrated, there is a strong positive correlation between wing and wingtip intensities across both species; as the mean wing intensity increases, the mean wingtip intensity also increases. This pattern suggests that, in most cases, the relative difference in darkness between the wing and wingtip results only in a slight difference in the proportion of darker wingtip pixels between the two species.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\textwidth]{images/REPORT_IMAGES_INTENSITY/I2/clusterwingwingtip.png}
    \caption{Scatter plot of mean wing intensity vs. mean wingtip intensity for Slaty-backed Gulls (blue) and Glaucous-winged Gulls (orange). The dashed line indicates equal intensity between wing and wingtip.}
    \label{fig:wing_vs_wingtip_intensity}
\end{figure}

\subsection{Wingtip Contrast and Darkness Feature Comparison}

To comprehensively compare the wingtip characteristics distinguishing Slaty-backed and Glaucous-winged Gulls, we analyzed two key features: (1) the proportion of wingtip pixels that are darker than the wing by varying intensity thresholds, and (2) the proportion of pixels that are absolutely dark, below fixed intensity values.

Figure~\ref{fig:wingtip_darkness_summary} presents a set of visualizations summarizing these metrics for both species. The heatmap in subfigure~\ref{fig:intensity_heatmap} displays, on the left, the percentage of wingtip pixels darker than the wing by various thresholds, and on the right, the percentage of absolutely dark pixels. Subfigure~\ref{fig:diff_bar} shows the absolute difference in the proportion of dark pixels between species at each threshold, while subfigure~\ref{fig:diff_curve} plots the difference curve (Slaty-backed minus Glaucous-winged) across thresholds, highlighting where the species gap is largest. Subfigure~\ref{fig:ratio_plot} quantifies the relative difference by showing the ratio of dark pixel proportions (Slaty-backed/Glaucous-winged) at each threshold, with the ratio peaking near 2.8--2.9$\times$ between thresholds of 50 and 80. Collectively, these visualizations demonstrate that Slaty-backed Gulls consistently have a much higher proportion of both strongly contrasting and absolutely dark pixels, making these features highly discriminative.

\begin{figure}[H]
    \centering
    % (a) Heatmap
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I1/wingtip_darkness_heatmap.png}
        \caption{Heatmap of wingtip darkness and contrast features for each species.}
        \label{fig:intensity_heatmap}
    \end{subfigure}
    \hfill
    % (b) Difference bar plot
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I1/diffbythresh.png}
        \caption{Absolute difference in percentage of dark pixels by threshold.}
        \label{fig:diff_bar}
    \end{subfigure}
    \vspace{1em}
    % (c) Difference curve
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I1/intensitydiffthreshold.png}
        \caption{Difference curve (Slaty-backed minus Glaucous-winged) across thresholds.}
        \label{fig:diff_curve}
    \end{subfigure}
    \hfill
    % (d) Ratio plot
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/REPORT_IMAGES_INTENSITY/I1/ratiogwgsbgdarkness.png}
        \caption{Ratio of dark pixel proportions (Slaty-backed/Glaucous-winged) by threshold.}
        \label{fig:ratio_plot}
    \end{subfigure}
    \caption{Summary of wingtip darkness and contrast metrics distinguishing Slaty-backed and Glaucous-winged Gulls. (a) Heatmap of key features; (b) absolute difference by threshold; (c) difference curve; (d) ratio of proportions. These visualizations collectively highlight the strong and persistent contrast in wingtip darkness between the two species.}
    \label{fig:wingtip_darkness_summary}
\end{figure}

 ENDDDDDDDDDDDDDDD

\subsection{Local Binary Pattern(LBP) Pattern Results}

\section{Results: Local Binary Pattern Texture Analysis}
\label{sec:lbp_results}

Our Local Binary Pattern (LBP) analysis revealed significant texture differences between Slaty-backed and Glaucous-winged Gulls across anatomical regions. The implementation focused on both uniform patterns and transition counts, with histogram-based feature extraction.

\subsection{Discriminative Power Analysis}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/LBP/discriminative_power_heatmap.png}
    \caption{Regional discriminative power of texture properties. Color intensity shows percentage difference between species, with wingtip std\_intensity (57.4\%) being most distinctive.}
    \label{fig:heatmap}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/LBP/top_discriminative_features.png}
    \caption{Top discriminative features}
    \label{fig:top_features}
\end{figure}

% LBP Top Discriminative Features
\begin{itemize}
    \item \textbf{Wingtip:} Homogeneity (\textit{45.74\% difference})
    \item \textbf{Wingtip:} Contrast (\textit{32.36\% difference})
    \item \textbf{Wing:} Homogeneity (\textit{30.83\% difference})
\end{itemize}

\subsection{Limitations of standard LBP code analysis}
\label{subsec:feature_limitations}

Although standard texture properties demonstrated substantial differences between species (with top features exceeding 50\% difference), the abstract features (Number of Ones and Transitions) showed more modest discriminative power (maximum 10.4\%). It is important to note that the features from LBP pattern codes are not guaranteed to be rotationally invariant in the implementation used unlike the abstract features (Number of Ones and Transitions).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/LBP/default_wingtip_ones_histogram.png}
        \caption{Wingtip Number of Ones distribution (9.38\% difference)}
        \label{fig:wingtip_ones}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/LBP/default_wing_transitions_histogram.png}
        \caption{Wing Transitions distribution (9.4\% difference)}
        \label{fig:wing_transitions}
    \end{subfigure}
    \vspace{2mm}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/LBP/default_wingtip_transitions_histogram.png}
        \caption{Wingtip Transitions distribution (8.0\% difference)}
        \label{fig:wingtip_transitions}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/LBP/default_wing_ones_histogram.png}
        \caption{Wing Number of Ones distribution (e.g., 7.5\% difference)}
        \label{fig:wing_ones}
    \end{subfigure}
    \caption{Abstract feature histograms showing transition complexity and bright neighbor distributions across different wing regions.}
    \label{fig:feature_histograms}
\end{figure}


Figure~\ref{fig:feature_histograms} provides detailed histograms of specific features, such as the wingtip's 'Number of Ones' and 'Transitions'.

\subsection{Abstract Feature Distributions}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.44\textwidth]{images/LBP/default_feature_diff_barchart.png}
    \caption{Abstract feature differences, showing head region's Number of Ones as most discriminative (10.4\% difference).}
    \label{fig:abstract_features}
\end{figure}


The figures illustrate the results of abstract features across different regions. Figure~\ref{fig:abstract_features} presents a bar chart highlighting the most discriminative feature from 1s and transitions histogram. , showcasing their respective differences of 9.38\% and 9.4\%. To further quantify these observations, Table~\ref{tab:top_discriminative_features} lists the top discriminative features based on their percentage differences between species, emphasizing the significance of texture properties in distinguishing between them.

\subsection{Top Discriminative Features from 1s and Transitions Histograms}
\begin{table}[htbp]
    \centering
    \caption{Top Discriminative Features by Percentage Difference}
    \begin{tabular}{clc}
        \hline
        Rank & Feature & Difference (\%) \\
        \hline
        1 & Wingtip-ones-energy & 23.5 \\
        2 & Wingtip-transitions-energy & 20.6 \\
        3 & Wing-transitions-energy & 16.4 \\
        \hline
    \end{tabular}
    \label{tab:top_features}
    \caption{Top discriminative features ordered by their percentage difference between species. Features are region-property pairs, with texture properties showing the highest discriminative power.}
    \label{tab:top_discriminative_features}
\end{table}

The analysis demonstrates that LBP texture features, particularly in the wingtip region, provide robust discriminative power for gull species identification. The combination of standard intensity measures and transition pattern analysis yields effective differentiation between morphologically similar species.

\section{Clustering}
The clustering performance is visualized through two complementary perspectives: the cluster formations in reduced PCA space (Figure~\ref{fig:cluster_visualizations}) and the classification accuracy via confusion matrices which were generated considering majority species label within each cluster to be the true label(Figure~\ref{fig:confusion_matrices}).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/gmm_clustering.png}
        \caption{GMM Clustering}
        \label{fig:gmm_cluster}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/hierarchical_clustering.png}
        \caption{Hierarchical}
        \label{fig:hierarchical_cluster}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/kmeans_clustering.png}
        \caption{K-means}
        \label{fig:kmeans_cluster}
    \end{subfigure}
    \caption{Cluster formations in PCA-reduced space: (a) GMM shows smooth probability contours, (b) Hierarchical clustering displays tree-like linkages, and (c) K-means exhibits distinct spherical clusters. All methods show effective separation between species.}
    \label{fig:cluster_visualizations}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/gmm_confusion_matrix.png}
        \caption{GMM (95.6\% acc)}
        \label{fig:gmm_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/hierarchical_confusion_matrix.png}
        \caption{Hierarchical (92.8\% acc)}
        \label{fig:hierarchical_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/kmeans_confusion_matrix.png}
        \caption{K-means (94.2\% acc)}
        \label{fig:kmeans_cm}
    \end{subfigure}
    \caption{Confusion matrices showing classification performance: (a) GMM achieves highest accuracy, (b) Hierarchical shows balanced errors, and (c) K-means demonstrates robust performance. Diagonal elements represent correct classifications.}
    \label{fig:confusion_matrices}
\end{figure}

Key observations from Figures~\ref{fig:cluster_visualizations} and~\ref{fig:confusion_matrices}:
\begin{itemize}
    \item \textbf{GMM} (Figs.~\ref{fig:gmm_cluster}, \ref{fig:gmm_cm}) shows probabilistic boundaries with highest accuracy (95.6\%)
    \item \textbf{K-means} (Figs.~\ref{fig:kmeans_cluster}, \ref{fig:kmeans_cm}) forms compact clusters with 94.2\% accuracy
    \item \textbf{Hierarchical} (Figs.~\ref{fig:hierarchical_cluster}, \ref{fig:hierarchical_cm}) reveals dendrogram structure while maintaining 92.8\% accuracy
\end{itemize}

\subsection{Clustering Performance Evaluation}

The classification performance of all three clustering algorithms is summarized in Figure~\ref{fig:all_confusion_matrices}, with quantitative metrics presented in Table~\ref{tab:performance_summary}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/gmm_confusion_matrix.png}
        \caption{GMM (Accuracy: 95.6\%)}
        \label{fig:gmm_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/hierarchical_confusion_matrix.png}
        \caption{Hierarchical (Accuracy: 92.8\%)}
        \label{fig:hierarchical_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/clustering/kmeans_confusion_matrix.png}
        \caption{K-means (Accuracy: 94.2\%)}
        \label{fig:kmeans_cm}
    \end{subfigure}
    \caption{Comparative confusion matrices showing classification performance across clustering algorithms. GMM demonstrates the highest accuracy with minimal misclassifications, while all methods show strong species separation.}
    \label{fig:all_confusion_matrices}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Performance comparison of clustering algorithms}
\label{tab:performance_summary}
\begin{tabular}{lcccc}
\toprule
Algorithm & Accuracy & Precision & Recall & F1-score \\
\midrule
GMM & 95.6\% & 96.1\% & 95.2\% & 95.6\% \\
K-means & 94.2\% & 94.5\% & 93.9\% & 94.2\% \\
Hierarchical & 92.8\% & 93.2\% & 92.4\% & 92.8\% \\
\bottomrule
\end{tabular}
\end{table}

All methods demonstrated effective species separation, with wingtip intensity being the most discriminative feature across algorithms (Figure~\ref{fig:kmeans_feature_importance}).

% \subsection{Misclassified Points Analysis}

% The misclassified points from each clustering algorithm provide insights into the limitations of each method. Tables~\ref{tab:gmm_misclassified}--\ref{tab:kmeans_misclassified} show representative samples of misclassified instances.

% \begin{table}[htbp]
% \centering
% \caption{Misclassified points in Gaussian Mixture Model clustering}
% \label{tab:gmm_misclassified}
% \begin{adjustbox}{width=0.9\textwidth}
% \csvautotabular{gmm_misclassified_points.csv}
% \end{adjustbox}
% \end{table}

% \begin{table}[htbp]
% \centering
% \caption{Misclassified points in Hierarchical clustering}
% \label{tab:hierarchical_misclassified}
% \begin{adjustbox}{width=0.9\textwidth}
% \csvautotabular{hierarchical_misclassified_points.csv}
% \end{adjustbox}
% \end{table}

% \begin{table}[htbp]
% \centering
% \caption{Misclassified points in K-means clustering}
% \label{tab:kmeans_misclassified}
% \begin{adjustbox}{width=0.9\textwidth}
% \csvautotabular{kmeans_misclassified_points.csv}
% \end{adjustbox}
% \end{table}

The clustering results demonstrate varying performance across the three algorithms. Figure~\ref{fig:clustering_results} visualizes the cluster assignments in the reduced PCA space, while the misclassification tables reveal specific cases where each algorithm struggled to correctly classify instances. The GMM approach (Figure~\ref{fig:gmm_cluster}) provides soft clustering assignments, which may be beneficial for ambiguous cases. The hierarchical method (Figure~\ref{fig:hierarchical_cluster}) shows the dendrogram structure, and K-means (Figure~\ref{fig:kmeans_cluster}) demonstrates its characteristic spherical cluster shapes. Regardless this proved that the features found to be of significant difference between the two classes are useful for classification.

\chapter{Discussion}

\section{Interpretation of Model Performance and Biological Significance}
The results demonstrate that deep learning models, particularly VGG-16, achieved high classification accuracy (95.74\% test accuracy) in distinguishing Slaty-backed and Glaucous-winged Gulls, validating the efficacy of transfer learning for fine-grained avian classification. The Grad-CAM visualizations confirmed that models consistently focused on wing and wingtip regions, mirroring ornithological expertise. For instance, in Slaty-backed Gulls, high activation coincided with dark primary feather patterns, while Glaucous-winged Gulls showed activations around lighter wingtips. This congruence between model attention and expert knowledge not only validates the models’ decision-making but also reinforces the biological relevance of wingtip morphology as a taxonomic marker aligning with the biological reality that diagnostic traits in these species are localized to specific morphological regions rather than global image contexts.

\section{Quantitative Validation of Morphological Differences}
The intensity analysis revealed statistically significant differences ($p < 0.001$) in wing and wingtip brightness between species. Slaty-backed Gulls exhibited mean wing intensities 108.3\% darker than Glaucous-winged Gulls, with wingtip intensities 90.5\% darker. Crucially, the presence of very dark wingtip pixels (intensity $<30$) emerged as a key discriminative feature: Slaty-backed Gulls had 25–48\% of wingtip pixels below this threshold, compared to near-zero in Glaucous-winged Gulls. This stark contrast could explain the potential models’ reliance on these regions and provides quantitative support for field identification guidelines.

However, the overlapping distributions in wingtip intensity highlight limitations. While mean wing intensity is a robust classifier, individual variation in wingtip darkness---potentially due to high individual variation or environmental factors---introduces ambiguity. This underscores the necessity of multi-feature analysis, as no single metric fully captures species differences.

\subsection{Feature Analysis and Clustering Validation}
Local Binary Pattern (LBP) analysis identified key features such as contrast and energy although the difference were not as significant as intensity values. Regardless the intensity results allowed clustering methods like GMM to achieve 95.6\% accuracy (corresponding to majority class of the cluster), demonstrating that traditional machine learning can effectively use the features identified to be significantly differnet to classify the species with high accuracies. Notably, the clustering results revealed natural groupings aligning with species labels, independent of supervised training. This unsupervised validation strengthens the conclusion that morphological differences are inherent and measurable.

\subsection{Limitations and Practical Implications}
The study’s reliance on high-quality, curated images (Stage 3 dataset) limits real-world applicability. While data augmentation and other techniques mitigated overfitting, performance on noisy field images remains untested. Furthermore, the exclusion of juveniles, birds in moulting stages, bird of varied postures and hybrids---a necessity for clear morphological analysis---creates a taxonomic ``blind spot" that must be addressed in future work.

Practically, these models could be integrated into citizen science platforms like \textit{iNaturalist} to assist non-experts in gull identification. The interpretability framework also provides a template for collaborative AI-ornithology research, enabling experts to validate and refine model decisions iteratively.

\chapter{Conclusion}

This study successfully addressed its primary research aims. First, high-accuracy classifiers, particularly VGG-16 achieving a test accuracy of 95.74\%, were developed using transfer learning, demonstrating that pre-trained convolutional neural network architectures can excel in fine-grained gull classification even with limited data. Iterative dataset refinement from 937 to 775 expert-curated images was vital, minimizing noise from age-related and environmental variability. Second, interpretability was achieved through Grad-CAM visualizations, which effectively bridged artificial intelligence and biological expertise by highlighting the wing and wingtip regions---corroborating that the models prioritize known diagnostic traits. This transparency is essential for building trust in automated taxonomic tools. Third, statistical and clustering analyses confirmed significant and quantifiable differences in wing darkness (with $p < 0.001$) and wingtip texture (with 57.4\% higher variability in Slaty-backed Gulls), empirically supporting field identification criteria while also revealing nuances such as interspecific overlap in wingtip patterns that require expert contextualization. Finally, the dominance of wingtip features in both model attention maps and statistical analysis suggests that artificial intelligence can not only replicate human expertise but also uncover underappreciated diagnostic traits, such as the prevalence of very dark pixels (intensity $<$30) in Slaty-backed Gulls. Collectively, these achievements demonstrate the potential of integrating deep learning with interpretability and quantitative analysis to advance fine-grained species identification and provide new insights into avian taxonomy.


\chapter{Appendix}

% \subsection*{Synthesis of Contributions}
% This work advances avian taxonomy by:
% \begin{itemize}
%     \item Providing a reproducible framework for AI-assisted morphological analysis.
%     \item Quantifying previously qualitative field characteristics (e.g., ``dark wingtips").
%     \item Demonstrating that unsupervised clustering can validate supervised model predictions.
% \end{itemize}

% Future research should expand to hybrid specimens and juvenile plumage while testing robustness across diverse imaging conditions. By integrating computational power with ecological knowledge, this approach offers a paradigm for addressing the taxonomic impediment in rapidly evolving species complexes.
% \chapter{Discussion}



% \section{Biological Significance}

% These results demonstrate clear, quantifiable differences between the two gull species:

% \begin{itemize}
%     \item \textbf{Overall Wing Color:} Slaty-backed Gulls have significantly darker wings, with intensity values approximately half those of Glaucous-winged Gulls.
    
%     \item \textbf{Wingtip Darkness Pattern:} Slaty-backed Gulls have a dramatically higher percentage of very dark pixels in their wingtips. Over 25\% of wingtip pixels have intensity below 30, compared to virtually none in Glaucous-winged Gulls.
    
%     \item \textbf{Species Identification Feature:} The presence of very dark pixels (intensity $<$ 30) in the wingtip appears to be a reliable diagnostic feature for distinguishing between these species.
    
%     \item \textbf{Contrast Pattern:} The higher percentage of dark pixels in Slaty-backed Gull wingtips creates a more pronounced visual contrast between wing and wingtip regions.
% \end{itemize}

% These quantitative differences align with field observations that Slaty-backed Gulls have darker wings and more prominent dark wingtips compared to Glaucous-winged Gulls, providing a reliable basis for species identification in image analysis.


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}