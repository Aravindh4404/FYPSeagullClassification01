{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1heMf-jgFuE04wuozhNl1C1ZjAmcoy01q",
      "authorship_tag": "ABX9TyNsofvFNYL6/UKgi+iIQu/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravindh4404/FYPSeagullClassification01/blob/main/VGG2optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He-kJDTR1gFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085b7fe2-4096-4cab-ddb7-e5be0abbb5bf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n",
            "Using device: cpu\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[I 2024-12-11 09:44:00,604] A new study created in memory with name: no-name-3e450dcf-0516-4503-8898-84b259122eb4\n",
            "<ipython-input-5-a6658be12d16>:127: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
            "<ipython-input-5-a6658be12d16>:128: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
            "<ipython-input-5-a6658be12d16>:129: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.3, 0.7)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['Glaucous_Winged_Gull', 'Slaty_Backed_Gull']\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0, Epoch 1/20, Train Loss: 0.3761\n",
            "Validation Loss: 0.1247, Validation Accuracy: 95.45%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "!pip install optuna\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm  # For progress bars\n",
        "\n",
        "# Install Optuna if not already installed\n",
        "# !pip install optuna\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define the device for computation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive to save and load the model (if using Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the folder to save model checkpoints\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "checkpoint_folder = f'/content/drive/My Drive/FYP/VGGModel/HQ2ltst_{date_str}/'\n",
        "os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "\n",
        "# Data Augmentation for Training Set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # VGG expects 224x224 input size\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Simple resizing for validation and test sets\n",
        "transform_val_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "data_path = '/content/drive/My Drive/FYP/Dataset/HQ2/train'\n",
        "test_data_path = '/content/drive/My Drive/FYP/Dataset/HQ2/test'\n",
        "train_dataset = datasets.ImageFolder(data_path, transform=transform_train)\n",
        "test_dataset = datasets.ImageFolder(test_data_path, transform=transform_val_test)\n",
        "\n",
        "# Split the dataset into 80% training and 20% validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size],\n",
        "                                       generator=torch.Generator().manual_seed(42))  # Ensure reproducibility\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "num_workers = 4  # Adjust based on your system\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# Define class names as per dataset\n",
        "class_names = train_dataset.classes  # Automatically get class names from ImageFolder\n",
        "num_classes = len(class_names)\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "# Use Pre-trained VGG-16 model and modify it for binary classification\n",
        "class VGG16Modified(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(VGG16Modified, self).__init__()\n",
        "        self.vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "        # Replace the classifier with a custom binary classification layer\n",
        "        num_ftrs = self.vgg.classifier[6].in_features\n",
        "        self.vgg.classifier[6] = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_ftrs, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vgg(x)\n",
        "\n",
        "# Define the validation loop returning accuracy\n",
        "def validate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    average_loss = val_loss / len(loader)\n",
        "    print(f'Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy, average_loss\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.3, 0.7)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "    momentum = 0.9  # Default momentum for SGD\n",
        "\n",
        "    # Initialize the model\n",
        "    model = VGG16Modified(num_classes=num_classes).to(device)\n",
        "\n",
        "    # Modify dropout rate if necessary\n",
        "    # Note: In the current model, dropout is fixed at 0.5. To make it tunable, you need to adjust the model definition.\n",
        "    # For simplicity, we'll proceed with the existing dropout.\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define optimizer\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    else:  # SGD\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # Define scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=False)\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 20\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        average_train_loss = running_loss / len(train_loader)\n",
        "        print(f\"Trial {trial.number}, Epoch {epoch+1}/{epochs}, Train Loss: {average_train_loss:.4f}\")\n",
        "\n",
        "        # Validate the model\n",
        "        val_acc, val_loss = validate(model, val_loader, criterion)\n",
        "\n",
        "        # Update the scheduler\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Report intermediate objective value\n",
        "        trial.report(val_acc, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        # Save the best validation accuracy for this trial\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "# Create the study\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
        "study.optimize(objective, n_trials=20, timeout=None)\n",
        "\n",
        "# Print study statistics\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Validation Accuracy: {trial.value:.2f}%\")\n",
        "print(\"  Best hyperparameters: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Save the study\n",
        "study_dir = os.path.join(checkpoint_folder, 'optuna_study')\n",
        "os.makedirs(study_dir, exist_ok=True)\n",
        "joblib.dump(study, os.path.join(study_dir, 'study.pkl'))\n",
        "print(f\"Optuna study saved at {study_dir}\")\n",
        "\n",
        "# Function to plot the optimization history\n",
        "def plot_optimization_history(study):\n",
        "    optuna.visualization.plot_optimization_history(study)\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot the parameter importances\n",
        "def plot_param_importances(study):\n",
        "    optuna.visualization.plot_param_importances(study)\n",
        "    plt.show()\n",
        "\n",
        "# Plot optimization history\n",
        "plot_optimization_history(study)\n",
        "\n",
        "# Plot parameter importances\n",
        "plot_param_importances(study)\n",
        "\n",
        "# Retrain the model with the best hyperparameters\n",
        "def train_with_best_params(best_params):\n",
        "    learning_rate = best_params['learning_rate']\n",
        "    weight_decay = best_params['weight_decay']\n",
        "    optimizer_name = best_params['optimizer']\n",
        "    # dropout_rate = best_params.get('dropout_rate', 0.5)  # Not used in current model\n",
        "\n",
        "    # Initialize the model\n",
        "    model = VGG16Modified(num_classes=num_classes).to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define optimizer\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    else:  # SGD\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "    # Define scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 20\n",
        "    best_val_acc = 0.0\n",
        "    best_model_path = os.path.join(checkpoint_folder, f\"best_model_vgg_{date_str}.pth\")\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        average_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(average_train_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {average_train_loss:.6f}\")\n",
        "\n",
        "        # Validate the model\n",
        "        val_acc, val_loss = validate(model, val_loader, criterion)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Update the scheduler\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved with accuracy: {best_val_acc:.2f}% at {best_model_path}\")\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the final model\n",
        "    final_model_path = os.path.join(checkpoint_folder, f\"final_model_vgg_{date_str}.pth\")\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"Final model saved at {final_model_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Retrain the model with the best hyperparameters\n",
        "best_params = study.best_trial.params\n",
        "print(\"Retraining the model with the best hyperparameters...\")\n",
        "trained_model = train_with_best_params(best_params)\n",
        "\n",
        "# Function to evaluate the model on the test set\n",
        "def test(model, loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    average_test_loss = test_loss / len(loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Loss: {average_test_loss:.6f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return average_test_loss, accuracy\n",
        "\n",
        "# Evaluate the trained model on the test set\n",
        "print(\"Evaluating the trained model on the test set...\")\n",
        "test_loss, test_accuracy = test(trained_model, test_loader, nn.CrossEntropyLoss())\n",
        "\n",
        "# Save the trained model\n",
        "final_model_path = os.path.join(checkpoint_folder, f\"trained_model_vgg_{date_str}.pth\")\n",
        "torch.save(trained_model.state_dict(), final_model_path)\n",
        "print(f\"Trained model saved at {final_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le-nFCaGCty8",
        "outputId": "4474ed7a-3491-4063-ed03-5f98c0d0b6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}